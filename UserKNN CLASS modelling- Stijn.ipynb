{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # to make jupyter print all outputs, not just the last one\n",
    "from IPython.core.display import HTML # to pretty print pandas df and be able to copy them over (e.g. to ppt slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df = pd.read_parquet('cleaned/strat_sample_netflix')\n",
    "movielens_df = pd.read_parquet('cleaned/strat_sample_movielens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47289 unique userIds are handled from the Netflix dataset.\n",
      "16 unique movieIds are handled from the Netflix dataset.\n",
      "\n",
      "6439 unique userIds are handled from the Movielens dataset.\n",
      "150 unique movieIds are handled from the Movielens dataset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# netflix\n",
    "review_data = netflix_df['review_data'].values\n",
    "user_ids = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data])\n",
    "ratings = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data])\n",
    "movieIds = np.concatenate([[movieId] * len(row) for movieId, row in zip(netflix_df['movieId'], review_data)])\n",
    "print(f\"{len(user_ids)} unique userIds are handled from the Netflix dataset.\")\n",
    "print(f\"{len(np.unique(movieIds))} unique movieIds are handled from the Netflix dataset.\")\n",
    "print()\n",
    "\n",
    "# movielens\n",
    "review_data2 = movielens_df['review_data'].values\n",
    "user_ids2 = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data2])\n",
    "ratings2 = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data2])\n",
    "movieIds2 = np.concatenate([[movieId] * len(row) for movieId, row in zip(movielens_df['movieId'], review_data2)])\n",
    "print(f\"{len(user_ids2)} unique userIds are handled from the Movielens dataset.\")\n",
    "print(f\"{len(np.unique(movieIds2))} unique movieIds are handled from the Movielens dataset.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Explanation\n",
    "\n",
    "`train_val_test_split`\n",
    "\n",
    "1. **Shuffle the Data**:\n",
    "   - The input data is shuffled using `data.sample(frac=1, random_state=42)` to ensure randomness. `random_state=42` ensures reproducibility.\n",
    "\n",
    "2. **Calculate Set Sizes**:\n",
    "   - The sizes of each set (training, validation, and test) are calculated based on the provided ratios and the total number of samples in the data.\n",
    "\n",
    "3. **Split the Data**:\n",
    "   - The shuffled data is split into three sets: training, validation, and test.\n",
    "   - The training data contains the first `num_train` samples.\n",
    "   - The validation data contains the next `num_val` samples, starting from the index immediately following the last training sample.\n",
    "   - The test data contains the remaining samples, starting from the index immediately following the last validation sample.\n",
    "\n",
    "4. **Reset Index**:\n",
    "   - The index of each set is reset to ensure that it starts from 0 and increases incrementally.\n",
    "\n",
    "5. **Return Sets**:\n",
    "   - The function returns the training, validation, and test sets as pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and test sets, simultaneously ensuring no training data flows into validation or test data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the data to be split.\n",
    "    - train_ratio: float, ratio of the training set size to the total data size (default: 0.8).\n",
    "    - val_ratio: float, ratio of the validation set size to the total data size (default: 0.1).\n",
    "    - test_ratio: float, ratio of the test set size to the total data size (default: 0.1).\n",
    "\n",
    "    Returns:\n",
    "    - train_data: pandas DataFrame, training set.\n",
    "    - val_data: pandas DataFrame, validation set.\n",
    "    - test_data: pandas DataFrame, test set.\n",
    "    \"\"\"\n",
    "    # Shuffle the data\n",
    "    data_shuffled = data.sample(frac=1, random_state=42)\n",
    "\n",
    "    # Calculate the sizes of each set\n",
    "    num_samples = len(data_shuffled)\n",
    "    num_train = int(train_ratio * num_samples)\n",
    "    num_val = int(val_ratio * num_samples)\n",
    "    num_test = num_samples - num_train - num_val\n",
    "\n",
    "    # Split the data into train, validation, and test sets\n",
    "    train_data = data_shuffled[:num_train]\n",
    "    \n",
    "    # Below is ensured the validation data and the test data starts after the indices which are already in the training data, ensuring that no training data will flow into validation of test data.\n",
    "    val_data = data_shuffled[num_train:num_train+num_val]\n",
    "    test_data = data_shuffled[num_train+num_val:]\n",
    "\n",
    "    # Reset index for each set\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    val_data.reset_index(drop=True, inplace=True)\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up user-item matrix\n",
    "First we will create a user-item matrix which records all the user-item interactions.\n",
    "\n",
    "\n",
    "### `create_user_item_matrix` Function Explanation\n",
    "\n",
    "### Steps:\n",
    "1. **Extract Review Data**:\n",
    "   - Extract the review data from the provided DataFrame, which contains user IDs, ratings, and movie IDs.\n",
    "\n",
    "2. **Create User and Movie IDs Arrays**:\n",
    "   - Extract user IDs, ratings, and movie IDs from the review data and concatenate them into separate arrays.\n",
    "   - Generate dictionaries to map user IDs and movie IDs to unique indices in the user-item matrix.\n",
    "\n",
    "3. **Initialize User-Item Matrix**:\n",
    "   - Determine the dimensions of the user-item matrix based on the number of unique users and movies.\n",
    "   - Initialize an empty user-item matrix filled with NaN values.\n",
    "\n",
    "4. **Populate User-Item Matrix**:\n",
    "   - Iterate through the review data and populate the user-item matrix with ratings.\n",
    "   - Map user and movie IDs to their corresponding indices in the matrix and insert the ratings.\n",
    "\n",
    "5. **Return Results**:\n",
    "   - Return the user-item matrix along with dictionaries mapping user and movie IDs to indices, and arrays containing user and movie IDs.\n",
    "  \n",
    "### Functions Used and Purpose:\n",
    "\n",
    "- **`np.concatenate()`**: Used to concatenate arrays containing user IDs, ratings, and movie IDs extracted from the review data.\n",
    "- **`enumerate()`**: Used to iterate over the unique user IDs and movie IDs and generate indices for mapping.\n",
    "- **`np.unique()`**: Used to find the unique user IDs and movie IDs in the review data.\n",
    "- **`np.full()`**: Used to initialize an empty user-item matrix filled with NaN values.\n",
    "- **`zip()`**: Used to iterate over multiple iterables simultaneously (user IDs, movie IDs, ratings).\n",
    "- **`enumerate()`**: Used to iterate over the indices and elements of an iterable (user IDs, movie IDs) simultaneously.\n",
    "- **Indexing and Slicing**: Used to access and modify elements in arrays and matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_matrix(train_test_val_set):\n",
    "    \"\"\"\n",
    "    Creates a user-item matrix from the provided dataset containing review data.\n",
    "\n",
    "    Parameters:\n",
    "    train_test_val_set (DataFrame): DataFrame containing review data with columns 'review_data',\n",
    "                                    which is a list of dictionaries with keys 'userId', 'rating',\n",
    "                                    and 'movieId'.\n",
    "\n",
    "    Returns:\n",
    "    user_item_matrix (numpy.ndarray): Matrix representing users' ratings for items (movies), the matrix is an NumPy array which contains lists of user-item interactions, meaning a user and their corresponding ratings to the movieIds.    \n",
    "    \n",
    "    user_id_dict (dict): Dictionary mapping user IDs to unique indices in the user-item matrix.\n",
    "    \n",
    "    movie_id_dict (dict): Dictionary mapping movie IDs to unique indices in the user-item matrix.\n",
    "    \n",
    "    user_ids (numpy.ndarray): Array containing user IDs corresponding to each rating in the matrix.\n",
    "    \n",
    "    movie_ids (numpy.ndarray): Array containing movie IDs corresponding to each rating in the matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    review_data = train_test_val_set['review_data'].values\n",
    "    user_ids = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data])\n",
    "    ratings = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data])\n",
    "    movieIds = np.concatenate([[movieId] * len(row) for movieId, row in zip(train_test_val_set['movieId'], review_data)])\n",
    "\n",
    "    # create dictionaries to map user IDs and movie IDs to unique indices to map over\n",
    "    user_id_dict = {user_id: index for index, user_id in enumerate(np.unique(user_ids))}\n",
    "    movie_id_dict = {movie_id: index for index, movie_id in enumerate(np.unique(movieIds))}\n",
    "\n",
    "    # initialize an empty user-item matrix\n",
    "    user_count = len(user_id_dict)\n",
    "    movie_count = len(movie_id_dict)\n",
    "    user_item_matrix = np.full((user_count, movie_count), np.nan)\n",
    "\n",
    "    # populate the user-item matrix with ratings from the dataset\n",
    "    for i, (user_id, movie_id, rating) in enumerate(zip(user_ids, movieIds, ratings)):\n",
    "        user_index = user_id_dict[user_id]\n",
    "        movie_index = movie_id_dict[movie_id]\n",
    "        user_item_matrix[user_index, movie_index] = rating\n",
    "\n",
    "    return user_item_matrix, user_id_dict, movie_id_dict, user_ids, movieIds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read UserKNN REGR modelling - Stijn .ipynb for function explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_user_similarity_manhattan(user_ratings_matrix, threshold):\n",
    "    \"\"\"\n",
    "    Calculate user similarity using Manhattan distance-based similarity measure.\n",
    "\n",
    "    Parameters:\n",
    "    user_ratings_matrix (numpy.ndarray): Matrix representing users' ratings for items (movies).\n",
    "    threshold (float): Threshold value for considering ratings in the similarity calculation.\n",
    "\n",
    "    Returns:\n",
    "    similarity_matrix (numpy.ndarray): Matrix representing similarity between users based on the Manhattan distance.\n",
    "\n",
    "    The Manhattan distance-based similarity measure is calculated as follows:\n",
    "    1. Compute the dot product of each pair of row vectors in the user_ratings_matrix, considering only values above the threshold.\n",
    "    2. Calculate the norms of each row vector, considering only values above the threshold.\n",
    "    3. Replace zero norms with a small value to avoid division by zero.\n",
    "    4. Calculate the similarity matrix using broadcasting, where the similarity between users i and j is given by the dot product\n",
    "       divided by the product of their norms.\n",
    "    5. Set diagonal elements to 0 to avoid self-similarity.\n",
    "\n",
    "    \"\"\"\n",
    "    # Fill in the missing data with 0s\n",
    "    user_ratings_matrix = np.nan_to_num(user_ratings_matrix, nan=0)\n",
    "    \n",
    "    # this line calculates the dot product of each pair of row vectors, therefore how similar they are, considering only values above the threshold to decrease computational weight\n",
    "    dot_products = np.dot(np.where(user_ratings_matrix >= threshold, user_ratings_matrix, 0), user_ratings_matrix.T)\n",
    "    \n",
    "    # this line calculates the distance using the manhatten norm, because the absolute values of the elements are summed up\n",
    "    norms = np.sum(np.abs(np.where(user_ratings_matrix >= threshold, user_ratings_matrix, 0)), axis=1)\n",
    "    \n",
    "    # Replace zero norms with a small value to avoid division by zero\n",
    "    norms[norms == 0] = 1e-8\n",
    "    \n",
    "    # compute similarity matrix using broadcasting\n",
    "    similarity_matrix = dot_products / (norms[:, None] * norms)\n",
    "    \n",
    "    # Set diagonal elements to 0 to avoid self-similarity\n",
    "    np.fill_diagonal(similarity_matrix, 0)\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UserKNN classifier:\n",
    "During user based classification, the most likely rating is found by letting the nearest neighbors 'vote' on the item. Using the similarity weights, each vote carries a different weight. By multiplying the similarity weights with the ratings of an item the user has not watched but its neighbours have, the most likely and therefore predicted weight is computed.\n",
    "\n",
    "### Function Explanation\n",
    "\n",
    "`generate_user_knn_recommendations_classifier`\n",
    "\n",
    "1. **Ensure User ID Existence**:\n",
    "   - Check if the user ID exists in the provided dictionary. If not found, print an error message and return an empty list.\n",
    "\n",
    "2. **Find User Index**:\n",
    "   - Find the index of the user in the user-item matrix based on the provided user ID.\n",
    "\n",
    "3. **Find Similar Users**:\n",
    "   - Retrieve the similarity scores between the target user and all other users.\n",
    "   - Sort the indices based on similarity in descending order and select the top `k` most similar users.\n",
    "\n",
    "4. **Identify Rated Movies**:\n",
    "   - Find movies that have been rated by the selected similar users.\n",
    "\n",
    "5. **Calculate Votes**:\n",
    "   - Calculate the \"votes\" for each movie from the similar users based on their ratings and similarity weights.\n",
    "\n",
    "6. **Select Top Recommendations**:\n",
    "   - Select the top 5 movies with the highest votes and return them as recommendations.\n",
    "\n",
    "**Numpy Functions Explanation**\n",
    "\n",
    "- `np.argsort`: Returns the indices that would sort an array in ascending order. By using `[::-1]`, it sorts the indices in descending order.\n",
    "- `np.where`: Returns the indices of elements that satisfy a given condition. In this case, it's used to find movies that have been rated by similar users.\n",
    "- `np.dot`: Computes the dot product of two arrays. Here, it's used to calculate the weighted sum of ratings from similar users.\n",
    "- `np.argsort` (again): It's used to find the indices that would sort the `votes` array in ascending order. By selecting the last 5 indices (`[-5:]`) and reversing them (`[::-1]`), we get the indices of the top 5 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_knn_classifier_with_movies(user_id, user_item_matrix, user_similarity_matrix, user_id_dict, movie_id_dict, k):\n",
    "    \"\"\"\n",
    "    Generates movie recommendations for a given user using user-based k-nearest neighbors (KNN) collaborative filtering with neighborhood-based classification.\n",
    "\n",
    "    Parameters:\n",
    "    user_id (int): ID of the user for whom recommendations are to be generated.\n",
    "    user_item_matrix (numpy.ndarray): Matrix representing users' ratings for items (movies).\n",
    "    user_similarity_matrix (numpy.ndarray): Matrix representing cosine similarity between users.\n",
    "    user_id_dict (dict): Dictionary mapping user IDs to unique indices in the user-item matrix.\n",
    "    movie_id_dict (dict): Dictionary mapping movie IDs to unique indices in the user-item matrix.\n",
    "    k (int): Number of nearest neighbors to consider for recommendations.\n",
    "\n",
    "    Returns:\n",
    "    recommendations (list): List of tuples containing recommended movie IDs and their predicted ratings for the given user.\n",
    "    \"\"\"\n",
    "    # Ensure user ID exists in the dictionary\n",
    "    if user_id not in user_id_dict:\n",
    "        print(f\"User with ID {user_id} not found.\")\n",
    "        return []\n",
    "\n",
    "    # find the index of the user in the user-item matrix\n",
    "    user_index = user_id_dict[user_id]\n",
    "\n",
    "    # this line calculates the similarity score between the target and other users and sorts it descendingly\n",
    "    similar_users_indices = np.argsort(user_similarity_matrix[user_index])[::-1][:k]\n",
    "\n",
    "    # the indices of users in the previous line are then used here to find the ratings of those users\n",
    "    rated_movies = np.where(~np.isnan(user_item_matrix[similar_users_indices]))[1]\n",
    "\n",
    "    # this line is selecting the ratings of the similar users which are the similar users indices\n",
    "    neighbor_ratings = user_item_matrix[similar_users_indices][:, rated_movies]\n",
    "    # thie line is computing similarity weights\n",
    "    similarity_weights = user_similarity_matrix[user_index, similar_users_indices][:, np.newaxis] # by converting to column vector the matrix multiplication can be performed because the shape is now the same as neighbor_ratings\n",
    "    # the dot product between neighborhood ratings and similarity weights transposed sums up the products of corresponding elements of the two matrices. This results in a single value for each movie, representing the aggregated \"vote\" or weighted sum of ratings from similar users.\n",
    "    votes = np.dot(neighbor_ratings.T, similarity_weights).flatten()\n",
    "\n",
    "    # Find the top 5 ratings with the maximum votes\n",
    "    top_indices = np.argsort(votes)[-5:][::-1]\n",
    "    top_recommendations = [(list(movie_id_dict.keys())[list(movie_id_dict.values()).index(rated_movies[idx])], votes[idx]) for idx in top_indices]\n",
    "\n",
    "    return top_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classification task because: when you take the dot product between the transposed neighbor_ratings matrix and the similarity_weights matrix, you effectively multiply each rating by the corresponding similarity weight and sum up these products across all similar users. This operation aggregates the ratings given by similar users, in other words: a class. Taking into account giving more weight to ratings from neighbors who are more similar to the target user.\n",
    "\n",
    "### See a first batch of recommendations:\n",
    "\n",
    "By using the functions above to recommend movies above the following results are generated for each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_netflix, val_data_netflix, test_data_netflix = train_val_test_split(netflix_df)\n",
    "train_data_movielens, val_data_movielens, test_data_movielens = train_val_test_split(movielens_df)\n",
    "\n",
    "# set up paramters and threshold for similarity\n",
    "k=1\n",
    "threshold=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Netflix:`\n",
    "\n",
    "Training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserKNN classifier Recommendations: \n",
      "Top 5 recommended movies with predicted ratings for user 2189855:\n",
      "Movie ID: 1932, Predicted Rating: 5.0\n"
     ]
    }
   ],
   "source": [
    "# set up\n",
    "user_item_matrix_train1, user_id_dict_train1, movie_id_dict_train1, user_ids_train1, movieIds_train1 = create_user_item_matrix(train_data_netflix)\n",
    "# _, user_ratings_matrix_classified_train1 = computing_neutral_scores(user_item_matrix_train1)\n",
    "user_similarity_matrix_manhattan_train1 = calculate_user_similarity_manhattan(user_item_matrix_train1, threshold)\n",
    "\n",
    "# generate recommendations\n",
    "user_id_train1 = user_ids_train1[1]\n",
    "top5_pred_train1 = generate_user_knn_classifier_with_movies(user_id_train1, user_item_matrix_train1, user_similarity_matrix_manhattan_train1, user_id_dict_train1, movie_id_dict_train1, k)\n",
    "\n",
    "# print result\n",
    "print(f\"UserKNN classifier Recommendations: \\nTop 5 recommended movies with predicted ratings for user {user_id_train1}:\")\n",
    "for movie_id, predicted_rating in top5_pred_train1:\n",
    "    print(f\"Movie ID: {movie_id}, Predicted Rating: {predicted_rating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserKNN classifier Recommendations: \n",
      "Top 5 recommended movies with predicted ratings for user 333490:\n",
      "Movie ID: 400, Predicted Rating: 3.0\n"
     ]
    }
   ],
   "source": [
    "# set up\n",
    "user_item_matrix_val1, user_id_dict_val1, movie_id_dict_val1, user_ids_val1, movieIds_val1 = create_user_item_matrix(val_data_netflix)\n",
    "# _, user_ratings_matrix_classified_val1 = computing_neutral_scores(user_item_matrix_val1)\n",
    "user_similarity_matrix_manhattan_val1 = calculate_user_similarity_manhattan(user_item_matrix_val1, threshold)\n",
    "\n",
    "# generate recommendations\n",
    "user_id_val1 = user_ids_val1[1]\n",
    "top5_pred_val1 = generate_user_knn_classifier_with_movies(user_id_val1, user_item_matrix_val1, user_similarity_matrix_manhattan_val1, user_id_dict_val1, movie_id_dict_val1, k)\n",
    "\n",
    "# print result\n",
    "print(f\"UserKNN classifier Recommendations: \\nTop 5 recommended movies with predicted ratings for user {user_id_val1}:\")\n",
    "for movie_id, rating in top5_pred_val1:\n",
    "    print(f\"Movie ID: {movie_id}, Predicted Rating: {rating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Movielens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserKNN classifier Recommendations: \n",
      "Top 5 recommended movies with predicted ratings for user 318074:\n",
      "Movie ID: 150379, Predicted Rating: 4.0\n"
     ]
    }
   ],
   "source": [
    "# set up\n",
    "user_item_matrix_train2, user_id_dict_train2, movie_id_dict_train2, user_ids_train2, movieIds_train2 = create_user_item_matrix(train_data_movielens)\n",
    "# _, user_ratings_matrix_classified_train2 = computing_neutral_scores(user_item_matrix_train2)\n",
    "user_similarity_matrix_manhattan_train2 = calculate_user_similarity_manhattan(user_item_matrix_train2, threshold)\n",
    "\n",
    "# generate recommendations\n",
    "user_id_train2 = user_ids_train2[1]\n",
    "top5_pred_train2 = generate_user_knn_classifier_with_movies(user_id_train2, user_item_matrix_train2, user_similarity_matrix_manhattan_train2, user_id_dict_train2, movie_id_dict_train2, k)\n",
    "\n",
    "# print result\n",
    "print(f\"UserKNN classifier Recommendations: \\nTop 5 recommended movies with predicted ratings for user {user_id_train2}:\")\n",
    "for movie_id, rating in top5_pred_train2:\n",
    "    print(f\"Movie ID: {movie_id}, Predicted Rating: {rating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserKNN classifier Recommendations: \n",
      "Top 5 recommended movies with predicted ratings for user 234859:\n",
      "Movie ID: 1467, Predicted Rating: 1.0\n"
     ]
    }
   ],
   "source": [
    "# set up\n",
    "user_item_matrix_val2, user_id_dict_val2, movie_id_dict_val2, user_ids_val2, movieIds_val2 = create_user_item_matrix(val_data_movielens)\n",
    "# _, user_ratings_matrix_classified_val2 = computing_neutral_scores(user_item_matrix_val2)\n",
    "user_similarity_matrix_manhattan_val2 = calculate_user_similarity_manhattan(user_item_matrix_val2, threshold)\n",
    "\n",
    "# generate recommendations\n",
    "user_id_val2 = user_ids_val2[1]\n",
    "top5_pred_val2 = generate_user_knn_classifier_with_movies(user_id_val2, user_item_matrix_val2, user_similarity_matrix_manhattan_val2, user_id_dict_val2, movie_id_dict_val2, k)\n",
    "\n",
    "# print result\n",
    "print(f\"UserKNN classifier Recommendations: \\nTop 5 recommended movies with predicted ratings for user {user_id_val2}:\")\n",
    "for movie_id, rating in top5_pred_val2:\n",
    "    print(f\"Movie ID: {movie_id}, Predicted Rating: {rating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the movieIds which are recommended are the same, the predicted rating differs somewhat, already indicating a difference between the two userKNN models.\n",
    "\n",
    "## Baseline performance\n",
    "\n",
    "To assess performance, we are going to compare the original ratings matrix with the predicted one after the userKnn model. In order to do so, we will generate a predicted rating matrix with the two functions below. Essentially an array of predicted ratings is generated in contrast with the tuple in the previous function `generate_user_knn_classifier_with_movies` with the top 5 results. The secon functions will append these in a new matrix. \n",
    "\n",
    "### Function Explanation\n",
    "\n",
    "`generate_predictions_array`\n",
    "1. Ensure that the provided user ID exists in the dictionary. If not found, print an error message and return an empty array.\n",
    "2. Find the index of the user in the user-item matrix based on the provided user ID.\n",
    "3. Get the similarity scores between the target user and all other users, then sort the indices based on similarity in descending order and select the top `k` most similar users.\n",
    "4. Find movies that have been rated by the selected similar users.\n",
    "5. Calculate the \"votes\" for each movie from the similar users based on their ratings and similarity weights.\n",
    "6. Generate an array of predicted ratings for the given user and all movies. Initialize with NaN for unrated movies, and fill in the predicted ratings for rated movies.\n",
    "\n",
    "`generate_predicted_user_item_matrix`\n",
    "1. Initialize an empty matrix to hold the predicted ratings for all users and movies.\n",
    "2. Iterate over each user in the `user_id_dict`.\n",
    "3. For each user, generate predictions using the `generate_predictions_array` function and fill in the corresponding row in the predicted matrix.\n",
    "4. Return the predicted user-item matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_array(user_id, user_item_matrix, user_similarity_matrix, user_id_dict, movie_id_dict, k):\n",
    "    \"\"\"\n",
    "    Generates movie ratings predictions for a given user using user-based k-nearest neighbors (KNN) collaborative filtering with neighborhood-based classification.\n",
    "\n",
    "    Parameters:\n",
    "    user_id (int): ID of the user for whom ratings are to be predicted.\n",
    "    user_item_matrix (numpy.ndarray): Matrix representing users' ratings for items (movies).\n",
    "    user_similarity_matrix (numpy.ndarray): Matrix representing cosine similarity between users.\n",
    "    user_id_dict (dict): Dictionary mapping user IDs to unique indices in the user-item matrix.\n",
    "    movie_id_dict (dict): Dictionary mapping movie IDs to unique indices in the user-item matrix.\n",
    "    k (int): Number of nearest neighbors to consider for recommendations.\n",
    "\n",
    "    Returns:\n",
    "    predicted_ratings (numpy.ndarray): Array containing predicted ratings for the given user and all movies in movie_id_dict.\n",
    "    \"\"\"\n",
    "    # Ensure user ID exists in the dictionary\n",
    "    if user_id not in user_id_dict:\n",
    "        print(f\"User with ID {user_id} not found.\")\n",
    "        return np.array([])\n",
    "\n",
    "    # Find the index of the user in the user-item matrix\n",
    "    user_index = user_id_dict[user_id]\n",
    "\n",
    "    # Get similarity scores of the user with other users and sort indices\n",
    "    similar_users_indices = np.argsort(user_similarity_matrix[user_index])[::-1][:k]\n",
    "\n",
    "    # Find movies rated by similar users\n",
    "    rated_movies = np.where(~np.isnan(user_item_matrix[similar_users_indices]))[1]\n",
    "\n",
    "    # the dot product between neighborhood ratings and similarity weights transposed sums up the products of corresponding elements of the two matrices. This results in a single value for each movie, representing the aggregated \"vote\" or weighted sum of ratings from similar users.\n",
    "    neighbor_ratings = user_item_matrix[similar_users_indices][:, rated_movies]\n",
    "    similarity_weights = user_similarity_matrix[user_index, similar_users_indices][:, np.newaxis]\n",
    "    votes = np.dot(neighbor_ratings.T, similarity_weights).flatten()\n",
    "\n",
    "    # Find the predicted ratings for the given user and all movies\n",
    "    predicted_ratings = np.full(len(movie_id_dict), np.nan)  # Initialize with NaN for unrated movies\n",
    "    for movie_id, movie_index in movie_id_dict.items():\n",
    "        if movie_index in rated_movies:\n",
    "            idx = np.where(rated_movies == movie_index)[0][0]\n",
    "            predicted_ratings[movie_index] = votes[idx]\n",
    "\n",
    "    return predicted_ratings\n",
    "\n",
    "def generate_predicted_user_item_matrix(user_item_matrix, user_similarity_matrix, user_id_dict, movie_id_dict, k):\n",
    "    \"\"\"\n",
    "    Generates the predicted user-item matrix using user-based k-nearest neighbors (KNN) collaborative filtering with neighborhood-based classification for all users.\n",
    "\n",
    "    Parameters:\n",
    "    user_item_matrix (numpy.ndarray): Matrix representing users' ratings for items (movies).\n",
    "    user_similarity_matrix (numpy.ndarray): Matrix representing cosine similarity between users.\n",
    "    user_id_dict (dict): Dictionary mapping user IDs to unique indices in the user-item matrix.\n",
    "    movie_id_dict (dict): Dictionary mapping movie IDs to unique indices in the user-item matrix.\n",
    "    k (int): Number of nearest neighbors to consider for recommendations.\n",
    "\n",
    "    Returns:\n",
    "    predicted_matrix (numpy.ndarray): Predicted user-item matrix containing ratings for all users and movies.\n",
    "    \"\"\"\n",
    "    num_users = user_item_matrix.shape[0]\n",
    "    num_movies = len(movie_id_dict)\n",
    "    predicted_matrix = np.zeros((num_users, num_movies))\n",
    "\n",
    "    # Iterate over each user\n",
    "    for user_id in user_id_dict:\n",
    "        predicted_ratings = generate_predictions_array(user_id, user_item_matrix, user_similarity_matrix, user_id_dict, movie_id_dict, k)\n",
    "        predicted_matrix[user_id_dict[user_id]] = predicted_ratings\n",
    "\n",
    "    return predicted_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use RMSE as performance metric, using the function below to compute it:**\n",
    "\n",
    "### Function explanation\n",
    "\n",
    "`compute_rmse`\n",
    "1. **Handle Implicit Ratings**: \n",
    "   - Convert `NaN` values in both `original_ratings` and `predicted_ratings` arrays to 0s. This is done using `np.nan_to_num()` function to ensure that non-rated items are treated as having a rating of 0 for comparison.\n",
    "   \n",
    "2. **Flatten Arrays**:\n",
    "   - Flatten both `original_ratings` and `predicted_ratings` arrays into 1D arrays to facilitate making masks.\n",
    "\n",
    "3. **Remove Unrated Items**:\n",
    "   - Create a mask to filter out entries where the original rating is 0 (unrated items). Only ratings for rated items are considered for RMSE calculation.\n",
    "\n",
    "4. **Compute Squared Differences**:\n",
    "   - Calculate the squared differences between original and predicted ratings for the rated items.\n",
    "\n",
    "5. **Compute Mean Squared Error (MSE)**:\n",
    "   - Compute the mean squared error (MSE) by averaging the squared differences.\n",
    "\n",
    "6. **Compute RMSE**:\n",
    "   - Compute the square root of the mean squared error to obtain the RMSE value, which indicates the average difference between the original and predicted ratings.\n",
    "\n",
    "7. **Return RMSE**:\n",
    "   - Return the computed RMSE value as the output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(original_ratings, predicted_ratings):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean Square Error (RMSE) between the original ratings and the predicted ratings. MovieIds a user has not interacted with is turned into 0 for now.\n",
    "\n",
    "    Parameters:\n",
    "    original_ratings (numpy.ndarray): Array containing the original ratings.\n",
    "    predicted_ratings (numpy.ndarray): Array containing the predicted ratings.\n",
    "\n",
    "    Returns:\n",
    "    float: The RMSE value.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # handle implicit ratings with 0s for now\n",
    "    original_ratings = np.nan_to_num(original_ratings, nan=0, posinf=0, neginf=0)\n",
    "    predicted_ratings = np.nan_to_num(predicted_ratings, nan=0, posinf=0, neginf=0)\n",
    "\n",
    "    # make 1d arrays by flattening them to be able to make masks\n",
    "    original_ratings_flat = original_ratings.flatten()\n",
    "    predicted_ratings_flat = predicted_ratings.flatten()\n",
    "    \n",
    "    # remove entries with no original rating (unrated items)\n",
    "    mask = original_ratings_flat != 0\n",
    "    original_ratings_flat = original_ratings_flat[mask]\n",
    "    predicted_ratings_flat = predicted_ratings_flat[mask]\n",
    "    \n",
    "    # Compute the squared differences\n",
    "    squared_diff = np.square(original_ratings_flat - predicted_ratings_flat)\n",
    "    \n",
    "    # Compute the mean squared error\n",
    "    mse = np.mean(squared_diff)\n",
    "    \n",
    "    # Compute the square root of the mean squared error to get RMSE\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Netflix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings_matrix_train1 = generate_predicted_user_item_matrix(user_item_matrix_train1, user_similarity_matrix_manhattan_train1, user_id_dict_train1, movie_id_dict_train1, k=1)\n",
    "predicted_ratings_matrix_val1 = generate_predicted_user_item_matrix(user_item_matrix_val1, user_similarity_matrix_manhattan_val1, user_id_dict_val1, movie_id_dict_val1, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training set: 1.7384809274722057\n",
      "RMSE on validation set: 1.3022715474584379\n"
     ]
    }
   ],
   "source": [
    "train1_rmse = compute_rmse(user_item_matrix_train1, predicted_ratings_matrix_train1)\n",
    "print(\"RMSE on training set:\", train1_rmse)\n",
    "val1_rmse = compute_rmse(user_item_matrix_val1, predicted_ratings_matrix_val1)\n",
    "print(\"RMSE on validation set:\", val1_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the kNN classifier shows promise, its performance still falls short of the desired range of 0 to 1 for the target variable. This may be due to several factors, including limited data points in the user-item interaction matrix, which hinders the classifier's ability to effectively capture user-item relationships. To improve the classifier's performance, it may be necessary to address these issues.\n",
    "\n",
    "This may be due to several factors, including limited data points in the user-item interaction matrix, which hinders the classifier's ability to effectively capture user-item relationships.\n",
    "\n",
    "Additionally, there may be insufficient similarity measures. Euclidean distance and cosine similarity are simple similarity metrics that may not fully capture the complexities of user preferences, potentially resulting in suboptimal classification outcomes.\n",
    "\n",
    "The classifier faces the cold start problem when dealing with new users or items due to limited training data, which can negatively impact prediction accuracy. It is important to tune hyperparameters to improve the performance of the classifier. Strategies such as feature augmentation or ensemble methods could help mitigate this issue.  \n",
    "\n",
    " The performance of the classifier is influenced by sensitivity to hyperparameters such as the number of neighbors (k) and distance metrics. Optimizing performance can be achieved by tuning these parameters using techniques such as grid search.\n",
    "\n",
    "Enhancing the classification accuracy of the kNN classifier and bringing predictions within the desired range of 0 to 1 can be achieved by addressing these factors and fine-tuning the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Movielens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings_matrix_train2 = generate_predicted_user_item_matrix(user_item_matrix_train2, user_similarity_matrix_manhattan_train2, user_id_dict_train2, movie_id_dict_train2, k=1)\n",
    "predicted_ratings_matrix_val2 = generate_predicted_user_item_matrix(user_item_matrix_val2, user_similarity_matrix_manhattan_val2, user_id_dict_val2, movie_id_dict_val2, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training set: 1.635124675119815\n",
      "RMSE on validation set: 1.6650019740592454\n"
     ]
    }
   ],
   "source": [
    "train2_rmse = compute_rmse(user_item_matrix_train2, predicted_ratings_matrix_train2)\n",
    "print(\"RMSE on training set:\", train2_rmse)\n",
    "val2_rmse = compute_rmse(user_item_matrix_val2, predicted_ratings_matrix_val2)\n",
    "print(\"RMSE on validation set:\", val2_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kNN classifier's current performance is below the desired range of 0 to 1 due to overfitting. This issue may arise from limited data points, insufficient similarity measures, and the cold start problem with new users or items. To mitigate overfitting, hyperparameter tuning is crucial. By addressing these factors and fine-tuning the classifier, we can enhance its accuracy and prevent overfitting, ensuring robust predictions within the desired range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Now we have recorded some baseline performance let's find the optimal value for K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Netflix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-value: 1 | RMSE: 1.7384809274722057\n",
      "K-value: 4 | RMSE: 7.575507505515374\n",
      "K-value: 10 | RMSE: 22.926300928777685\n",
      "K-value: 15 | RMSE: 34.98932603882835\n",
      "\n",
      "Best K-value: 1 | Best RMSE: 1.7384809274722057\n"
     ]
    }
   ],
   "source": [
    "k_list = [1, 4, 10, 15]\n",
    "rmse_list = []\n",
    "best_k_train1 = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for k in k_list:\n",
    "    predicted_item_matrix = generate_predicted_user_item_matrix(user_item_matrix_train1, user_similarity_matrix_manhattan_train1, user_id_dict_train1, movie_id_dict_train1, k=k)\n",
    "    \n",
    "    # Compute Root Mean Square Error (RMSE)\n",
    "    rmse = compute_rmse(user_item_matrix_train1, predicted_item_matrix)\n",
    "    \n",
    "    # Append the RMSE value to the list\n",
    "    rmse_list.append(rmse)\n",
    "    \n",
    "    # Check if current k gives the best RMSE\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_k_train1 = k\n",
    "\n",
    "# Print the result descendingly\n",
    "for i, k_value in enumerate(k_list):\n",
    "    print(f\"K-value: {k_value} | RMSE: {rmse_list[i]}\")\n",
    "\n",
    "print(f\"\\nBest K-value: {best_k_train1} | Best RMSE: {best_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Movielens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-value: 1 | RMSE: 1.635124675119815\n",
      "K-value: 4 | RMSE: 10.12441762145811\n",
      "K-value: 10 | RMSE: 29.679104198009984\n",
      "K-value: 15 | RMSE: 45.44467200152859\n",
      "\n",
      "Best K-value: 1 | Best RMSE: 1.635124675119815\n"
     ]
    }
   ],
   "source": [
    "k_list = [1, 4, 10, 15]\n",
    "rmse_list = []\n",
    "best_k_train2 = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for k in k_list:\n",
    "    predicted_item_matrix = generate_predicted_user_item_matrix(user_item_matrix_train2, user_similarity_matrix_manhattan_train2, user_id_dict_train2, movie_id_dict_train2, k=k)\n",
    "    \n",
    "    # Compute Root Mean Square Error (RMSE)\n",
    "    rmse = compute_rmse(user_item_matrix_train2, predicted_item_matrix)\n",
    "    \n",
    "    # Append the RMSE value to the list\n",
    "    rmse_list.append(rmse)\n",
    "    \n",
    "    # Check if current k gives the best RMSE\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_k_train2 = k\n",
    "\n",
    "# Print the result descendingly\n",
    "for i, k_value in enumerate(k_list):\n",
    "    print(f\"K-value: {k_value} | RMSE: {rmse_list[i]}\")\n",
    "\n",
    "print(f\"\\nBest K-value: {best_k_train2} | Best RMSE: {best_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more k nearest neighbours does not seem to let the knn classifier perform better. Various reasons with relation to knn classifier recommender systems can cause this:\n",
    "\n",
    "Additional neighbors can introduce noisy or irrelevant data points, which can impact prediction accuracy.\n",
    "Moreover, having more neighbors can reduce variance but may increase bias, affecting the model's sensitivity to data variations.\n",
    "\n",
    "It is important to avoid overfitting. Including too many neighbours can lead to overfitting and poor generalisation.\n",
    " \n",
    "Sparse datasets pose a challenge in finding enough similar instances with more neighbours.\n",
    "\n",
    "Higher dimensions make proximity less meaningful, affecting prediction accuracy. More neighbours increase computational burden and processing times.\n",
    "\n",
    "Achieving the right balance is essential to prevent a higher RMSE while improving prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final predictions on test set:\n",
    "\n",
    "`Netflix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test set: 1.2742789227402465\n"
     ]
    }
   ],
   "source": [
    "# set up\n",
    "user_item_matrix_test1, user_id_dict_test1, movie_id_dict_test1, user_ids_test1, movieIds_test1 = create_user_item_matrix(test_data_netflix)\n",
    "# _, user_ratings_matrix_classified_test1 = computing_neutral_scores(user_item_matrix_test1)\n",
    "user_similarity_matrix_manhattan_test1 = calculate_user_similarity_manhattan(user_item_matrix_test1, threshold)\n",
    "\n",
    "# set up predictions matrix\n",
    "predicted_item_matrix_test1 = generate_predicted_user_item_matrix(user_item_matrix_test1, user_similarity_matrix_manhattan_test1, user_id_dict_test1, movie_id_dict_test1, k=best_k_train1)\n",
    "\n",
    "# compute Root Mean Square Error (RMSE)\n",
    "rmse_test1 = compute_rmse(user_item_matrix_test1, predicted_item_matrix_test1)\n",
    "# print result on test set\n",
    "print(\"RMSE on test set:\", rmse_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Movielens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test set: 1.1523314651082859\n"
     ]
    }
   ],
   "source": [
    "# set up\n",
    "user_item_matrix_test2, user_id_dict_test2, movie_id_dict_test2, user_ids_test2, movieIds_test2 = create_user_item_matrix(test_data_movielens)\n",
    "# _, user_ratings_matrix_classified_test2 = computing_neutral_scores(user_item_matrix_test2)\n",
    "user_similarity_matrix_manhattan_test2 = calculate_user_similarity_manhattan(user_item_matrix_test2, threshold)\n",
    "\n",
    "# set up predictions matrix\n",
    "predicted_item_matrix_test2 = generate_predicted_user_item_matrix(user_item_matrix_test2, user_similarity_matrix_manhattan_test2, user_id_dict_test2, movie_id_dict_test2, k=best_k_train2)\n",
    "\n",
    "# compute Root Mean Square Error (RMSE)\n",
    "rmse_test2 = compute_rmse(user_item_matrix_test2, predicted_item_matrix_test2)\n",
    "# print result on test set\n",
    "print(\"RMSE on test set:\", rmse_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both datasets show better performance on the test set, with the best amount of k nearest neighbours. However, it is still not between 0 and 1, which would be desireable. To see that a classifier performs similar to a regression knn recommenders system is interesting, because the ratings are not defined as classes but as numbers, indicating the majority voting system in the rating prediction is a valid way to predict ratings. Ofcourse still taking into account that this model is not perfect.\n",
    "\n",
    "## Overall conclusion\n",
    "\n",
    "In UserKNN + genres modelling notebook I will add genre similarity to the similarity matrix, to see whether it will lead to model to have a better performance by for example giving it more user information (features in terms of genre preference) to deal with the cold start problem. Also, more user features could lead the model to catch the complexity of the data in terms of preferences, as it has more angles of training data to compute similarity and therefore recommendations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_ddb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
