{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # to make jupyter print all outputs, not just the last one\n",
    "from IPython.core.display import HTML # to pretty print pandas df and be able to copy them over (e.g. to ppt slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('cleaned/strat_2NDsample_netflix')\n",
    "df2 = pd.read_parquet('cleaned/strat_sample_movielens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netflix data stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1612370 reviews in the NETFLIX dataframe.\n",
      "There are 344699 unique users who have reviewed a movie.\n",
      "There are 250 movieIds in the NETFLIX dataset.\n",
      "A unique user places 5 reviews on average in the NETFLIX dataset.\n",
      "A movieId receives 6449 reviews on average in the NETFLIX dataset.\n"
     ]
    }
   ],
   "source": [
    "# Extract all user IDs from the 'review_data' column using list comprehension\n",
    "user_ids = [review_entry.get('userId') for row in df['review_data'] for review_entry in row if review_entry.get('userId')]\n",
    "\n",
    "# Count the number of unique users and reviews\n",
    "unique_users = set(user_ids)\n",
    "amount_of_reviews = len(user_ids)\n",
    "\n",
    "# Calculate averages\n",
    "avg_reviews_per_unique_user = amount_of_reviews / len(unique_users)\n",
    "avg_reviews_per_movie_id = amount_of_reviews / len(df)\n",
    "\n",
    "# Print results\n",
    "print(\"There are {} reviews in the NETFLIX dataframe.\".format(amount_of_reviews))\n",
    "print(\"There are {} unique users who have reviewed a movie.\".format(len(unique_users)))\n",
    "print(\"There are {} movieIds in the NETFLIX dataset.\".format(len(df)))\n",
    "print(\"A unique user places {} reviews on average in the NETFLIX dataset.\".format(round(avg_reviews_per_unique_user)))\n",
    "print(\"A movieId receives {} reviews on average in the NETFLIX dataset.\".format(round(avg_reviews_per_movie_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movielens data stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16555 reviews in the MOVIELENS dataframe.\n",
      "There are 14324 unique users who have reviewed a movie.\n",
      "There are 200 movieIds in the MOVIELENS dataset.\n",
      "A unique user places 1 reviews on average in the MOVIELENS dataset.\n",
      "A movieId receives 83 reviews on average in the MOVIELENS dataset.\n"
     ]
    }
   ],
   "source": [
    "review_data2 = df2['review_data'].values\n",
    "review_data2 = [row for row in review_data2 if row is not None]\n",
    "user_ids2 = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data2])\n",
    "ratings2 = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data2])\n",
    "movieIds2 = np.concatenate([[movieId] * len(row) for movieId, row in zip(df['movieId'], review_data2)])\n",
    "\n",
    "avg_reviews_per_unique_user2 = len(ratings2) / len(np.unique(user_ids2))\n",
    "avg_reviews_per_movie_id2 = len(ratings2) / len(df2)\n",
    "\n",
    "# print results\n",
    "print(\"There are {} reviews in the MOVIELENS dataframe.\".format(len(ratings2)))\n",
    "print(\"There are {} unique users who have reviewed a movie.\".format(len(np.unique(user_ids2))))\n",
    "print(\"There are {} movieIds in the MOVIELENS dataset.\".format(len(df2)))\n",
    "print(\"A unique user places {} reviews on average in the MOVIELENS dataset.\".format(round(avg_reviews_per_unique_user2)))\n",
    "print(\"A movieId receives {} reviews on average in the MOVIELENS dataset.\".format(round(avg_reviews_per_movie_id2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** the movielens can be considered as more sparse (meaning more null values) than Netflix as the amount of movieIds is much higher, but the avg. review per userId and per movieId is much lower. We will see in performance if this makes a difference.\n",
    "\n",
    "Define dataframe without date item in review_data dictionary to start with, later date features may be added for both Netflix and movielens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df = df[df.columns]\n",
    "netflix_df['review_data'] = netflix_df['review_data'].apply(lambda x: None if x is None else [{'userId': review['userId'], 'rating': review['rating']} for review in x if 'userId' in review and 'rating' in review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_df = df2[df2.columns]\n",
    "movielens_df['review_data'] = movielens_df['review_data'].apply(lambda x: None if x is None else [{'userId': review['userId'], 'rating': review['rating']} for review in x if 'userId' in review and 'rating' in review])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering and pre processing:\n",
    "\n",
    "Year and title will be dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_sample = df.drop(['year','title'],axis=1)\n",
    "movielens_sample = df2.drop(['year','title'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's split our data into train, validation and test sets where we ensure that no training data flows into test and validation sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Explanation\n",
    "\n",
    "`train_val_test_split`\n",
    "\n",
    "1. **Shuffle the Data**:\n",
    "   - The input data is shuffled using `data.sample(frac=1, random_state=42)` to ensure randomness. `random_state=42` ensures reproducibility.\n",
    "\n",
    "2. **Calculate Set Sizes**:\n",
    "   - The sizes of each set (training, validation, and test) are calculated based on the provided ratios and the total number of samples in the data.\n",
    "\n",
    "3. **Split the Data**:\n",
    "   - The shuffled data is split into three sets: training, validation, and test.\n",
    "   - The training data contains the first `num_train` samples.\n",
    "   - The validation data contains the next `num_val` samples, starting from the index immediately following the last training sample.\n",
    "   - The test data contains the remaining samples, starting from the index immediately following the last validation sample.\n",
    "\n",
    "4. **Reset Index**:\n",
    "   - The index of each set is reset to ensure that it starts from 0 and increases incrementally.\n",
    "\n",
    "5. **Return Sets**:\n",
    "   - The function returns the training, validation, and test sets as pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and test sets, simultaneously ensuring no training data flows into validation or test data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the data to be split.\n",
    "    - train_ratio: float, ratio of the training set size to the total data size (default: 0.8).\n",
    "    - val_ratio: float, ratio of the validation set size to the total data size (default: 0.1).\n",
    "    - test_ratio: float, ratio of the test set size to the total data size (default: 0.1).\n",
    "\n",
    "    Returns:\n",
    "    - train_data: pandas DataFrame, training set.\n",
    "    - val_data: pandas DataFrame, validation set.\n",
    "    - test_data: pandas DataFrame, test set.\n",
    "    \"\"\"\n",
    "    # Shuffle the data\n",
    "    data_shuffled = data.sample(frac=1, random_state=42)\n",
    "\n",
    "    # Calculate the sizes of each set\n",
    "    num_samples = len(data_shuffled)\n",
    "    num_train = int(train_ratio * num_samples)\n",
    "    num_val = int(val_ratio * num_samples)\n",
    "    num_test = num_samples - num_train - num_val\n",
    "\n",
    "    # Split the data into train, validation, and test sets\n",
    "    train_data = data_shuffled[:num_train]\n",
    "    # Below is ensured the validation data and the test data starts after the indices which are already in the training data, ensuring that no training data will flow into validation of test data.\n",
    "    val_data = data_shuffled[num_train:num_train+num_val]\n",
    "    test_data = data_shuffled[num_train+num_val:]\n",
    "\n",
    "    # Reset index for each set\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    val_data.reset_index(drop=True, inplace=True)\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data accordingly and take two differenct sample sizes to see what effect it has on model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netflix dataset splitting\n",
    "train_data, val_data, test_data = train_val_test_split(netflix_sample)\n",
    "train_data2, val_data2, test_data2 = train_val_test_split(movielens_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, let's define some function to make our life easer for the compatibility of more datasets. We gather unique item and user ids, create user-item matrix which will be centered, followed by performing SVD en making recommendations using the dot product between the decomposed matrices resulting from SVD:\n",
    "\n",
    "### Set-up user-item matrix\n",
    "First we will create a user-item matrix which records all the user-item interactions.\n",
    "\n",
    "\n",
    "### `create_user_item_matrix` Function Explanation\n",
    "\n",
    "### Steps:\n",
    "1. **Extract Review Data**:\n",
    "   - Extract the review data from the provided DataFrame, which contains user IDs, ratings, and movie IDs.\n",
    "\n",
    "2. **Create User and Movie IDs Arrays**:\n",
    "   - Extract user IDs, ratings, and movie IDs from the review data and concatenate them into separate arrays.\n",
    "   - Generate dictionaries to map user IDs and movie IDs to unique indices in the user-item matrix.\n",
    "\n",
    "3. **Initialize User-Item Matrix**:\n",
    "   - Determine the dimensions of the user-item matrix based on the number of unique users and movies.\n",
    "   - Initialize an empty user-item matrix filled with NaN values.\n",
    "\n",
    "4. **Populate User-Item Matrix**:\n",
    "   - Iterate through the review data and populate the user-item matrix with ratings.\n",
    "   - Map user and movie IDs to their corresponding indices in the matrix and insert the ratings.\n",
    "\n",
    "5. **Return Results**:\n",
    "   - Return the user-item matrix along with dictionaries mapping user and movie IDs to indices, and arrays containing user and movie IDs.\n",
    "  \n",
    "### Functions Used and Purpose:\n",
    "\n",
    "- **`np.concatenate()`**: Used to concatenate arrays containing user IDs, ratings, and movie IDs extracted from the review data.\n",
    "- **`enumerate()`**: Used to iterate over the unique user IDs and movie IDs and generate indices for mapping.\n",
    "- **`np.unique()`**: Used to find the unique user IDs and movie IDs in the review data.\n",
    "- **`np.full()`**: Used to initialize an empty user-item matrix filled with NaN values.\n",
    "- **`zip()`**: Used to iterate over multiple iterables simultaneously (user IDs, movie IDs, ratings).\n",
    "- **`enumerate()`**: Used to iterate over the indices and elements of an iterable (user IDs, movie IDs) simultaneously.\n",
    "- **Indexing and Slicing**: Used to access and modify elements in arrays and matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_matrix(train_test_val_set):\n",
    "    \"\"\"\n",
    "    Creates a user-item matrix from the provided dataset containing review data.\n",
    "\n",
    "    Parameters:\n",
    "    train_test_val_set (DataFrame): DataFrame containing review data with columns 'review_data',\n",
    "                                    which is a list of dictionaries with keys 'userId', 'rating',\n",
    "                                    and 'movieId'.\n",
    "\n",
    "    Returns:\n",
    "    user_item_matrix (numpy.ndarray): Matrix representing users' ratings for items (movies), the matrix is an NumPy array which contains lists of user-item interactions, meaning a user and their corresponding ratings to the movieIds.    \n",
    "    \n",
    "    user_id_dict (dict): Dictionary mapping user IDs to unique indices in the user-item matrix.\n",
    "    \n",
    "    movie_id_dict (dict): Dictionary mapping movie IDs to unique indices in the user-item matrix.\n",
    "    \n",
    "    user_ids (numpy.ndarray): Array containing user IDs corresponding to each rating in the matrix.\n",
    "    \n",
    "    movie_ids (numpy.ndarray): Array containing movie IDs corresponding to each rating in the matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    review_data = train_test_val_set['review_data'].values\n",
    "    user_ids = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data])\n",
    "    ratings = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data])\n",
    "    movieIds = np.concatenate([[movieId] * len(row) for movieId, row in zip(train_test_val_set['movieId'], review_data)])\n",
    "\n",
    "    # create dictionaries to map user IDs and movie IDs to unique indices to map over\n",
    "    user_id_dict = {user_id: index for index, user_id in enumerate(np.unique(user_ids))}\n",
    "    movie_id_dict = {movie_id: index for index, movie_id in enumerate(np.unique(movieIds))}\n",
    "\n",
    "    # initialize an empty user-item matrix\n",
    "    user_count = len(user_id_dict)\n",
    "    movie_count = len(movie_id_dict)\n",
    "    user_item_matrix = np.zeros((user_count, movie_count))\n",
    "\n",
    "    # populate the user-item matrix with ratings from netflix dataset\n",
    "    for i, (user_id, movie_id, rating) in enumerate(zip(user_ids, movieIds, ratings)):\n",
    "        user_index = user_id_dict[user_id]\n",
    "        movie_index = movie_id_dict[movie_id]\n",
    "        user_item_matrix[user_index, movie_index] = rating\n",
    "\n",
    "    return user_item_matrix, user_id_dict, movie_id_dict, user_ids, movieIds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To account for variantions in the ratings, let's center our rating matrix with the following function by substracting the user_mean of each row of the row total of each user (row_vector)\n",
    "### Function explanation\n",
    "`center_data`\n",
    "\n",
    "This function creates a centered matrix of the user-item matrix, which is commonly used in matrix factorization algorithms such as Singular Value Decomposition (SVD) and collaborative filtering.\n",
    "\n",
    "1. **Replace NaN Values**:\n",
    "   - Check for NaN values in the `user_item_matrix` and replace them with 0. This step is crucial for processing the data, as missing ratings are commonly represented as NaN.\n",
    "\n",
    "2. **Compute User Means**:\n",
    "   - Calculate the mean rating for each user across all items. This provides a measure of the average rating given by each user.\n",
    "\n",
    "3. **Center the Data**:\n",
    "   - Subtract the mean rating of each user (retrieved from the `user_means` array) from the corresponding ratings in the `user_item_matrix`. This centers the data by removing the user-specific variations in ratings.\n",
    "\n",
    "4. **Return Centered Matrix**:\n",
    "   - Return the centered user-item matrix, where each user's ratings are adjusted to reflect deviations from their mean rating.\n",
    "   - Also return the `user_means` array for potential future use or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_data(user_item_matrix):\n",
    "    \"\"\"\n",
    "    Creates a centered matrix of the previously created user-item matrix\n",
    "\n",
    "    Parameters:\n",
    "    User-item matrix which is made a Numpy array with appended lists with ratings of each users of each item. Each position in each list corresponds to the same movieId. Datatype within the matrix is float64. Each NaN value is converted to 0. In other words, for the time being the implicit feedback is converted to 0.\n",
    "\n",
    "    Return:\n",
    "    A centered user item matrix, where the row mean of each user is subtracted from the initial ratings, to account for variations in ratings\n",
    "    \n",
    "    \"\"\"\n",
    "    # Check for NaN values and replace them with 0\n",
    "    user_item_matrix[np.isnan(user_item_matrix)] = 0\n",
    "    \n",
    "    # Compute user means\n",
    "    user_means = np.mean(user_item_matrix, axis=1)\n",
    "    \n",
    "    # Center the data\n",
    "    centered_user_item_matrix = user_item_matrix - user_means[:, np.newaxis]\n",
    "    \n",
    "    return centered_user_item_matrix, user_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After centering the data, we would like to have less dimensions to work with, as it takes less time and computation power, and at the same time does not lower the quality of the predictions.\n",
    "\n",
    "### Function explanation\n",
    "`apply_svd`\n",
    "\n",
    "1. **SVD Decomposition**:\n",
    "   - Apply the SVD decomposition to the `centered_user_item_matrix` using the `np.linalg.svd` function from NumPy. This results in three matrices: U, Sigma, and Vt.\n",
    "\n",
    "2. **Sigma Matrix Adjustment**:\n",
    "   - Set up the Sigma matrix by keeping only the first `num_latent_factors` singular values and forming a diagonal matrix with them.\n",
    "\n",
    "3. **Truncate U and Vt**:\n",
    "   - Keep only the columns corresponding to the first `num_latent_factors` in both U and Vt matrices. This ensures that U represents the relationship between users and latent factors, and Vt represents the relationship between items and latent factors.\n",
    "\n",
    "4. **Return Decomposed Matrices**:\n",
    "   - Return the decomposed matrices U, Sigma, and Vt, which capture the underlying structure of the user-item interactions in terms of latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will decompose the user item matrix in this function using numpy\n",
    "def apply_svd(centered_user_item_matrix, num_latent_factors):\n",
    "    \"\"\"\n",
    "    Applies Singular Value Decomposition (SVD) to decompose the centered user-item matrix into three matrices:\n",
    "    U, Sigma, and Vt.\n",
    "\n",
    "    U: user matrix with values which represent the relation between the chosen latent factors, Users are the rows, matrix is orthonormal to Vt\n",
    "    Sigma: diagonal matrix where the chosen latent factors are in the diagonal line, ordered descendingly. \n",
    "    Vt: Item matrix with values which represent the relation between the chosen latent factors, Items are the columns, matrix is orthonormal to U\n",
    "\n",
    "    Parameters:\n",
    "    centered_user_item_matrix (numpy.ndarray): Centered user-item matrix to be decomposed.\n",
    "    num_latent_factors (int): Number of latent factors to retain in the decomposition.\n",
    "\n",
    "    Returns:\n",
    "    U (numpy.ndarray): Matrix representing the relationship between users and latent factors.\n",
    "    Sigma (numpy.ndarray): Diagonal matrix containing the singular values, representing the importance of each latent factor.\n",
    "    Vt (numpy.ndarray): Transpose of the matrix representing the relationship between items and latent factors.\n",
    "\n",
    "    \"\"\"\n",
    "    # U, sigma and Vt are created using the svd function from numpy\n",
    "    U, Sigma, Vt = np.linalg.svd(centered_user_item_matrix, full_matrices=False)\n",
    "    # set up sigma, which is the diagonal matrix from the decomposition, where the dimensions are dependent on the amount of latent factors\n",
    "    Sigma = np.diag(Sigma[:num_latent_factors])\n",
    "    # set up U and Vt which have to orthonormal to each other to ensure U represents each user and Vt represents each item, otherwise the total matrix would not add up.\n",
    "    U = U[:, :num_latent_factors]\n",
    "    Vt = Vt[:num_latent_factors, :]\n",
    "    return U, Sigma, Vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our decomposed matrices from the original matrix, let's make rating predictions by building up the original matrix with the given amount of latent factors by performing matrix multiplication in the function below:\n",
    "\n",
    "### Function explanation:\n",
    "\n",
    "`compute_recommendations_for_all_users`\n",
    "\n",
    "1. **Compute Predicted Ratings**:\n",
    "   - Use matrix multiplication to compute all predicted ratings based on the decomposed matrices U, Sigma, and Vt. Add back the user mean ratings to obtain the actual predicted ratings.\n",
    "\n",
    "2. **Mask Interacted Items**:\n",
    "   - Mask out items that users have already interacted with in the `user_item_matrix` by setting their predicted ratings to negative infinity. This ensures that these items are not recommended again.\n",
    "\n",
    "3. **Get Top Recommendations**:\n",
    "   - Sort the predicted ratings for each user in descending order to get the top recommendations. Extract the indices of the top items for each user.\n",
    "\n",
    "4. **Adjust Item IDs**:\n",
    "   - Adjust the indices to match item IDs by adding 1 to each index, as item IDs typically start from 1.\n",
    "\n",
    "5. **Create Recommendations Dictionary**:\n",
    "   - Create a dictionary `all_recommendations` mapping each user ID to a list of top recommended item IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recommendations_for_all_users(U, Sigma, Vt, user_means, user_ids, num_recommendations, user_item_matrix):\n",
    "    \"\"\"\n",
    "    Computes recommendations for all users based on the decomposed matrices from Singular Value Decomposition (SVD).\n",
    "\n",
    "    Parameters:\n",
    "    U: user matrix with values which represent the relation between the chosen latent factors, Users are the rows, matrix is orthonormal to Vt\n",
    "    Sigma: diagonal matrix where the chosen latent factors are in the diagonal line, ordered descendingly. \n",
    "    Vt: Item matrix with values which represent the relation between the chosen latent factors, Items are the columns, matrix is orthonormal to U\n",
    "\n",
    "    user_means (numpy.ndarray): Array containing mean ratings for each user.\n",
    "    user_ids (numpy.ndarray): Array containing user IDs.\n",
    "    num_recommendations (int): Number of recommendations to generate for each user.\n",
    "    user_item_matrix (numpy.ndarray): Matrix representing user-item interactions, where rows correspond to users and columns correspond to items.\n",
    "\n",
    "    Returns:\n",
    "    all_recommendations (dict): Dictionary mapping user IDs to lists of top recommended item IDs.\n",
    "    all_predicted_centered_ratings (numpy.ndarray): Array of predicted centered ratings for all users and items.\n",
    "                                                    Predicted ratings are centered by adding the mean rating for each user.\n",
    "                                                    Each row corresponds to a user, and each column corresponds to an item.\n",
    "    \"\"\"\n",
    "    # this line computes the predicted ratings by doing matrix multiplication with the decomposed matrices, it essentially builts up the original rating matrix with less features, thanks to SVD. The matrix multiplication(dot product) estimates the ratings with less features, meaning the ratings will be predicted with less features. By adding up the user means in the end, we will take the centering of the data into account as well, just like the original rating matrix and also account for variations in the rating.\n",
    "    all_predicted_ratings = np.dot(U, np.dot(Sigma, Vt)) + user_means[:, np.newaxis]\n",
    "\n",
    "    # mask out items already interacted with by users\n",
    "    all_predicted_ratings[user_item_matrix > 0] = -np.inf\n",
    "\n",
    "    # get top recommendations for each user\n",
    "    top_indices = np.argsort(all_predicted_ratings, axis=1)[:, ::-1]\n",
    "    top_recommendations = top_indices[:, :num_recommendations] + 1  # Adjust indices to match item IDs\n",
    "\n",
    "    # Create dictionary mapping user IDs to top recommended item IDs\n",
    "    all_recommendations = {user_id: top_items for user_id, top_items in zip(user_ids, top_recommendations)}\n",
    "\n",
    "    return all_recommendations, all_predicted_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I can be sure user_ids across functions are the same, because:** in the function compute_recommendations_for_all_users, the user IDs are used to retrieve the corresponding user indices within the centered matrix. Here's how:\n",
    "\n",
    "User IDs are used to retrieve the corresponding user indices using the user_id_to_index dictionary.\n",
    "The predicted ratings for each user are computed based on their index within the centered matrix.\n",
    "After computation, the recommendations and predicted ratings are stored and returned in a manner that preserves the correspondence between user IDs and their respective predictions.\n",
    "\n",
    "**Therefore**, when accessing the predictions or recommendations for a specific user ID from the returned results, you can be confident that they correspond to the same user ID in the original centered matrix.\n",
    "\n",
    "***********************\n",
    "\n",
    "Before parameter tuning, I will run the recommender system for the train and validation set and record some baseline performance. Root Mean Squared Error (RMSE) will be used as performance metric. \n",
    "\n",
    "- Reason behind this is the corresponding original and predicted centered ratings from the train_data and val_data will be used for measuring performance. A form of squared mean error is appropriate for such cases. Recall and precision revolve around ratings which are relevant to the user or not, which is difficult and subjective to identify within this model. \n",
    "  \n",
    "- Furthermore, RMSE will is expressed in the same units as the input data, making it easy to interpret for a user but a stakeholder as well.\n",
    "- RMSE tend to highlight differences more on smaller sample sizes than MSE would do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I make a baseline selection of latent factors\n",
    "num_latent_factors = 1\n",
    "\n",
    "# here I make a baseline selection of recommendations per user\n",
    "num_recommendations = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Netflix`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_train, user_id_dict_train, movie_id_dict_train, user_ids_train, movie_ids_train = create_user_item_matrix(train_data)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_train = list(set(user_ids_train))\n",
    "item_ids_train = list(set(movie_ids_train))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_train, user_means_train = center_data(user_item_matrix_train)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_TRAIN, Sigma_train, Vt_train = apply_svd(centered_user_item_matrix_train, num_latent_factors)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_train, all_predicted_centered_ratings_train = compute_recommendations_for_all_users(U_TRAIN, Sigma_train, Vt_train, user_means_train, user_ids_train, num_recommendations, user_item_matrix_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_val, user_id_dict_val, movie_id_dict_val, user_ids_val, movie_ids_val = create_user_item_matrix(val_data)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_val = list(set(user_ids_val))\n",
    "item_ids_val = list(set(movie_ids_val))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_val, user_means_val = center_data(user_item_matrix_val)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_VAL, Sigma_val, Vt_val = apply_svd(centered_user_item_matrix_val, num_latent_factors)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_val, all_predicted_centered_ratings_val = compute_recommendations_for_all_users(U_VAL, Sigma_val, Vt_val, user_means_val, user_ids_val, num_recommendations, user_item_matrix_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Movielens`\n",
    "\n",
    "Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_train2, user_id_dict_train2, movie_id_dict_train2, user_ids_train2, movie_ids_train2 = create_user_item_matrix(train_data2)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_train2 = list(set(user_ids_train2))\n",
    "item_ids_train2 = list(set(movie_ids_train2))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_train2, user_means_train2 = center_data(user_item_matrix_train2)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_TRAIN2, Sigma_train2, Vt_train2 = apply_svd(centered_user_item_matrix_train2, num_latent_factors)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_train2, all_predicted_centered_ratings_train2 = compute_recommendations_for_all_users(U_TRAIN2, Sigma_train2, Vt_train2, user_means_train2, user_ids_train2, num_recommendations, user_item_matrix_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_val2, user_id_dict_val2, movie_id_dict_val2, user_ids_val2, movie_ids_val2 = create_user_item_matrix(val_data2)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_val2 = list(set(user_ids_val2))\n",
    "item_ids_val2 = list(set(movie_ids_val2))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_val2, user_means_val2 = center_data(user_item_matrix_val2)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_val2, Sigma_val2, Vt_val2 = apply_svd(centered_user_item_matrix_val2, num_latent_factors)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_val2, all_predicted_centered_ratings_val2 = compute_recommendations_for_all_users(U_val2, Sigma_val2, Vt_val2, user_means_val2, user_ids_val2, num_recommendations, user_item_matrix_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use RMSE as performance metric, using the function below to compute it:**\n",
    "\n",
    "### Function explanation\n",
    "\n",
    "`compute_rmse`\n",
    "1. **Handle Implicit Ratings**: \n",
    "   - Convert `NaN` values in both `original_ratings` and `predicted_ratings` arrays to 0s. This is done using `np.nan_to_num()` function to ensure that non-rated items are treated as having a rating of 0 for comparison.\n",
    "   \n",
    "2. **Flatten Arrays**:\n",
    "   - Flatten both `original_ratings` and `predicted_ratings` arrays into 1D arrays to facilitate making masks.\n",
    "\n",
    "3. **Remove Unrated Items**:\n",
    "   - Create a mask to filter out entries where the original rating is 0 (unrated items). Only ratings for rated items are considered for RMSE calculation.\n",
    "\n",
    "4. **Compute Squared Differences**:\n",
    "   - Calculate the squared differences between original and predicted ratings for the rated items.\n",
    "\n",
    "5. **Compute Mean Squared Error (MSE)**:\n",
    "   - Compute the mean squared error (MSE) by averaging the squared differences.\n",
    "\n",
    "6. **Compute RMSE**:\n",
    "   - Compute the square root of the mean squared error to obtain the RMSE value, which indicates the average difference between the original and predicted ratings.\n",
    "\n",
    "7. **Return RMSE**:\n",
    "   - Return the computed RMSE value as the output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(original_ratings, predicted_ratings):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean Square Error (RMSE) between the original ratings and the predicted ratings. MovieIds a user has not interacted with is turned into 0 for now.\n",
    "\n",
    "    Parameters:\n",
    "    original_ratings (numpy.ndarray): Array containing the original ratings.\n",
    "    predicted_ratings (numpy.ndarray): Array containing the predicted ratings.\n",
    "\n",
    "    Returns:\n",
    "    float: The RMSE value.\n",
    "    \n",
    "    \"\"\"\n",
    "    # handle implicit ratings with 0s for now\n",
    "    original_ratings = np.nan_to_num(original_ratings, nan=0, posinf=0, neginf=0)\n",
    "    predicted_ratings = np.nan_to_num(predicted_ratings, nan=0, posinf=0, neginf=0)\n",
    "\n",
    "    # make 1d arrays by flattening them to be able to make masks\n",
    "    original_ratings_flat = original_ratings.flatten()\n",
    "    predicted_ratings_flat = predicted_ratings.flatten()\n",
    "    \n",
    "    # remove entries with no original rating (unrated items)\n",
    "    mask = original_ratings_flat != 0\n",
    "    original_ratings_flat = original_ratings_flat[mask]\n",
    "    predicted_ratings_flat = predicted_ratings_flat[mask]\n",
    "    \n",
    "    # Compute the squared differences\n",
    "    squared_diff = np.square(original_ratings_flat - predicted_ratings_flat)\n",
    "    \n",
    "    # Compute the mean squared error\n",
    "    mse = np.mean(squared_diff)\n",
    "    \n",
    "    # Compute the square root of the mean squared error to get RMSE\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on the FIRST training set\n",
    "train_rmse = compute_rmse(centered_user_item_matrix_train, all_predicted_centered_ratings_train)\n",
    "# Evaluate performance on the FIRST validation set\n",
    "val_rmse = compute_rmse(centered_user_item_matrix_val, all_predicted_centered_ratings_val)\n",
    "\n",
    "# Evaluate performance on the SECOND training set\n",
    "train_rmse2 = compute_rmse(centered_user_item_matrix_train2, all_predicted_centered_ratings_train2)\n",
    "# Evaluate performance on the SECOND validation set\n",
    "val_rmse2 = compute_rmse(centered_user_item_matrix_val2, all_predicted_centered_ratings_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Netflix` baseline performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training set: 0.5734839206350196\n",
      "RMSE on validation set: 0.8369768965842413\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE on training set:\", train_rmse)\n",
    "print(\"RMSE on validation set:\", val_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline findings:** MSE is higher on validation set than training set, indicating the model overfits to data it has already seen. I only set the number of latent factors at 1. It could mean that this amount of latent factorsm makes the model less flexible to account for new patterns in the validation data. Less latent factors also tend to be more sensitive to noise in the data.\n",
    "</BR>\n",
    "\n",
    "`Movielens` baseline performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training set: 0.3102560320623854\n",
      "RMSE on validation set: 0.7805121058103744\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE on training set:\", train_rmse2)\n",
    "print(\"RMSE on validation set:\", val_rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline performance:** \n",
    "\n",
    "Same goes for the movielens dataset: a single latent factor model has limited capacity to capture complex data structures the interactions might have, as well as the underlying structure of the interactions. The movielens dataset has even more noise in terms of null values, so that could be why the model is overfitting massively on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning\n",
    "\n",
    "`Netflix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Latent Factors: 1 | RMSE: 0.5734839206350196\n",
      "Num Latent Factors: 10 | RMSE: 0.5457853462705187\n",
      "Num Latent Factors: 25 | RMSE: 0.5383698369814283\n",
      "Num Latent Factors: 50 | RMSE: 0.5337370895901662\n",
      "Num Latent Factors: 250 | RMSE: 0.5309695396848648\n",
      "\n",
      "Best number of latent factors: 250 | Best RMSE: 0.5309695396848648\n"
     ]
    }
   ],
   "source": [
    "latent_factors_range2 = [1, 10, 25, 50, 250]\n",
    "rmse_values2 = []\n",
    "best_nlf_train = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for num_latent_factors in latent_factors_range2:\n",
    "    # perform only SVD and generationg of recommendations again while doing the loop, as the matrix and centered matrix will not change depending on the amount of latent factors    \n",
    "    # apply Singular Value Decomposition (SVD)\n",
    "    U_TRAIN, Sigma_train, Vt_train = apply_svd(centered_user_item_matrix_train, num_latent_factors)\n",
    "    \n",
    "    # compute recommendations for all users\n",
    "    all_recommendations_train, all_predicted_centered_ratings_train = compute_recommendations_for_all_users(U_TRAIN, Sigma_train, Vt_train, user_means_train, user_ids_train, num_recommendations, user_item_matrix_train)\n",
    "    \n",
    "    # compute Mean Squared Error (MSE) or Root Mean Square Error (RMSE) with the function I have written for it\n",
    "    rmse = compute_rmse(centered_user_item_matrix_train, all_predicted_centered_ratings_train)\n",
    "    \n",
    "    # append the RMSE value to the list\n",
    "    rmse_values2.append(rmse)\n",
    "\n",
    "        # Check if current k gives the best RMSE\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_nlf_train = num_latent_factors\n",
    "\n",
    "# print the result descendingly\n",
    "for i, num_latent_factors in enumerate(latent_factors_range2):\n",
    "    print(f\"Num Latent Factors: {num_latent_factors} | RMSE: {rmse_values2[i]}\")\n",
    "\n",
    "print(f\"\\nBest number of latent factors: {best_nlf_train} | Best RMSE: {best_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Movielens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Latent Factors: 1 | RMSE: 0.3102560320623854\n",
      "Num Latent Factors: 10 | RMSE: 0.307779326501893\n",
      "Num Latent Factors: 25 | RMSE: 0.30663196005867877\n",
      "Num Latent Factors: 50 | RMSE: 0.3064720326682541\n",
      "Num Latent Factors: 250 | RMSE: 0.3063378920193254\n",
      "\n",
      "Best number of latent factors: 250 | Best RMSE: 0.3063378920193254\n"
     ]
    }
   ],
   "source": [
    "latent_factors_range2 = [1, 10, 25, 50, 250]\n",
    "rmse_values2 = []\n",
    "best_nlf_train2 = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for num_latent_factors in latent_factors_range2:\n",
    "    # perform only SVD and generationg of recommendations again while doing the loop, as the matrix and centered matrix will not change depending on the amount of latent factors    \n",
    "    # apply Singular Value Decomposition (SVD)\n",
    "    U_TRAIN2, Sigma_train2, Vt_train2 = apply_svd(centered_user_item_matrix_train2, num_latent_factors)\n",
    "    \n",
    "    # compute recommendations for all users\n",
    "    all_recommendations_train2, all_predicted_centered_ratings_train2 = compute_recommendations_for_all_users(U_TRAIN2, Sigma_train2, Vt_train2, user_means_train2, user_ids_train2, num_recommendations, user_item_matrix_train2)\n",
    "    \n",
    "    # compute Mean Squared Error (MSE) or Root Mean Square Error (RMSE) with the function I have written for it\n",
    "    rmse = compute_rmse(centered_user_item_matrix_train2, all_predicted_centered_ratings_train2)\n",
    "    \n",
    "    # append the RMSE value to the list\n",
    "    rmse_values2.append(rmse)\n",
    "\n",
    "        # Check if current k gives the best RMSE\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_nlf_train2 = num_latent_factors\n",
    "\n",
    "# print the result descendingly\n",
    "for i, num_latent_factors in enumerate(latent_factors_range2):\n",
    "    print(f\"Num Latent Factors: {num_latent_factors} | RMSE: {rmse_values2[i]}\")\n",
    "\n",
    "print(f\"\\nBest number of latent factors: {best_nlf_train2} | Best RMSE: {best_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final predictions on test set:\n",
    "\n",
    "Best amount of latent factors are used to predict on test set:\n",
    "\n",
    "`Netflix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent_factors_test1 = best_nlf_train\n",
    "\n",
    "user_item_matrix_test1, user_id_dict_test1, movie_id_dict_test1, user_ids_test1, movie_ids_test1 = create_user_item_matrix(test_data)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_test1 = list(set(user_ids_test1))\n",
    "item_ids_test1 = list(set(movie_ids_test1))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_test1, user_means_test1 = center_data(user_item_matrix_test1)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_test1, Sigma_test1, Vt_test1 = apply_svd(centered_user_item_matrix_test1, num_latent_factors_test1)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_test1, all_predicted_centered_ratings_test1 = compute_recommendations_for_all_users(U_test1, Sigma_test1, Vt_test1, user_means_test1, user_ids_test1, num_recommendations, user_item_matrix_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test set: 0.8045300162758322\n",
      "RMSE on training set: 0.5734839206350196\n",
      "RMSE on val set: 0.8369768965842413\n"
     ]
    }
   ],
   "source": [
    "test_rmse = compute_rmse(centered_user_item_matrix_test1, all_predicted_centered_ratings_test1)\n",
    "print(\"RMSE on test set:\", test_rmse)\n",
    "print(\"RMSE on training set:\", train_rmse)\n",
    "print(\"RMSE on val set:\", val_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyper parameter tuning, the rmse went down a bit with relation to the validation data, meaning selecting a higher amount of latent factors were more able to catch the complexitiy of the data. In other words: the preferences of the users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Movielens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent_factors_test2 = best_nlf_train2\n",
    "\n",
    "user_item_matrix_test2, user_id_dict_test2, movie_id_dict_test2, user_ids_test2, movie_ids_test2 = create_user_item_matrix(test_data2)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_test2 = list(set(user_ids_test2))\n",
    "item_ids_test2 = list(set(movie_ids_test2))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_test2, user_means_test2 = center_data(user_item_matrix_test2)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_test2, Sigma_test2, Vt_test2 = apply_svd(centered_user_item_matrix_test2, num_latent_factors_test2)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_test2, all_predicted_centered_ratings_test2 = compute_recommendations_for_all_users(U_test2, Sigma_test2, Vt_test2, user_means_test2, user_ids_test2, num_recommendations, user_item_matrix_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test set: 0.7514817317977059\n",
      "RMSE on training set: 0.3102560320623854\n",
      "RMSE on val set: 0.7805121058103744\n"
     ]
    }
   ],
   "source": [
    "test_rmse2 = compute_rmse(centered_user_item_matrix_test2, all_predicted_centered_ratings_test2)\n",
    "print(\"RMSE on test set:\", test_rmse2)\n",
    "print(\"RMSE on training set:\", train_rmse2)\n",
    "print(\"RMSE on val set:\", val_rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting a higher amount of laten factors, the model starts to perform similarly to the training data, meaning the hyper parameter tuning worked succesfully to make the model perform better.\n",
    "\n",
    "250 laten factors did indeed make up for the overfitting behaviour on the validation data. More latent factors were able to catch the complexitiy of the movielens data more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall conclusion:\n",
    "\n",
    "SVD is a very quick way of creating a recommender system, resulting in quick calculations compared to KNN, which takes approx. 20 minutes for a fraction of the amount of reviews SVD can handle.\n",
    "\n",
    "The dimensionality reduction method, which runs through the veins of this model, really caused this model to catch the complexity of the data with less features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_ddb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
