{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # to make jupyter print all outputs, not just the last one\n",
    "from IPython.core.display import HTML # to pretty print pandas df and be able to copy them over (e.g. to ppt slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('cleaned/netflix_parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, I am going to make multiple random samples using random sample with replacement:\n",
    "\n",
    "*only samples from EDA which were able to catch differences during ANOVA are selected, and which showed similar distribution to the complete dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make samples using random sampling:\n",
    "sample_third = df.sample(frac=1/3, random_state=42)\n",
    "sample_quarter = df.sample(frac=1/4, random_state=42)\n",
    "sample_sixth = df.sample(frac=1/6, random_state=42)\n",
    "sample_tenth = df.sample(frac=1/10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering:\n",
    "\n",
    "Year and title will be dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>review_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>124</td>\n",
       "      <td>[{'date': 2002-04-01, 'rating': 3.0, 'userId':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>13434</td>\n",
       "      <td>[{'date': 2003-02-20, 'rating': 2.0, 'userId':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>4601</td>\n",
       "      <td>[{'date': 2003-12-27, 'rating': 3.0, 'userId':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>352</td>\n",
       "      <td>[{'date': 2003-10-13, 'rating': 4.0, 'userId':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>9560</td>\n",
       "      <td>[{'date': 2003-07-06, 'rating': 4.0, 'userId':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>4506</td>\n",
       "      <td>[{'date': 2005-07-06, 'rating': 4.0, 'userId':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>13656</td>\n",
       "      <td>[{'date': 2003-11-21, 'rating': 2.0, 'userId':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>9270</td>\n",
       "      <td>[{'date': 2002-09-26, 'rating': 4.0, 'userId':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>13459</td>\n",
       "      <td>[{'date': 2000-07-18, 'rating': 2.0, 'userId':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>4727</td>\n",
       "      <td>[{'date': 2004-09-16, 'rating': 4.0, 'userId':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieId                                        review_data\n",
       "123       124  [{'date': 2002-04-01, 'rating': 3.0, 'userId':...\n",
       "1193    13434  [{'date': 2003-02-20, 'rating': 2.0, 'userId':...\n",
       "462      4601  [{'date': 2003-12-27, 'rating': 3.0, 'userId':...\n",
       "351       352  [{'date': 2003-10-13, 'rating': 4.0, 'userId':...\n",
       "1058     9560  [{'date': 2003-07-06, 'rating': 4.0, 'userId':...\n",
       "...       ...                                                ...\n",
       "367      4506  [{'date': 2005-07-06, 'rating': 4.0, 'userId':...\n",
       "1415    13656  [{'date': 2003-11-21, 'rating': 2.0, 'userId':...\n",
       "768      9270  [{'date': 2002-09-26, 'rating': 4.0, 'userId':...\n",
       "1218    13459  [{'date': 2000-07-18, 'rating': 2.0, 'userId':...\n",
       "588      4727  [{'date': 2004-09-16, 'rating': 4.0, 'userId':...\n",
       "\n",
       "[143 rows x 2 columns]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflix_df = sample_tenth.drop(['year','title'],axis=1)\n",
    "netflix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's work with movies and reviews first, add other features later:\n",
    "\n",
    "Only rating and userId of dictionary will be kept to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df['review_data'] = netflix_df['review_data'].apply(lambda x: None if x is None else [{'userId': review['userId'], 'rating': review['rating']} for review in x if 'userId' in review and 'rating' in review])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's split our data into train, validation and test sets where we ensure that no training data flows into test and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and test sets, simultaneously ensuring no training data flows into validation or test data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the data to be split.\n",
    "    - train_ratio: float, ratio of the training set size to the total data size (default: 0.8).\n",
    "    - val_ratio: float, ratio of the validation set size to the total data size (default: 0.1).\n",
    "    - test_ratio: float, ratio of the test set size to the total data size (default: 0.1).\n",
    "\n",
    "    Returns:\n",
    "    - train_data: pandas DataFrame, training set.\n",
    "    - val_data: pandas DataFrame, validation set.\n",
    "    - test_data: pandas DataFrame, test set.\n",
    "    \"\"\"\n",
    "    # Shuffle the data\n",
    "    data_shuffled = data.sample(frac=1, random_state=42)\n",
    "\n",
    "    # Calculate the sizes of each set\n",
    "    num_samples = len(data_shuffled)\n",
    "    num_train = int(train_ratio * num_samples)\n",
    "    num_val = int(val_ratio * num_samples)\n",
    "    num_test = num_samples - num_train - num_val\n",
    "\n",
    "    # Split the data into train, validation, and test sets\n",
    "    train_data = data_shuffled[:num_train]\n",
    "    # Below is ensured the validation data and the test data starts after the indices which are already in the training data, ensuring that no training data will flow into validation of test data.\n",
    "    val_data = data_shuffled[num_train:num_train+num_val]\n",
    "    test_data = data_shuffled[num_train+num_val:]\n",
    "\n",
    "    # Reset index for each set\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    val_data.reset_index(drop=True, inplace=True)\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = train_val_test_split(netflix_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, let's define some function to make our life easer for the compatibility of more datasets. We gather unique item and user ids, create user-item matrix which will be centered, followed by performing SVD en making recommendations using the dot product between the decomposed matrices resulting from SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_matrix(train_test_val_set):\n",
    "    \"\"\"\n",
    "    Creates a user-item matrix from the provided dataset containing review data.\n",
    "\n",
    "    Parameters:\n",
    "    train_test_val_set (DataFrame): DataFrame containing review data with columns 'review_data',\n",
    "                                    which is a list of dictionaries with keys 'userId', 'rating',\n",
    "                                    and 'movieId'.\n",
    "\n",
    "    Returns:\n",
    "    user_item_matrix (numpy.ndarray): Matrix representing users' ratings for items (movies), the matrix is an NumPy array which contains lists of user-item interactions, meaning a user and their corresponding ratings to the movieIds.    \n",
    "    \n",
    "    user_id_dict (dict): Dictionary mapping user IDs to unique indices in the user-item matrix.\n",
    "    \n",
    "    movie_id_dict (dict): Dictionary mapping movie IDs to unique indices in the user-item matrix.\n",
    "    \n",
    "    user_ids (numpy.ndarray): Array containing user IDs corresponding to each rating in the matrix.\n",
    "    \n",
    "    movie_ids (numpy.ndarray): Array containing movie IDs corresponding to each rating in the matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    review_data = train_test_val_set['review_data'].values\n",
    "    user_ids = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data])\n",
    "    ratings = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data])\n",
    "    movieIds = np.concatenate([[movieId] * len(row) for movieId, row in zip(train_data['movieId'], review_data)])\n",
    "\n",
    "    # create dictionaries to map user IDs and movie IDs to unique indices to map over\n",
    "    user_id_dict = {user_id: index for index, user_id in enumerate(np.unique(user_ids))}\n",
    "    movie_id_dict = {movie_id: index for index, movie_id in enumerate(np.unique(movieIds))}\n",
    "\n",
    "    # initialize an empty user-item matrix\n",
    "    user_count = len(user_id_dict)\n",
    "    movie_count = len(movie_id_dict)\n",
    "    user_item_matrix = np.zeros((user_count, movie_count))\n",
    "\n",
    "    # populate the user-item matrix with ratings from netflix dataset\n",
    "    for i, (user_id, movie_id, rating) in enumerate(zip(user_ids, movieIds, ratings)):\n",
    "        user_index = user_id_dict[user_id]\n",
    "        movie_index = movie_id_dict[movie_id]\n",
    "        user_item_matrix[user_index, movie_index] = rating\n",
    "\n",
    "    return user_item_matrix, user_id_dict, movie_id_dict, user_ids, movieIds\n",
    "\n",
    "# I will center the data in the function below, to make the matrix more robust to handle variations in user ratings\n",
    "def center_data(user_item_matrix):\n",
    "    \"\"\"\n",
    "    Creates a centered matrix of the previously created user-item matrix\n",
    "\n",
    "    Parameters:\n",
    "    User-item matrix which is made a Numpy array with appended lists with ratings of each users of each item. Each position in each list corresponds to the same movieId. Datatype within the matrix is float64.\n",
    "\n",
    "    Return:\n",
    "    A centered user item matrix, where the row mean of each user is substracted from the initial ratings, to account for variations in ratings\n",
    "    \n",
    "    \"\"\"\n",
    "    user_means = np.mean(user_item_matrix, axis=1)\n",
    "    centered_user_item_matrix = user_item_matrix - user_means[:, np.newaxis]\n",
    "    return centered_user_item_matrix, user_means\n",
    "\n",
    "# I will decompose the user item matrix in this function using numpy\n",
    "def apply_svd(centered_user_item_matrix, num_latent_factors):\n",
    "    \"\"\"\n",
    "    Applies Singular Value Decomposition (SVD) to decompose the centered user-item matrix into three matrices:\n",
    "    U, Sigma, and Vt.\n",
    "\n",
    "    U: user matrix with values which represent the relation between the chosen latent factors, Users are the rows, matrix is orthonormal to Vt\n",
    "    Sigma: diagonal matrix where the chosen latent factors are in the diagonal line, ordered descendingly. \n",
    "    Vt: Item matrix with values which represent the relation between the chosen latent factors, Items are the columns, matrix is orthonormal to U\n",
    "\n",
    "    Parameters:\n",
    "    centered_user_item_matrix (numpy.ndarray): Centered user-item matrix to be decomposed.\n",
    "    num_latent_factors (int): Number of latent factors to retain in the decomposition.\n",
    "\n",
    "    Returns:\n",
    "    U (numpy.ndarray): Matrix representing the relationship between users and latent factors.\n",
    "    Sigma (numpy.ndarray): Diagonal matrix containing the singular values, representing the importance of each latent factor.\n",
    "    Vt (numpy.ndarray): Transpose of the matrix representing the relationship between items and latent factors.\n",
    "\n",
    "    \"\"\"\n",
    "    # U, sigma and Vt are created using the svd function from numpy\n",
    "    U, Sigma, Vt = np.linalg.svd(centered_user_item_matrix, full_matrices=False)\n",
    "    # set up sigma, which is the diagonal matrix from the decomposition\n",
    "    Sigma = np.diag(Sigma[:num_latent_factors])\n",
    "    # set up U and Vt which have to orthonormal to each other to ensure U represents each user and Vt represents each item, otherwise the total matrix would not add up.\n",
    "    U = U[:, :num_latent_factors]\n",
    "    Vt = Vt[:num_latent_factors, :]\n",
    "    return U, Sigma, Vt\n",
    "\n",
    "# I will compute recommendations by the dotproduct of the decomposed matrices from svd in this function\n",
    "def compute_recommendations_for_all_users(U, Sigma, Vt, user_means, user_ids, num_recommendations, user_item_matrix):\n",
    "    \"\"\"\n",
    "    Computes recommendations for all users based on the decomposed matrices from Singular Value Decomposition (SVD).\n",
    "\n",
    "    Parameters:\n",
    "    U: user matrix with values which represent the relation between the chosen latent factors, Users are the rows, matrix is orthonormal to Vt\n",
    "    Sigma: diagonal matrix where the chosen latent factors are in the diagonal line, ordered descendingly. \n",
    "    Vt: Item matrix with values which represent the relation between the chosen latent factors, Items are the columns, matrix is orthonormal to U\n",
    "\n",
    "    user_means (numpy.ndarray): Array containing mean ratings for each user.\n",
    "    user_ids (numpy.ndarray): Array containing user IDs.\n",
    "    num_recommendations (int): Number of recommendations to generate for each user.\n",
    "    user_item_matrix (numpy.ndarray): Matrix representing user-item interactions, where rows correspond to users and columns correspond to items.\n",
    "\n",
    "    Returns:\n",
    "    all_recommendations (dict): Dictionary mapping user IDs to lists of top recommended item IDs.\n",
    "    all_predicted_centered_ratings (numpy.ndarray): Array of predicted centered ratings for all users and items.\n",
    "                                                    Predicted ratings are centered by adding the mean rating for each user.\n",
    "                                                    Each row corresponds to a user, and each column corresponds to an item.\n",
    "    \"\"\"\n",
    "    all_recommendations = {}\n",
    "    all_predicted_centered_ratings = np.zeros_like(user_item_matrix)  # Initialize array for predicted ratings\n",
    "\n",
    "    user_id_to_index = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "\n",
    "    for user_id in user_ids:\n",
    "        user_index = user_id_to_index[user_id]\n",
    "        # Matrix multiplication between Sigma and Vt to reconstruct an item matrix with fewer features,\n",
    "        # followed by the dot product of U and the reconstruction of the item matrix.\n",
    "        user_ratings = np.dot(U[user_index, :], np.dot(Sigma, Vt)) + user_means[user_index]\n",
    "        # Mask out items the user has already interacted with\n",
    "        user_ratings[user_item_matrix[user_index, :] > 0] = -np.inf\n",
    "        # Sort the predicted ratings in descending order\n",
    "        top_indices = np.argsort(user_ratings)[::-1]\n",
    "        # Store the top predicted ratings for the current user\n",
    "        all_predicted_centered_ratings[user_index, :] = user_ratings\n",
    "        # Select the top 'num_recommendations' items as recommendations\n",
    "        top_items = top_indices[:num_recommendations] + 1\n",
    "        # Store the top recommended items for the current user\n",
    "        all_recommendations[user_id] = top_items\n",
    "\n",
    "    return all_recommendations, all_predicted_centered_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before parameter tuning, I will run the recommender system for the train and validation set and record some baseline performance. Root Mean Squared Error (RMSE) will be used as performance metric. \n",
    "\n",
    "- Reason behind this is the corresponding original and predicted centered ratings from the train_data and val_data will be used for measuring performance. A form of squared mean error is appropriate for such cases. Recall and precision revolve around ratings which are relevant to the user or not, which is difficult and subjective to identify within this model. \n",
    "  \n",
    "- Furthermore, RMSE will is expressed in the same units as the input data, making it easy to interpret for a user but a stakeholder as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I make a baseline selection of latent factors\n",
    "num_latent_factors = 4\n",
    "\n",
    "# here I make a baseline selection of recommendations per user\n",
    "num_recommendations = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_train, user_id_dict_train, movie_id_dict_train, user_ids_train, movie_ids_train = create_user_item_matrix(train_data)\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_train = list(set(user_ids_train))\n",
    "item_ids_train = list(set(movie_ids_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_train, user_means_train = center_data(user_item_matrix_train)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_TRAIN, Sigma_train, Vt_train = apply_svd(centered_user_item_matrix_train, num_latent_factors)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_train, all_predicted_centered_ratings_train = compute_recommendations_for_all_users(U_TRAIN, Sigma_train, Vt_train, user_means_train, user_ids_train, num_recommendations, user_item_matrix_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_val, user_id_dict_val, movie_id_dict_val, user_ids_val, movie_ids_val = create_user_item_matrix(val_data)\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_val = list(set(user_ids_val))\n",
    "item_ids_val = list(set(movie_ids_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_val, user_means_val = center_data(user_item_matrix_val)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_VAL, Sigma_val, Vt_val = apply_svd(centered_user_item_matrix_val, num_latent_factors)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_val, all_predicted_centered_ratings_val = compute_recommendations_for_all_users(U_VAL, Sigma_val, Vt_val, user_means_val, user_ids_val, num_recommendations, user_item_matrix_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.87719298, -0.12280702, -0.12280702, ..., -0.12280702,\n",
       "        -0.12280702, -0.12280702],\n",
       "       [ 3.92105263, -0.07894737, -0.07894737, ..., -0.07894737,\n",
       "        -0.07894737, -0.07894737],\n",
       "       [ 4.95614035, -0.04385965, -0.04385965, ..., -0.04385965,\n",
       "        -0.04385965, -0.04385965],\n",
       "       ...,\n",
       "       [-0.03508772, -0.03508772, -0.03508772, ..., -0.03508772,\n",
       "        -0.03508772, -0.03508772],\n",
       "       [-0.04385965, -0.04385965, -0.04385965, ..., -0.04385965,\n",
       "        -0.04385965, -0.04385965],\n",
       "       [ 2.97368421, -0.02631579, -0.02631579, ..., -0.02631579,\n",
       "        -0.02631579, -0.02631579]])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[       -inf,  0.1817749 ,  0.06512649, ...,  0.03168683,\n",
       "         0.014135  ,  0.47402227],\n",
       "       [       -inf, -0.05485963, -0.02833147, ..., -0.0061352 ,\n",
       "        -0.02866221,  0.28722274],\n",
       "       [       -inf,  0.08364216,  0.01483195, ..., -0.002823  ,\n",
       "        -0.01259906,  0.09270115],\n",
       "       ...,\n",
       "       [ 0.05052406, -0.09741868, -0.03215763, ..., -0.00310144,\n",
       "        -0.01486637,  0.17044946],\n",
       "       [ 0.21805977,  0.21440508,  0.01588955, ..., -0.03242487,\n",
       "        -0.03262818,  0.1972004 ],\n",
       "       [       -inf,  0.0501853 ,  0.00889917, ..., -0.0016938 ,\n",
       "        -0.00755943,  0.05562069]])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.21428571,  2.78571429, -0.21428571, ..., -0.21428571,\n",
       "        -0.21428571, -0.21428571],\n",
       "       [-0.21428571,  2.78571429, -0.21428571, ..., -0.21428571,\n",
       "        -0.21428571, -0.21428571],\n",
       "       [-0.28571429,  3.71428571, -0.28571429, ..., -0.28571429,\n",
       "        -0.28571429, -0.28571429],\n",
       "       ...,\n",
       "       [-0.28571429, -0.28571429, -0.28571429, ..., -0.28571429,\n",
       "        -0.28571429, -0.28571429],\n",
       "       [-0.28571429,  3.71428571, -0.28571429, ..., -0.28571429,\n",
       "        -0.28571429, -0.28571429],\n",
       "       [-0.35714286,  4.64285714, -0.35714286, ..., -0.35714286,\n",
       "        -0.35714286, -0.35714286]])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00776757,        -inf, -0.00862977, ...,  0.0139554 ,\n",
       "         0.00463803, -0.00947849],\n",
       "       [-0.00776757,        -inf, -0.00862977, ...,  0.0139554 ,\n",
       "         0.00463803, -0.00947849],\n",
       "       [-0.01035677,        -inf, -0.01150637, ...,  0.01860719,\n",
       "         0.00618404, -0.01263799],\n",
       "       ...,\n",
       "       [ 0.07378203,  0.00484103,  0.05702355, ..., -0.08011124,\n",
       "        -0.00482495,  0.04585588],\n",
       "       [-0.01035677,        -inf, -0.01150637, ...,  0.01860719,\n",
       "         0.00618404, -0.01263799],\n",
       "       [-0.01294596,        -inf, -0.01438296, ...,  0.02325899,\n",
       "         0.00773005, -0.01579749]])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All arrays are in the same format, meaning they are appropriately prepped for model evaluation\n"
     ]
    }
   ],
   "source": [
    "centered_user_item_matrix_train # variable for centered user item matrix train\n",
    "all_predicted_centered_ratings_train # variable for predicted centered user item matrix train\n",
    "centered_user_item_matrix_val # variable for centered user item matrix val\n",
    "all_predicted_centered_ratings_val # variable for predicted centered user item matrix val\n",
    "print('All arrays are in the same format, meaning they are appropriately prepped for model evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach Baseline model performance evaluation\n",
    "\n",
    "Yes, comparing the predicted matrix filled with user-item interactions to the original one from both the training and validation data is a common approach to evaluating the performance of a recommender system. Here's how you can do it:\n",
    "\n",
    "Calculate Predicted Ratings: After performing Singular Value Decomposition (SVD) on the training data and reconstructing the predicted matrix, you'll have a matrix of predicted ratings for all users and items. This matrix represents the system's predictions of how users would rate items.\n",
    "\n",
    "Compare with Original Training Data: Compare the predicted ratings in the reconstructed matrix with the original ratings in the training data. You can use metrics such as Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE) to quantify the differences between the predicted and actual ratings. Lower values indicate better performance.\n",
    "\n",
    "Compare with Validation Data: Similarly, compare the predicted ratings with the actual ratings in the validation data. This step evaluates how well the recommender system generalizes to unseen data. Again, use MAE or RMSE to assess the differences.\n",
    "\n",
    "Analyze Differences: Examine the differences between predicted and actual ratings to understand where the recommender system performs well and where it struggles. You can identify cases where the system makes accurate predictions and cases where it makes significant errors.\n",
    "\n",
    "Iterate and Improve: Based on the analysis, refine your recommender system to improve its performance. This could involve adjusting hyperparameters, using different algorithms, or incorporating additional features into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(original_ratings, predicted_ratings):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean Square Error (RMSE) between the original ratings and the predicted ratings. MovieIds a user has not interacted with is turned into 0 for now.\n",
    "\n",
    "    Parameters:\n",
    "    original_ratings (numpy.ndarray): Array containing the original ratings.\n",
    "    predicted_ratings (numpy.ndarray): Array containing the predicted ratings.\n",
    "\n",
    "    Returns:\n",
    "    float: The RMSE value.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Replace inf and -inf values with 0\n",
    "    original_ratings = np.nan_to_num(original_ratings, nan=0, posinf=0, neginf=0)\n",
    "    predicted_ratings = np.nan_to_num(predicted_ratings, nan=0, posinf=0, neginf=0)\n",
    "\n",
    "    # Flatten the matrices to 1D arrays\n",
    "    original_ratings_flat = original_ratings.flatten()\n",
    "    predicted_ratings_flat = predicted_ratings.flatten()\n",
    "    \n",
    "    # Remove entries with no original rating (unrated items)\n",
    "    mask = original_ratings_flat != 0\n",
    "    original_ratings_flat = original_ratings_flat[mask]\n",
    "    predicted_ratings_flat = predicted_ratings_flat[mask]\n",
    "    \n",
    "    # Compute the squared differences\n",
    "    squared_diff = np.square(original_ratings_flat - predicted_ratings_flat)\n",
    "    \n",
    "    # Compute the mean squared error\n",
    "    mse = np.mean(squared_diff)\n",
    "    \n",
    "    # # Compute the square root of the mean squared error to get RMSE\n",
    "    # rmse = np.sqrt(mse)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on training set: 0.3586038276856281\n",
      "MSE on validation set: 1.2250391032010444\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance on the training set\n",
    "train_mse = compute_mse(centered_user_item_matrix_train, all_predicted_centered_ratings_train)\n",
    "# print(\"RMSE on training set:\", train_rmse)\n",
    "print(\"MSE on training set:\", train_mse)\n",
    "\n",
    "# Evaluate performance on the validation set\n",
    "val_mse = compute_mse(centered_user_item_matrix_val, all_predicted_centered_ratings_val)\n",
    "# print(\"RMSE on validation set:\", val_rmse)\n",
    "print(\"MSE on validation set:\", val_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline findings:** MSE is lower on validation set than training set, indication the model overfits to data it has already seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare performance from centered and uncentered data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include time feature in matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data2, val_data2, test_data2 = train_val_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract titles, user IDs, ratings, and dates\n",
    "# review_data2 = train_data2['review_data'].values\n",
    "# user_ids2 = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data2])\n",
    "# ratings2 = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data2])\n",
    "# dates = np.concatenate([np.array([entry['date'] for entry in row], dtype='datetime64') for row in review_data2])\n",
    "# movieIds2 = np.concatenate([[movieId] * len(row) for movieId, row in zip(train_data['movieId'], review_data2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movieIds2\n",
    "# user_ids2\n",
    "# ratings\n",
    "# dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define function to convert datetime64[D] to months to normalize the dates\n",
    "# def get_month(date):\n",
    "#     month = (date.astype('datetime64[M]').astype(int) % 12) + 1\n",
    "#     return month\n",
    "\n",
    "# # Convert datetime64[D] dates to months\n",
    "# months = np.array([get_month(date) for date in dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dictionaries to map user IDs and movie IDs to unique indices\n",
    "# user_id_dict2 = {user_id: index for index, user_id in enumerate(np.unique(user_ids2))}\n",
    "# movie_id_dict2 = {movie_id: index for index, movie_id in enumerate(np.unique(movieIds2))}\n",
    "\n",
    "# # Initialize the user-item-time matrix\n",
    "# user_count2 = len(user_id_dict2)\n",
    "# movie_count2 = len(movie_id_dict2)\n",
    "# matrix_3d = np.zeros((user_count2, movie_count2, 2))\n",
    "\n",
    "# # Populate the matrix with ratings and normalized timestamps\n",
    "# for user_id, movie_id, rating, month in zip(user_ids2, movieIds2, ratings2, months):\n",
    "#     user_index = user_id_dict2[user_id]\n",
    "#     movie_index = movie_id_dict2[movie_id]\n",
    "#     matrix_3d[user_index, movie_index] = [rating, month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_values = np.unique(matrix_3d)\n",
    "# print(\"Unique values in the user-item-time matrix:\", unique_values)\n",
    "# matrix_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set is used because it does not allow for duplicates\n",
    "# user_ids2 = set()\n",
    "\n",
    "# # iterate over each row\n",
    "# for index, row in train_data2.iterrows():\n",
    "#     # iterate over each dictionary in the 'review_data' column of the current row\n",
    "#     for review_dict in row['review_data']:\n",
    "#         user_id = review_dict.get('userId')  # Extract userId from the dictionary\n",
    "#         if user_id:  # Check if userId exists\n",
    "#             user_ids2.add(int(user_id))  # Add user ID to the set\n",
    "\n",
    "# user_ids2 = list(user_ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # put movieids in set so duplicates are not allowed here either\n",
    "# item_ids2 = list(set(train_data2['movieId'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def center_data_3d(matrix_3d):\n",
    "#     # Calculate mean along the second axis (movies axis)\n",
    "#     user_means = np.mean(matrix_3d, axis=(1, 2), keepdims=True)\n",
    "#     # Subtract the mean from the original matrix\n",
    "#     centered_user_item_matrix_3d = matrix_3d - user_means\n",
    "#     return centered_user_item_matrix_3d, user_means\n",
    "\n",
    "# def apply_svd_3d(centered_user_item_matrix_3d, num_latent_factors):\n",
    "#     # Reshape the matrix to be 2D for SVD\n",
    "#     reshaped_matrix = centered_user_item_matrix_3d.reshape(centered_user_item_matrix_3d.shape[0], -1)\n",
    "#     # Perform SVD\n",
    "#     U, Sigma, Vt = np.linalg.svd(reshaped_matrix, full_matrices=False)\n",
    "#     # Keep only the specified number of latent factors\n",
    "#     U = U[:, :num_latent_factors]\n",
    "#     Sigma = np.diag(Sigma[:num_latent_factors])\n",
    "#     Vt = Vt[:num_latent_factors, :]\n",
    "#     return U, Sigma, Vt\n",
    "\n",
    "# def compute_recommendations_for_all_users_3d(U, Sigma, Vt, user_means, user_ids2, num_recommendations):\n",
    "#     all_recommendations = {}\n",
    "#     user_id_to_index = {user_id: idx for idx, user_id in enumerate(user_ids2)}\n",
    "#     for user_id in user_ids2:\n",
    "#         user_index = user_id_to_index[user_id]\n",
    "#         # Perform dot product for each user\n",
    "#         user_ratings = np.dot(U[user_index], np.dot(Sigma, Vt)) + user_means[user_index]\n",
    "#         # Exclude items already interacted with\n",
    "#         user_ratings[user_item_matrix[user_index] > 0] = -np.inf\n",
    "#         # Get indices of top recommendations\n",
    "#         top_indices = np.argsort(user_ratings)[::-1][:num_recommendations]\n",
    "#         top_items = top_indices + 1\n",
    "#         all_recommendations[user_id] = top_items\n",
    "#     return all_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select the Number of Latent Factors\n",
    "# num_latent_factors = 4 \n",
    "\n",
    "# # unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "# centered_user_item_matrix_3d, user_means = center_data_3d(matrix_3d)\n",
    "\n",
    "# # apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "# U, Sigma, Vt = apply_svd_3d(centered_user_item_matrix_3d, num_latent_factors)\n",
    "# U \n",
    "# Sigma\n",
    "# Vt\n",
    "\n",
    "# # define number of recommendations per user\n",
    "# num_recommendations = 4\n",
    "\n",
    "# # compute the recommendations\n",
    "# all_recommendations2 = compute_recommendations_for_all_users_3d(U, Sigma, Vt, user_means, user_ids, num_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_recommendations2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redundant but maybe useful for troubleshooting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract unique user IDs from the dataset\n",
    "# dataset_user_ids = set()\n",
    "# for review_list in train_data['review_data']:\n",
    "#     for review_dict in review_list:\n",
    "#         user_id = review_dict.get('userId')\n",
    "#         if user_id:\n",
    "#             dataset_user_ids.add(user_id)\n",
    "\n",
    "# # Check if all user IDs in the matrix are also in the dataset, and vice versa\n",
    "# user_ids_in_dataset_not_in_matrix = dataset_user_ids - set(user_ids)\n",
    "# user_ids_in_matrix_not_in_dataset = set(user_ids) - dataset_user_ids\n",
    "# len(user_ids_in_dataset_not_in_matrix)\n",
    "# len(user_ids_in_matrix_not_in_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User item matrix with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract review dates, user IDs, and ratings using NumPy\n",
    "# review_data = train_data['review_data'].values\n",
    "# user_ids = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data])\n",
    "# ratings = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data])\n",
    "\n",
    "# # Extract movie titles\n",
    "# movieIds = np.concatenate([[movieId] * len(row) for movieId, row in zip(train_data['movieId'], review_data)])\n",
    "\n",
    "# # Create a DataFrame with review dates, user IDs, ratings, and movie titles\n",
    "# review_df = pd.DataFrame({'userId': user_ids, 'rating': ratings, 'movieId': movieIds})\n",
    "\n",
    "# # Pivot review_df to get user-item matrix with reviews as values\n",
    "# user_item_matrix_df = review_df.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "# # Fill NaN values with 0\n",
    "# user_item_matrix_df = user_item_matrix_df.fillna(0)\n",
    "\n",
    "# # Convert DataFrame to NumPy array\n",
    "# user_item_matrix = user_item_matrix_df.to_numpy()\n",
    "\n",
    "# user_item_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract titles from dataframe, user IDs, and ratings from dictionary using NumPy\n",
    "# review_data = train_data['review_data'].values\n",
    "# user_ids = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data])\n",
    "# ratings = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data])\n",
    "# movieIds = np.concatenate([[movieId] * len(row) for movieId, row in zip(train_data['movieId'], review_data)])\n",
    "\n",
    "# # create dictionaries to map user IDs and movie IDs to unique indices to map over\n",
    "# user_id_dict = {user_id: index for index, user_id in enumerate(np.unique(user_ids))}\n",
    "# movie_id_dict = {movie_id: index for index, movie_id in enumerate(np.unique(movieIds))}\n",
    "\n",
    "# # initialize an empty user-item matrix\n",
    "# user_count = len(user_id_dict)\n",
    "# movie_count = len(movie_id_dict)\n",
    "# user_item_matrix = np.zeros((user_count, movie_count))\n",
    "\n",
    "# # populate the user-item matrix with ratings from netflix dataset\n",
    "# for i, (user_id, movie_id, rating) in enumerate(zip(user_ids, movieIds, ratings)):\n",
    "#     user_index = user_id_dict[user_id]\n",
    "#     movie_index = movie_id_dict[movie_id]\n",
    "#     user_item_matrix[user_index, movie_index] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set is used because it does not allow for duplicates\n",
    "# user_ids = set()\n",
    "\n",
    "# # iterate over each row\n",
    "# for index, row in train_data.iterrows():\n",
    "#     # iterate over each dictionary in the 'review_data' column of the current row\n",
    "#     for review_dict in row['review_data']:\n",
    "#         user_id = review_dict.get('userId')  # Extract userId from the dictionary\n",
    "#         if user_id:  # Check if userId exists\n",
    "#             user_ids.add(int(user_id))  # Add user ID to the set\n",
    "\n",
    "# user_ids = list(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # put movieids in set so duplicates are not allowed here either\n",
    "# item_ids = list(set(train_data['movieId'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function which computes recommendations and returns recommendations only, not the predicted ratings for every item after the svd matrix dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will compute recommendations for each user_id in the training/test/validation data by performing the dot product between the previous reviews in the matrix by the reconstruction of the user item matrix with less features\n",
    "# def compute_recommendations_for_all_users(U, Sigma, Vt, user_means, user_ids, num_recommendations):\n",
    "#     all_recommendations = {}\n",
    "#     user_id_to_index = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "#     for user_id in user_ids:\n",
    "#         user_index = user_id_to_index[user_id]\n",
    "#         # matrix multriplication between Sigma and Vt, to reconstruct an item matrix with less features, followed by the dot product of U and the reconstruction of item matrix. It essentially calculates the predicted ratings for each item for the given user based on their latent given ratings. user_means[user_index] rules out the items the user already interacted with.\n",
    "#         user_ratings = np.dot(U[user_index, :], np.dot(Sigma, Vt)) + user_means[user_index]\n",
    "#         user_ratings[user_item_matrix[user_index, :] > 0] = -np.inf\n",
    "#         top_indices = np.argsort(user_ratings)[::-1][:num_recommendations]\n",
    "#         top_items = top_indices + 1\n",
    "#         all_recommendations[user_id] = top_items\n",
    "#     return all_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old functions to extract unique user and item ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_unique_user_ids(train_test_val_set):\n",
    "#     user_ids = set()\n",
    "#     # iterate over each row\n",
    "#     for index, row in train_test_val_set.iterrows():\n",
    "#         # iterate over each dictionary in the 'review_data' column of the current row\n",
    "#         for review_dict in row['review_data']:\n",
    "#             user_id = review_dict.get('userId')  # Extract userId from the dictionary\n",
    "#             if user_id:  # Check if userId exists\n",
    "#                 user_ids.add(int(user_id))  # Add user ID to the set\n",
    "#     return list(user_ids)\n",
    "\n",
    "# def extract_unique_movie_ids(train_test_val_set):\n",
    "#     movie_ids = set(train_test_val_set['movieId'].unique())\n",
    "#     return list(movie_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_ddb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
