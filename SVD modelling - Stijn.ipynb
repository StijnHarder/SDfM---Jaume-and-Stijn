{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # to make jupyter print all outputs, not just the last one\n",
    "from IPython.core.display import HTML # to pretty print pandas df and be able to copy them over (e.g. to ppt slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('cleaned/strat_sample_netflix')\n",
    "df2 = pd.read_parquet('cleaned/strat_sample_movielens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netflix data stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 47289 reviews in the NETFLIX dataframe.\n",
      "There are 37760 unique users who have reviewed a movie.\n",
      "There are 16 movieIds in the NETFLIX dataset.\n",
      "A unique user places 1 reviews on average in the NETFLIX dataset.\n",
      "A movieId receives 2956 reviews on average in the NETFLIX dataset.\n"
     ]
    }
   ],
   "source": [
    "# Extract all user IDs from the 'review_data' column using list comprehension\n",
    "user_ids = [review_entry.get('userId') for row in df['review_data'] for review_entry in row if review_entry.get('userId')]\n",
    "\n",
    "# Count the number of unique users and reviews\n",
    "unique_users = set(user_ids)\n",
    "amount_of_reviews = len(user_ids)\n",
    "\n",
    "# Calculate averages\n",
    "avg_reviews_per_unique_user = amount_of_reviews / len(unique_users)\n",
    "avg_reviews_per_movie_id = amount_of_reviews / len(df)\n",
    "\n",
    "# Print results\n",
    "print(\"There are {} reviews in the NETFLIX dataframe.\".format(amount_of_reviews))\n",
    "print(\"There are {} unique users who have reviewed a movie.\".format(len(unique_users)))\n",
    "print(\"There are {} movieIds in the NETFLIX dataset.\".format(len(df)))\n",
    "print(\"A unique user places {} reviews on average in the NETFLIX dataset.\".format(round(avg_reviews_per_unique_user)))\n",
    "print(\"A movieId receives {} reviews on average in the NETFLIX dataset.\".format(round(avg_reviews_per_movie_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOVIELENS data stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6439 reviews in the MOVIELENS dataframe.\n",
      "There are 5660 unique users who have reviewed a movie.\n",
      "There are 150 movieIds in the MOVIELENS dataset.\n",
      "A unique user places 1 reviews on average in the MOVIELENS dataset.\n",
      "A movieId receives 43 reviews on average in the MOVIELENS dataset.\n"
     ]
    }
   ],
   "source": [
    "review_data2 = df2['review_data'].values\n",
    "review_data2 = [row for row in review_data2 if row is not None]\n",
    "user_ids2 = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data2])\n",
    "ratings2 = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data2])\n",
    "movieIds2 = np.concatenate([[movieId] * len(row) for movieId, row in zip(df['movieId'], review_data2)])\n",
    "\n",
    "avg_reviews_per_unique_user2 = len(ratings2) / len(np.unique(user_ids2))\n",
    "avg_reviews_per_movie_id2 = len(ratings2) / len(df2)\n",
    "\n",
    "# Print results\n",
    "print(\"There are {} reviews in the MOVIELENS dataframe.\".format(len(ratings2)))\n",
    "print(\"There are {} unique users who have reviewed a movie.\".format(len(np.unique(user_ids2))))\n",
    "print(\"There are {} movieIds in the MOVIELENS dataset.\".format(len(df2)))\n",
    "print(\"A unique user places {} reviews on average in the MOVIELENS dataset.\".format(round(avg_reviews_per_unique_user2)))\n",
    "print(\"A movieId receives {} reviews on average in the MOVIELENS dataset.\".format(round(avg_reviews_per_movie_id2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** the movielens can be considered as more sparse (meaning more null values) than Netflix as the amount of movieIds is much higher, but the avg. review per userId and per movieId is much lower. We will see in performance if this makes a difference.\n",
    "\n",
    "Define dataframe without date item in review_data dictionary to start with, later date features may be added for both Netflix and movielens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df = df[df.columns]\n",
    "netflix_df['review_data'] = netflix_df['review_data'].apply(lambda x: None if x is None else [{'userId': review['userId'], 'rating': review['rating']} for review in x if 'userId' in review and 'rating' in review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_df = df2[df2.columns]\n",
    "movielens_df['review_data'] = movielens_df['review_data'].apply(lambda x: None if x is None else [{'userId': review['userId'], 'rating': review['rating']} for review in x if 'userId' in review and 'rating' in review])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering and pre processing:\n",
    "\n",
    "Year and title will be dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_sample = df.drop(['year','title'],axis=1)\n",
    "movielens_sample = df2.drop(['year','title'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's split our data into train, validation and test sets where we ensure that no training data flows into test and validation sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Explanation\n",
    "\n",
    "`train_val_test_split`\n",
    "\n",
    "1. **Shuffle the Data**:\n",
    "   - The input data is shuffled using `data.sample(frac=1, random_state=42)` to ensure randomness. `random_state=42` ensures reproducibility.\n",
    "\n",
    "2. **Calculate Set Sizes**:\n",
    "   - The sizes of each set (training, validation, and test) are calculated based on the provided ratios and the total number of samples in the data.\n",
    "\n",
    "3. **Split the Data**:\n",
    "   - The shuffled data is split into three sets: training, validation, and test.\n",
    "   - The training data contains the first `num_train` samples.\n",
    "   - The validation data contains the next `num_val` samples, starting from the index immediately following the last training sample.\n",
    "   - The test data contains the remaining samples, starting from the index immediately following the last validation sample.\n",
    "\n",
    "4. **Reset Index**:\n",
    "   - The index of each set is reset to ensure that it starts from 0 and increases incrementally.\n",
    "\n",
    "5. **Return Sets**:\n",
    "   - The function returns the training, validation, and test sets as pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and test sets, simultaneously ensuring no training data flows into validation or test data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the data to be split.\n",
    "    - train_ratio: float, ratio of the training set size to the total data size (default: 0.8).\n",
    "    - val_ratio: float, ratio of the validation set size to the total data size (default: 0.1).\n",
    "    - test_ratio: float, ratio of the test set size to the total data size (default: 0.1).\n",
    "\n",
    "    Returns:\n",
    "    - train_data: pandas DataFrame, training set.\n",
    "    - val_data: pandas DataFrame, validation set.\n",
    "    - test_data: pandas DataFrame, test set.\n",
    "    \"\"\"\n",
    "    # Shuffle the data\n",
    "    data_shuffled = data.sample(frac=1, random_state=42)\n",
    "\n",
    "    # Calculate the sizes of each set\n",
    "    num_samples = len(data_shuffled)\n",
    "    num_train = int(train_ratio * num_samples)\n",
    "    num_val = int(val_ratio * num_samples)\n",
    "    num_test = num_samples - num_train - num_val\n",
    "\n",
    "    # Split the data into train, validation, and test sets\n",
    "    train_data = data_shuffled[:num_train]\n",
    "    # Below is ensured the validation data and the test data starts after the indices which are already in the training data, ensuring that no training data will flow into validation of test data.\n",
    "    val_data = data_shuffled[num_train:num_train+num_val]\n",
    "    test_data = data_shuffled[num_train+num_val:]\n",
    "\n",
    "    # Reset index for each set\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    val_data.reset_index(drop=True, inplace=True)\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data accordingly and take two differenct sample sizes to see what effect it has on model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netflix dataset splitting\n",
    "train_data, val_data, test_data = train_val_test_split(netflix_sample)\n",
    "train_data2, val_data2, test_data2 = train_val_test_split(movielens_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, let's define some function to make our life easer for the compatibility of more datasets. We gather unique item and user ids, create user-item matrix which will be centered, followed by performing SVD en making recommendations using the dot product between the decomposed matrices resulting from SVD:\n",
    "\n",
    "### Set-up user-item matrix\n",
    "First we will create a user-item matrix which records all the user-item interactions.\n",
    "\n",
    "\n",
    "### `create_user_item_matrix` Function Explanation\n",
    "\n",
    "### Steps:\n",
    "1. **Extract Review Data**:\n",
    "   - Extract the review data from the provided DataFrame, which contains user IDs, ratings, and movie IDs.\n",
    "\n",
    "2. **Create User and Movie IDs Arrays**:\n",
    "   - Extract user IDs, ratings, and movie IDs from the review data and concatenate them into separate arrays.\n",
    "   - Generate dictionaries to map user IDs and movie IDs to unique indices in the user-item matrix.\n",
    "\n",
    "3. **Initialize User-Item Matrix**:\n",
    "   - Determine the dimensions of the user-item matrix based on the number of unique users and movies.\n",
    "   - Initialize an empty user-item matrix filled with NaN values.\n",
    "\n",
    "4. **Populate User-Item Matrix**:\n",
    "   - Iterate through the review data and populate the user-item matrix with ratings.\n",
    "   - Map user and movie IDs to their corresponding indices in the matrix and insert the ratings.\n",
    "\n",
    "5. **Return Results**:\n",
    "   - Return the user-item matrix along with dictionaries mapping user and movie IDs to indices, and arrays containing user and movie IDs.\n",
    "  \n",
    "### Functions Used and Purpose:\n",
    "\n",
    "- **`np.concatenate()`**: Used to concatenate arrays containing user IDs, ratings, and movie IDs extracted from the review data.\n",
    "- **`enumerate()`**: Used to iterate over the unique user IDs and movie IDs and generate indices for mapping.\n",
    "- **`np.unique()`**: Used to find the unique user IDs and movie IDs in the review data.\n",
    "- **`np.full()`**: Used to initialize an empty user-item matrix filled with NaN values.\n",
    "- **`zip()`**: Used to iterate over multiple iterables simultaneously (user IDs, movie IDs, ratings).\n",
    "- **`enumerate()`**: Used to iterate over the indices and elements of an iterable (user IDs, movie IDs) simultaneously.\n",
    "- **Indexing and Slicing**: Used to access and modify elements in arrays and matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_matrix(train_test_val_set):\n",
    "    \"\"\"\n",
    "    Creates a user-item matrix from the provided dataset containing review data.\n",
    "\n",
    "    Parameters:\n",
    "    train_test_val_set (DataFrame): DataFrame containing review data with columns 'review_data',\n",
    "                                    which is a list of dictionaries with keys 'userId', 'rating',\n",
    "                                    and 'movieId'.\n",
    "\n",
    "    Returns:\n",
    "    user_item_matrix (numpy.ndarray): Matrix representing users' ratings for items (movies), the matrix is an NumPy array which contains lists of user-item interactions, meaning a user and their corresponding ratings to the movieIds.    \n",
    "    \n",
    "    user_id_dict (dict): Dictionary mapping user IDs to unique indices in the user-item matrix.\n",
    "    \n",
    "    movie_id_dict (dict): Dictionary mapping movie IDs to unique indices in the user-item matrix.\n",
    "    \n",
    "    user_ids (numpy.ndarray): Array containing user IDs corresponding to each rating in the matrix.\n",
    "    \n",
    "    movie_ids (numpy.ndarray): Array containing movie IDs corresponding to each rating in the matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    review_data = train_test_val_set['review_data'].values\n",
    "    user_ids = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data])\n",
    "    ratings = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data])\n",
    "    movieIds = np.concatenate([[movieId] * len(row) for movieId, row in zip(train_test_val_set['movieId'], review_data)])\n",
    "\n",
    "    # create dictionaries to map user IDs and movie IDs to unique indices to map over\n",
    "    user_id_dict = {user_id: index for index, user_id in enumerate(np.unique(user_ids))}\n",
    "    movie_id_dict = {movie_id: index for index, movie_id in enumerate(np.unique(movieIds))}\n",
    "\n",
    "    # initialize an empty user-item matrix\n",
    "    user_count = len(user_id_dict)\n",
    "    movie_count = len(movie_id_dict)\n",
    "    user_item_matrix = np.zeros((user_count, movie_count))\n",
    "\n",
    "    # populate the user-item matrix with ratings from netflix dataset\n",
    "    for i, (user_id, movie_id, rating) in enumerate(zip(user_ids, movieIds, ratings)):\n",
    "        user_index = user_id_dict[user_id]\n",
    "        movie_index = movie_id_dict[movie_id]\n",
    "        user_item_matrix[user_index, movie_index] = rating\n",
    "\n",
    "    return user_item_matrix, user_id_dict, movie_id_dict, user_ids, movieIds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To account for variantions in the ratings, let's center our rating matrix with the following function by substracting the user_mean of each row of the row total of each user (row_vector)\n",
    "### Function explanation\n",
    "`center_data`\n",
    "\n",
    "This function creates a centered matrix of the user-item matrix, which is commonly used in matrix factorization algorithms such as Singular Value Decomposition (SVD) and collaborative filtering.\n",
    "\n",
    "1. **Replace NaN Values**:\n",
    "   - Check for NaN values in the `user_item_matrix` and replace them with 0. This step is crucial for processing the data, as missing ratings are commonly represented as NaN.\n",
    "\n",
    "2. **Compute User Means**:\n",
    "   - Calculate the mean rating for each user across all items. This provides a measure of the average rating given by each user.\n",
    "\n",
    "3. **Center the Data**:\n",
    "   - Subtract the mean rating of each user (retrieved from the `user_means` array) from the corresponding ratings in the `user_item_matrix`. This centers the data by removing the user-specific variations in ratings.\n",
    "\n",
    "4. **Return Centered Matrix**:\n",
    "   - Return the centered user-item matrix, where each user's ratings are adjusted to reflect deviations from their mean rating.\n",
    "   - Also return the `user_means` array for potential future use or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_data(user_item_matrix):\n",
    "    \"\"\"\n",
    "    Creates a centered matrix of the previously created user-item matrix\n",
    "\n",
    "    Parameters:\n",
    "    User-item matrix which is made a Numpy array with appended lists with ratings of each users of each item. Each position in each list corresponds to the same movieId. Datatype within the matrix is float64. Each NaN value is converted to 0. In other words, for the time being the implicit feedback is converted to 0.\n",
    "\n",
    "    Return:\n",
    "    A centered user item matrix, where the row mean of each user is subtracted from the initial ratings, to account for variations in ratings\n",
    "    \n",
    "    \"\"\"\n",
    "    # Check for NaN values and replace them with 0\n",
    "    user_item_matrix[np.isnan(user_item_matrix)] = 0\n",
    "    \n",
    "    # Compute user means\n",
    "    user_means = np.mean(user_item_matrix, axis=1)\n",
    "    \n",
    "    # Center the data\n",
    "    centered_user_item_matrix = user_item_matrix - user_means[:, np.newaxis]\n",
    "    \n",
    "    return centered_user_item_matrix, user_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After centering the data, we would like to have less dimensions to work with, as it takes less time and computation power, and at the same does not lower the quality of the predictions.\n",
    "\n",
    "### Function explanation\n",
    "`apply_svd`\n",
    "\n",
    "1. **SVD Decomposition**:\n",
    "   - Apply the SVD decomposition to the `centered_user_item_matrix` using the `np.linalg.svd` function from NumPy. This results in three matrices: U, Sigma, and Vt.\n",
    "\n",
    "2. **Sigma Matrix Adjustment**:\n",
    "   - Set up the Sigma matrix by keeping only the first `num_latent_factors` singular values and forming a diagonal matrix with them.\n",
    "\n",
    "3. **Truncate U and Vt**:\n",
    "   - Keep only the columns corresponding to the first `num_latent_factors` in both U and Vt matrices. This ensures that U represents the relationship between users and latent factors, and Vt represents the relationship between items and latent factors.\n",
    "\n",
    "4. **Return Decomposed Matrices**:\n",
    "   - Return the decomposed matrices U, Sigma, and Vt, which capture the underlying structure of the user-item interactions in terms of latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will decompose the user item matrix in this function using numpy\n",
    "def apply_svd(centered_user_item_matrix, num_latent_factors):\n",
    "    \"\"\"\n",
    "    Applies Singular Value Decomposition (SVD) to decompose the centered user-item matrix into three matrices:\n",
    "    U, Sigma, and Vt.\n",
    "\n",
    "    U: user matrix with values which represent the relation between the chosen latent factors, Users are the rows, matrix is orthonormal to Vt\n",
    "    Sigma: diagonal matrix where the chosen latent factors are in the diagonal line, ordered descendingly. \n",
    "    Vt: Item matrix with values which represent the relation between the chosen latent factors, Items are the columns, matrix is orthonormal to U\n",
    "\n",
    "    Parameters:\n",
    "    centered_user_item_matrix (numpy.ndarray): Centered user-item matrix to be decomposed.\n",
    "    num_latent_factors (int): Number of latent factors to retain in the decomposition.\n",
    "\n",
    "    Returns:\n",
    "    U (numpy.ndarray): Matrix representing the relationship between users and latent factors.\n",
    "    Sigma (numpy.ndarray): Diagonal matrix containing the singular values, representing the importance of each latent factor.\n",
    "    Vt (numpy.ndarray): Transpose of the matrix representing the relationship between items and latent factors.\n",
    "\n",
    "    \"\"\"\n",
    "    # U, sigma and Vt are created using the svd function from numpy\n",
    "    U, Sigma, Vt = np.linalg.svd(centered_user_item_matrix, full_matrices=False)\n",
    "    # set up sigma, which is the diagonal matrix from the decomposition, where the dimensions are dependent on the amount of latent factors\n",
    "    Sigma = np.diag(Sigma[:num_latent_factors])\n",
    "    # set up U and Vt which have to orthonormal to each other to ensure U represents each user and Vt represents each item, otherwise the total matrix would not add up.\n",
    "    U = U[:, :num_latent_factors]\n",
    "    Vt = Vt[:num_latent_factors, :]\n",
    "    return U, Sigma, Vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our decomposed matrices from the original matrix, let's make rating predictions by building up the original matrix with the given amount of latent factors by performing matrix multiplication in the function below:\n",
    "\n",
    "### Function explanation:\n",
    "\n",
    "`compute_recommendations_for_all_users`\n",
    "\n",
    "1. **Compute Predicted Ratings**:\n",
    "   - Use matrix multiplication to compute all predicted ratings based on the decomposed matrices U, Sigma, and Vt. Add back the user mean ratings to obtain the actual predicted ratings.\n",
    "\n",
    "2. **Mask Interacted Items**:\n",
    "   - Mask out items that users have already interacted with in the `user_item_matrix` by setting their predicted ratings to negative infinity. This ensures that these items are not recommended again.\n",
    "\n",
    "3. **Get Top Recommendations**:\n",
    "   - Sort the predicted ratings for each user in descending order to get the top recommendations. Extract the indices of the top items for each user.\n",
    "\n",
    "4. **Adjust Item IDs**:\n",
    "   - Adjust the indices to match item IDs by adding 1 to each index, as item IDs typically start from 1.\n",
    "\n",
    "5. **Create Recommendations Dictionary**:\n",
    "   - Create a dictionary `all_recommendations` mapping each user ID to a list of top recommended item IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recommendations_for_all_users(U, Sigma, Vt, user_means, user_ids, num_recommendations, user_item_matrix):\n",
    "    \"\"\"\n",
    "    Computes recommendations for all users based on the decomposed matrices from Singular Value Decomposition (SVD).\n",
    "\n",
    "    Parameters:\n",
    "    U: user matrix with values which represent the relation between the chosen latent factors, Users are the rows, matrix is orthonormal to Vt\n",
    "    Sigma: diagonal matrix where the chosen latent factors are in the diagonal line, ordered descendingly. \n",
    "    Vt: Item matrix with values which represent the relation between the chosen latent factors, Items are the columns, matrix is orthonormal to U\n",
    "\n",
    "    user_means (numpy.ndarray): Array containing mean ratings for each user.\n",
    "    user_ids (numpy.ndarray): Array containing user IDs.\n",
    "    num_recommendations (int): Number of recommendations to generate for each user.\n",
    "    user_item_matrix (numpy.ndarray): Matrix representing user-item interactions, where rows correspond to users and columns correspond to items.\n",
    "\n",
    "    Returns:\n",
    "    all_recommendations (dict): Dictionary mapping user IDs to lists of top recommended item IDs.\n",
    "    all_predicted_centered_ratings (numpy.ndarray): Array of predicted centered ratings for all users and items.\n",
    "                                                    Predicted ratings are centered by adding the mean rating for each user.\n",
    "                                                    Each row corresponds to a user, and each column corresponds to an item.\n",
    "    \"\"\"\n",
    "    # this line computes the predicted ratings by doing matrix multiplication with the decomposed matrices, it essentially built up the original rating matrix with less features, thanks to SVD. The matrix multiplication(dot product) estimates the ratings with less features, meaning the ratings will also be predicted. By adding up the user means in the end, we will take the centering of the data into account as well, just like the original rating matrix.\n",
    "    all_predicted_ratings = np.dot(U, np.dot(Sigma, Vt)) + user_means[:, np.newaxis]\n",
    "\n",
    "    # mask out items already interacted with by users\n",
    "    all_predicted_ratings[user_item_matrix > 0] = -np.inf\n",
    "\n",
    "    # get top recommendations for each user\n",
    "    top_indices = np.argsort(all_predicted_ratings, axis=1)[:, ::-1]\n",
    "    top_recommendations = top_indices[:, :num_recommendations] + 1  # Adjust indices to match item IDs\n",
    "\n",
    "    # Create dictionary mapping user IDs to top recommended item IDs\n",
    "    all_recommendations = {user_id: top_items for user_id, top_items in zip(user_ids, top_recommendations)}\n",
    "\n",
    "    return all_recommendations, all_predicted_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I can be sure user_ids across functions are the same, because:** in the function compute_recommendations_for_all_users, the user IDs are used to retrieve the corresponding user indices within the centered matrix. Here's how:\n",
    "\n",
    "User IDs are used to retrieve the corresponding user indices using the user_id_to_index dictionary.\n",
    "The predicted ratings for each user are computed based on their index within the centered matrix.\n",
    "After computation, the recommendations and predicted ratings are stored and returned in a manner that preserves the correspondence between user IDs and their respective predictions.\n",
    "\n",
    "**Therefore**, when accessing the predictions or recommendations for a specific user ID from the returned results, you can be confident that they correspond to the same user ID in the original centered matrix.\n",
    "\n",
    "***********************\n",
    "\n",
    "Before parameter tuning, I will run the recommender system for the train and validation set and record some baseline performance. Root Mean Squared Error (RMSE) will be used as performance metric. \n",
    "\n",
    "- Reason behind this is the corresponding original and predicted centered ratings from the train_data and val_data will be used for measuring performance. A form of squared mean error is appropriate for such cases. Recall and precision revolve around ratings which are relevant to the user or not, which is difficult and subjective to identify within this model. \n",
    "  \n",
    "- Furthermore, RMSE will is expressed in the same units as the input data, making it easy to interpret for a user but a stakeholder as well.\n",
    "- RMSE tend to highlight differences more on smaller sample sizes than MSE would do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I make a baseline selection of latent factors\n",
    "num_latent_factors = 1\n",
    "\n",
    "# here I make a baseline selection of recommendations per user\n",
    "num_recommendations = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Netflix`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_train, user_id_dict_train, movie_id_dict_train, user_ids_train, movie_ids_train = create_user_item_matrix(train_data)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_train = list(set(user_ids_train))\n",
    "item_ids_train = list(set(movie_ids_train))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_train, user_means_train = center_data(user_item_matrix_train)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_TRAIN, Sigma_train, Vt_train = apply_svd(centered_user_item_matrix_train, num_latent_factors)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_train, all_predicted_centered_ratings_train = compute_recommendations_for_all_users(U_TRAIN, Sigma_train, Vt_train, user_means_train, user_ids_train, num_recommendations, user_item_matrix_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_val, user_id_dict_val, movie_id_dict_val, user_ids_val, movie_ids_val = create_user_item_matrix(val_data)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_val = list(set(user_ids_val))\n",
    "item_ids_val = list(set(movie_ids_val))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_val, user_means_val = center_data(user_item_matrix_val)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_VAL, Sigma_val, Vt_val = apply_svd(centered_user_item_matrix_val, num_latent_factors)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_val, all_predicted_centered_ratings_val = compute_recommendations_for_all_users(U_VAL, Sigma_val, Vt_val, user_means_val, user_ids_val, num_recommendations, user_item_matrix_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Movielens`\n",
    "\n",
    "Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_train2, user_id_dict_train2, movie_id_dict_train2, user_ids_train2, movie_ids_train2 = create_user_item_matrix(train_data2)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_train2 = list(set(user_ids_train2))\n",
    "item_ids_train2 = list(set(movie_ids_train2))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_train2, user_means_train2 = center_data(user_item_matrix_train2)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_TRAIN2, Sigma_train2, Vt_train2 = apply_svd(centered_user_item_matrix_train2, num_latent_factors)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_train2, all_predicted_centered_ratings_train2 = compute_recommendations_for_all_users(U_TRAIN2, Sigma_train2, Vt_train2, user_means_train2, user_ids_train2, num_recommendations, user_item_matrix_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_val2, user_id_dict_val2, movie_id_dict_val2, user_ids_val2, movie_ids_val2 = create_user_item_matrix(val_data2)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_val2 = list(set(user_ids_val2))\n",
    "item_ids_val2 = list(set(movie_ids_val2))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_val2, user_means_val2 = center_data(user_item_matrix_val2)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_val2, Sigma_val2, Vt_val2 = apply_svd(centered_user_item_matrix_val2, num_latent_factors)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_val2, all_predicted_centered_ratings_val2 = compute_recommendations_for_all_users(U_val2, Sigma_val2, Vt_val2, user_means_val2, user_ids_val2, num_recommendations, user_item_matrix_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use RMSE as performance metric, using the function below to compute it:**\n",
    "\n",
    "### Function explanation\n",
    "\n",
    "`compute_rmse`\n",
    "1. **Handle Implicit Ratings**: \n",
    "   - Convert `NaN` values in both `original_ratings` and `predicted_ratings` arrays to 0s. This is done using `np.nan_to_num()` function to ensure that non-rated items are treated as having a rating of 0 for comparison.\n",
    "   \n",
    "2. **Flatten Arrays**:\n",
    "   - Flatten both `original_ratings` and `predicted_ratings` arrays into 1D arrays to facilitate making masks.\n",
    "\n",
    "3. **Remove Unrated Items**:\n",
    "   - Create a mask to filter out entries where the original rating is 0 (unrated items). Only ratings for rated items are considered for RMSE calculation.\n",
    "\n",
    "4. **Compute Squared Differences**:\n",
    "   - Calculate the squared differences between original and predicted ratings for the rated items.\n",
    "\n",
    "5. **Compute Mean Squared Error (MSE)**:\n",
    "   - Compute the mean squared error (MSE) by averaging the squared differences.\n",
    "\n",
    "6. **Compute RMSE**:\n",
    "   - Compute the square root of the mean squared error to obtain the RMSE value, which indicates the average difference between the original and predicted ratings.\n",
    "\n",
    "7. **Return RMSE**:\n",
    "   - Return the computed RMSE value as the output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(original_ratings, predicted_ratings):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean Square Error (RMSE) between the original ratings and the predicted ratings. MovieIds a user has not interacted with is turned into 0 for now.\n",
    "\n",
    "    Parameters:\n",
    "    original_ratings (numpy.ndarray): Array containing the original ratings.\n",
    "    predicted_ratings (numpy.ndarray): Array containing the predicted ratings.\n",
    "\n",
    "    Returns:\n",
    "    float: The RMSE value.\n",
    "    \n",
    "    \"\"\"\n",
    "    # handle implicit ratings with 0s for now\n",
    "    original_ratings = np.nan_to_num(original_ratings, nan=0, posinf=0, neginf=0)\n",
    "    predicted_ratings = np.nan_to_num(predicted_ratings, nan=0, posinf=0, neginf=0)\n",
    "\n",
    "    # make 1d arrays by flattening them to be able to make masks\n",
    "    original_ratings_flat = original_ratings.flatten()\n",
    "    predicted_ratings_flat = predicted_ratings.flatten()\n",
    "    \n",
    "    # remove entries with no original rating (unrated items)\n",
    "    mask = original_ratings_flat != 0\n",
    "    original_ratings_flat = original_ratings_flat[mask]\n",
    "    predicted_ratings_flat = predicted_ratings_flat[mask]\n",
    "    \n",
    "    # Compute the squared differences\n",
    "    squared_diff = np.square(original_ratings_flat - predicted_ratings_flat)\n",
    "    \n",
    "    # Compute the mean squared error\n",
    "    mse = np.mean(squared_diff)\n",
    "    \n",
    "    # Compute the square root of the mean squared error to get RMSE\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SJYHa\\Desktop\\venv_ddb\\ddb_pymer4\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\SJYHa\\Desktop\\venv_ddb\\ddb_pymer4\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance on the FIRST training set\n",
    "train_rmse = compute_rmse(centered_user_item_matrix_train, all_predicted_centered_ratings_train)\n",
    "# Evaluate performance on the FIRST validation set\n",
    "val_rmse = compute_rmse(centered_user_item_matrix_val, all_predicted_centered_ratings_val)\n",
    "\n",
    "# Evaluate performance on the SECOND training set\n",
    "train_rmse2 = compute_rmse(centered_user_item_matrix_train2, all_predicted_centered_ratings_train2)\n",
    "# Evaluate performance on the SECOND validation set\n",
    "val_rmse2 = compute_rmse(centered_user_item_matrix_val2, all_predicted_centered_ratings_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Netflix` baseline performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training set: 1.0822219381958835\n",
      "RMSE on validation set: nan\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE on training set:\", train_rmse)\n",
    "print(\"RMSE on validation set:\", val_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline findings (sample_tenth):** MSE is lower on validation set than training set, indicating the model overfits to data it has already seen. Then again, it could also have something to do with the sample size. It choose the smaller one first. Having a smaller amount of reviews to rely on, could cause the model to perform worse on the validation data as the model has less reviews to work with.\n",
    "\n",
    "As in a SVD model the number of latent factors is the only tunable hyperparameter, I will perform hyperparameter tuning below on it below by looping throug different amounts of latent factors and computing their corresponding RMSE with relation to the orignal centered ratings.\n",
    "\n",
    "</BR>\n",
    "\n",
    "`Movielens` baseline performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training set: 0.337759066292501\n",
      "RMSE on validation set: 0.8186338848837073\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE on training set:\", train_rmse2)\n",
    "print(\"RMSE on validation set:\", val_rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline performance (increased sample size to a third of the dataset (sample_third)):** \n",
    "\n",
    "we can see two things:\n",
    "- the error stays more or less the same on the training and the validation sets \n",
    "- the difference between the error on the training and the validation set is substantially smaller\n",
    "\n",
    "**Conclusion:** the SVD recommender system is overfitting less to the training data when the sample size increases. This is a logic thing to happen, as the validation set becomes bigger, leading the RecSys to have more unseen reviews to work with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning\n",
    "\n",
    "`Netflix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Latent Factors: 1 | RMSE: 1.0822219381958835\n",
      "Num Latent Factors: 10 | RMSE: 1.003657412932295\n",
      "Num Latent Factors: 25 | RMSE: 1.0023398555063965\n",
      "Num Latent Factors: 50 | RMSE: 1.0023398555063965\n",
      "Num Latent Factors: 250 | RMSE: 1.0023398555063965\n",
      "\n",
      "Best number of latent factors: 25 | Best RMSE: 1.0023398555063965\n"
     ]
    }
   ],
   "source": [
    "latent_factors_range2 = [1, 10, 25, 50, 250]\n",
    "rmse_values2 = []\n",
    "best_nlf_train = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for num_latent_factors in latent_factors_range2:\n",
    "    # perform only SVD and generationg of recommendations again while doing the loop, as the matrix and centered matrix will not change depending on the amount of latent factors    \n",
    "    # apply Singular Value Decomposition (SVD)\n",
    "    U_TRAIN, Sigma_train, Vt_train = apply_svd(centered_user_item_matrix_train, num_latent_factors)\n",
    "    \n",
    "    # compute recommendations for all users\n",
    "    all_recommendations_train, all_predicted_centered_ratings_train = compute_recommendations_for_all_users(U_TRAIN, Sigma_train, Vt_train, user_means_train, user_ids_train, num_recommendations, user_item_matrix_train)\n",
    "    \n",
    "    # compute Mean Squared Error (MSE) or Root Mean Square Error (RMSE) with the function I have written for it\n",
    "    rmse = compute_rmse(centered_user_item_matrix_train, all_predicted_centered_ratings_train)\n",
    "    \n",
    "    # append the RMSE value to the list\n",
    "    rmse_values2.append(rmse)\n",
    "\n",
    "        # Check if current k gives the best RMSE\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_nlf_train = num_latent_factors\n",
    "\n",
    "# print the result descendingly\n",
    "for i, num_latent_factors in enumerate(latent_factors_range2):\n",
    "    print(f\"Num Latent Factors: {num_latent_factors} | RMSE: {rmse_values2[i]}\")\n",
    "\n",
    "print(f\"\\nBest number of latent factors: {best_nlf_train} | Best RMSE: {best_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Movielens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Latent Factors: 1 | RMSE: 0.337759066292501\n",
      "Num Latent Factors: 10 | RMSE: 0.3357396552833613\n",
      "Num Latent Factors: 25 | RMSE: 0.33513762431669625\n",
      "Num Latent Factors: 50 | RMSE: 0.33487058379424434\n",
      "Num Latent Factors: 250 | RMSE: 0.3344956269750285\n",
      "\n",
      "Best number of latent factors: 250 | Best RMSE: 0.3344956269750285\n"
     ]
    }
   ],
   "source": [
    "latent_factors_range2 = [1, 10, 25, 50, 250]\n",
    "rmse_values2 = []\n",
    "best_nlf_train2 = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for num_latent_factors in latent_factors_range2:\n",
    "    # perform only SVD and generationg of recommendations again while doing the loop, as the matrix and centered matrix will not change depending on the amount of latent factors    \n",
    "    # apply Singular Value Decomposition (SVD)\n",
    "    U_TRAIN2, Sigma_train2, Vt_train2 = apply_svd(centered_user_item_matrix_train2, num_latent_factors)\n",
    "    \n",
    "    # compute recommendations for all users\n",
    "    all_recommendations_train2, all_predicted_centered_ratings_train2 = compute_recommendations_for_all_users(U_TRAIN2, Sigma_train2, Vt_train2, user_means_train2, user_ids_train2, num_recommendations, user_item_matrix_train2)\n",
    "    \n",
    "    # compute Mean Squared Error (MSE) or Root Mean Square Error (RMSE) with the function I have written for it\n",
    "    rmse = compute_rmse(centered_user_item_matrix_train2, all_predicted_centered_ratings_train2)\n",
    "    \n",
    "    # append the RMSE value to the list\n",
    "    rmse_values2.append(rmse)\n",
    "\n",
    "        # Check if current k gives the best RMSE\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_nlf_train2 = num_latent_factors\n",
    "\n",
    "# print the result descendingly\n",
    "for i, num_latent_factors in enumerate(latent_factors_range2):\n",
    "    print(f\"Num Latent Factors: {num_latent_factors} | RMSE: {rmse_values2[i]}\")\n",
    "\n",
    "print(f\"\\nBest number of latent factors: {best_nlf_train2} | Best RMSE: {best_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final predictions on test set:\n",
    "\n",
    "Best amount of latent factors are used to predict on test set:\n",
    "\n",
    "`Netflix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent_factors_test1 = best_nlf_train\n",
    "\n",
    "user_item_matrix_test1, user_id_dict_test1, movie_id_dict_test1, user_ids_test1, movie_ids_test1 = create_user_item_matrix(test_data)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_test1 = list(set(user_ids_test1))\n",
    "item_ids_test1 = list(set(movie_ids_test1))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_test1, user_means_test1 = center_data(user_item_matrix_test1)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_test1, Sigma_test1, Vt_test1 = apply_svd(centered_user_item_matrix_test1, num_latent_factors_test1)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_test1, all_predicted_centered_ratings_test1 = compute_recommendations_for_all_users(U_test1, Sigma_test1, Vt_test1, user_means_test1, user_ids_test1, num_recommendations, user_item_matrix_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test set: 1.6994397797552048\n"
     ]
    }
   ],
   "source": [
    "test_rmse = compute_rmse(centered_user_item_matrix_test1, all_predicted_centered_ratings_test1)\n",
    "print(\"RMSE on test set:\", test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Movielens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent_factors_test2 = best_nlf_train2\n",
    "\n",
    "user_item_matrix_test2, user_id_dict_test2, movie_id_dict_test2, user_ids_test2, movie_ids_test2 = create_user_item_matrix(test_data2)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_test2 = list(set(user_ids_test2))\n",
    "item_ids_test2 = list(set(movie_ids_test2))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_test2, user_means_test2 = center_data(user_item_matrix_test2)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_test2, Sigma_test2, Vt_test2 = apply_svd(centered_user_item_matrix_test2, num_latent_factors_test2)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_test2, all_predicted_centered_ratings_test2 = compute_recommendations_for_all_users(U_test2, Sigma_test2, Vt_test2, user_means_test2, user_ids_test2, num_recommendations, user_item_matrix_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test set: 0.8950463716751159\n"
     ]
    }
   ],
   "source": [
    "test_rmse2 = compute_rmse(centered_user_item_matrix_test2, all_predicted_centered_ratings_test2)\n",
    "print(\"RMSE on test set:\", test_rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall conclusion:\n",
    "\n",
    "Hyperparametertuning and increasing the sample size did eventually not lead to a lower error on the test set. Maybe adding more features will help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter which of the iterated number of latent factors I put in, the RecSys model is not able to generalize to unseen data, meaning the model operates bad.\n",
    "\n",
    "### Overall conclusion:\n",
    "\n",
    "The current amount of features is not able to capture the complexity of the data, meaning it is not able to catch each preference of the user. Adding more features will maybe lead to a better result.\n",
    "\n",
    "\n",
    "**BY ADDING FILTER MATRIX IS LESS SPARSE, IMPLICIT FEEDBACK IS NOT TAKEN INTO ACCOUNT. BOTH ARE CASES WHICH COULD LEAD THE MODEL TO PERFORM BETTER. BUT THIS DOES NOT MEAN THE MODEL PERFORM WELL, AS IT ALSO NEEDS TO GENERATE RECOMMENDATIONS FOR NEW USERS (COLD START PROBLEM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include time feature in matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data2, val_data2, test_data2 = train_val_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract titles, user IDs, ratings, and dates\n",
    "# review_data2 = train_data2['review_data'].values\n",
    "# user_ids2 = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data2])\n",
    "# ratings2 = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data2])\n",
    "# dates = np.concatenate([np.array([entry['date'] for entry in row], dtype='datetime64') for row in review_data2])\n",
    "# movieIds2 = np.concatenate([[movieId] * len(row) for movieId, row in zip(train_data['movieId'], review_data2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movieIds2\n",
    "# user_ids2\n",
    "# ratings\n",
    "# dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define function to convert datetime64[D] to months to normalize the dates\n",
    "# def get_month(date):\n",
    "#     month = (date.astype('datetime64[M]').astype(int) % 12) + 1\n",
    "#     return month\n",
    "\n",
    "# # Convert datetime64[D] dates to months\n",
    "# months = np.array([get_month(date) for date in dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dictionaries to map user IDs and movie IDs to unique indices\n",
    "# user_id_dict2 = {user_id: index for index, user_id in enumerate(np.unique(user_ids2))}\n",
    "# movie_id_dict2 = {movie_id: index for index, movie_id in enumerate(np.unique(movieIds2))}\n",
    "\n",
    "# # Initialize the user-item-time matrix\n",
    "# user_count2 = len(user_id_dict2)\n",
    "# movie_count2 = len(movie_id_dict2)\n",
    "# matrix_3d = np.zeros((user_count2, movie_count2, 2))\n",
    "\n",
    "# # Populate the matrix with ratings and normalized timestamps\n",
    "# for user_id, movie_id, rating, month in zip(user_ids2, movieIds2, ratings2, months):\n",
    "#     user_index = user_id_dict2[user_id]\n",
    "#     movie_index = movie_id_dict2[movie_id]\n",
    "#     matrix_3d[user_index, movie_index] = [rating, month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_values = np.unique(matrix_3d)\n",
    "# print(\"Unique values in the user-item-time matrix:\", unique_values)\n",
    "# matrix_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set is used because it does not allow for duplicates\n",
    "# user_ids2 = set()\n",
    "\n",
    "# # iterate over each row\n",
    "# for index, row in train_data2.iterrows():\n",
    "#     # iterate over each dictionary in the 'review_data' column of the current row\n",
    "#     for review_dict in row['review_data']:\n",
    "#         user_id = review_dict.get('userId')  # Extract userId from the dictionary\n",
    "#         if user_id:  # Check if userId exists\n",
    "#             user_ids2.add(int(user_id))  # Add user ID to the set\n",
    "\n",
    "# user_ids2 = list(user_ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # put movieids in set so duplicates are not allowed here either\n",
    "# item_ids2 = list(set(train_data2['movieId'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def center_data_3d(matrix_3d):\n",
    "#     # Calculate mean along the second axis (movies axis)\n",
    "#     user_means = np.mean(matrix_3d, axis=(1, 2), keepdims=True)\n",
    "#     # Subtract the mean from the original matrix\n",
    "#     centered_user_item_matrix_3d = matrix_3d - user_means\n",
    "#     return centered_user_item_matrix_3d, user_means\n",
    "\n",
    "# def apply_svd_3d(centered_user_item_matrix_3d, num_latent_factors):\n",
    "#     # Reshape the matrix to be 2D for SVD\n",
    "#     reshaped_matrix = centered_user_item_matrix_3d.reshape(centered_user_item_matrix_3d.shape[0], -1)\n",
    "#     # Perform SVD\n",
    "#     U, Sigma, Vt = np.linalg.svd(reshaped_matrix, full_matrices=False)\n",
    "#     # Keep only the specified number of latent factors\n",
    "#     U = U[:, :num_latent_factors]\n",
    "#     Sigma = np.diag(Sigma[:num_latent_factors])\n",
    "#     Vt = Vt[:num_latent_factors, :]\n",
    "#     return U, Sigma, Vt\n",
    "\n",
    "# def compute_recommendations_for_all_users_3d(U, Sigma, Vt, user_means, user_ids2, num_recommendations):\n",
    "#     all_recommendations = {}\n",
    "#     user_id_to_index = {user_id: idx for idx, user_id in enumerate(user_ids2)}\n",
    "#     for user_id in user_ids2:\n",
    "#         user_index = user_id_to_index[user_id]\n",
    "#         # Perform dot product for each user\n",
    "#         user_ratings = np.dot(U[user_index], np.dot(Sigma, Vt)) + user_means[user_index]\n",
    "#         # Exclude items already interacted with\n",
    "#         user_ratings[user_item_matrix[user_index] > 0] = -np.inf\n",
    "#         # Get indices of top recommendations\n",
    "#         top_indices = np.argsort(user_ratings)[::-1][:num_recommendations]\n",
    "#         top_items = top_indices + 1\n",
    "#         all_recommendations[user_id] = top_items\n",
    "#     return all_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select the Number of Latent Factors\n",
    "# num_latent_factors = 4 \n",
    "\n",
    "# # unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "# centered_user_item_matrix_3d, user_means = center_data_3d(matrix_3d)\n",
    "\n",
    "# # apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "# U, Sigma, Vt = apply_svd_3d(centered_user_item_matrix_3d, num_latent_factors)\n",
    "# U \n",
    "# Sigma\n",
    "# Vt\n",
    "\n",
    "# # define number of recommendations per user\n",
    "# num_recommendations = 4\n",
    "\n",
    "# # compute the recommendations\n",
    "# all_recommendations2 = compute_recommendations_for_all_users_3d(U, Sigma, Vt, user_means, user_ids, num_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_recommendations2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redundant but maybe useful for troubleshooting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract unique user IDs from the dataset\n",
    "# dataset_user_ids = set()\n",
    "# for review_list in train_data['review_data']:\n",
    "#     for review_dict in review_list:\n",
    "#         user_id = review_dict.get('userId')\n",
    "#         if user_id:\n",
    "#             dataset_user_ids.add(user_id)\n",
    "\n",
    "# # Check if all user IDs in the matrix are also in the dataset, and vice versa\n",
    "# user_ids_in_dataset_not_in_matrix = dataset_user_ids - set(user_ids)\n",
    "# user_ids_in_matrix_not_in_dataset = set(user_ids) - dataset_user_ids\n",
    "# len(user_ids_in_dataset_not_in_matrix)\n",
    "# len(user_ids_in_matrix_not_in_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User item matrix with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract review dates, user IDs, and ratings using NumPy\n",
    "# review_data = train_data['review_data'].values\n",
    "# user_ids = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data])\n",
    "# ratings = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data])\n",
    "\n",
    "# # Extract movie titles\n",
    "# movieIds = np.concatenate([[movieId] * len(row) for movieId, row in zip(train_data['movieId'], review_data)])\n",
    "\n",
    "# # Create a DataFrame with review dates, user IDs, ratings, and movie titles\n",
    "# review_df = pd.DataFrame({'userId': user_ids, 'rating': ratings, 'movieId': movieIds})\n",
    "\n",
    "# # Pivot review_df to get user-item matrix with reviews as values\n",
    "# user_item_matrix_df = review_df.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "# # Fill NaN values with 0\n",
    "# user_item_matrix_df = user_item_matrix_df.fillna(0)\n",
    "\n",
    "# # Convert DataFrame to NumPy array\n",
    "# user_item_matrix = user_item_matrix_df.to_numpy()\n",
    "\n",
    "# user_item_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract titles from dataframe, user IDs, and ratings from dictionary using NumPy\n",
    "# review_data = train_data['review_data'].values\n",
    "# user_ids = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data])\n",
    "# ratings = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data])\n",
    "# movieIds = np.concatenate([[movieId] * len(row) for movieId, row in zip(train_data['movieId'], review_data)])\n",
    "\n",
    "# # create dictionaries to map user IDs and movie IDs to unique indices to map over\n",
    "# user_id_dict = {user_id: index for index, user_id in enumerate(np.unique(user_ids))}\n",
    "# movie_id_dict = {movie_id: index for index, movie_id in enumerate(np.unique(movieIds))}\n",
    "\n",
    "# # initialize an empty user-item matrix\n",
    "# user_count = len(user_id_dict)\n",
    "# movie_count = len(movie_id_dict)\n",
    "# user_item_matrix = np.zeros((user_count, movie_count))\n",
    "\n",
    "# # populate the user-item matrix with ratings from netflix dataset\n",
    "# for i, (user_id, movie_id, rating) in enumerate(zip(user_ids, movieIds, ratings)):\n",
    "#     user_index = user_id_dict[user_id]\n",
    "#     movie_index = movie_id_dict[movie_id]\n",
    "#     user_item_matrix[user_index, movie_index] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set is used because it does not allow for duplicates\n",
    "# user_ids = set()\n",
    "\n",
    "# # iterate over each row\n",
    "# for index, row in train_data.iterrows():\n",
    "#     # iterate over each dictionary in the 'review_data' column of the current row\n",
    "#     for review_dict in row['review_data']:\n",
    "#         user_id = review_dict.get('userId')  # Extract userId from the dictionary\n",
    "#         if user_id:  # Check if userId exists\n",
    "#             user_ids.add(int(user_id))  # Add user ID to the set\n",
    "\n",
    "# user_ids = list(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # put movieids in set so duplicates are not allowed here either\n",
    "# item_ids = list(set(train_data['movieId'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function which computes recommendations and returns recommendations only, not the predicted ratings for every item after the svd matrix dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will compute recommendations for each user_id in the training/test/validation data by performing the dot product between the previous reviews in the matrix by the reconstruction of the user item matrix with less features\n",
    "# def compute_recommendations_for_all_users(U, Sigma, Vt, user_means, user_ids, num_recommendations):\n",
    "#     all_recommendations = {}\n",
    "#     user_id_to_index = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "#     for user_id in user_ids:\n",
    "#         user_index = user_id_to_index[user_id]\n",
    "#         # matrix multriplication between Sigma and Vt, to reconstruct an item matrix with less features, followed by the dot product of U and the reconstruction of item matrix. It essentially calculates the predicted ratings for each item for the given user based on their latent given ratings. user_means[user_index] rules out the items the user already interacted with.\n",
    "#         user_ratings = np.dot(U[user_index, :], np.dot(Sigma, Vt)) + user_means[user_index]\n",
    "#         user_ratings[user_item_matrix[user_index, :] > 0] = -np.inf\n",
    "#         top_indices = np.argsort(user_ratings)[::-1][:num_recommendations]\n",
    "#         top_items = top_indices + 1\n",
    "#         all_recommendations[user_id] = top_items\n",
    "#     return all_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old functions to extract unique user and item ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_unique_user_ids(train_test_val_set):\n",
    "#     user_ids = set()\n",
    "#     # iterate over each row\n",
    "#     for index, row in train_test_val_set.iterrows():\n",
    "#         # iterate over each dictionary in the 'review_data' column of the current row\n",
    "#         for review_dict in row['review_data']:\n",
    "#             user_id = review_dict.get('userId')  # Extract userId from the dictionary\n",
    "#             if user_id:  # Check if userId exists\n",
    "#                 user_ids.add(int(user_id))  # Add user ID to the set\n",
    "#     return list(user_ids)\n",
    "\n",
    "# def extract_unique_movie_ids(train_test_val_set):\n",
    "#     movie_ids = set(train_test_val_set['movieId'].unique())\n",
    "#     return list(movie_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For looking at arrays if they are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_recommendations_train\n",
    "# all_recommendations_val\n",
    "# centered_user_item_matrix_train # variable for centered user item matrix train\n",
    "# all_predicted_centered_ratings_train # variable for predicted centered user item matrix train\n",
    "# centered_user_item_matrix_val # variable for centered user item matrix val\n",
    "# all_predicted_centered_ratings_val # variable for predicted centered user item matrix val\n",
    "# print('All arrays are in the same format, meaning they are appropriately prepped for model evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old code which works for computing recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I will compute recommendations by the dotproduct of the decomposed matrices from svd in this function\n",
    "# def compute_recommendations_for_all_users(U, Sigma, Vt, user_means, user_ids, num_recommendations, user_item_matrix):\n",
    "#     \"\"\"\n",
    "#     Computes recommendations for all users based on the decomposed matrices from Singular Value Decomposition (SVD).\n",
    "\n",
    "#     Parameters:\n",
    "#     U: user matrix with values which represent the relation between the chosen latent factors, Users are the rows, matrix is orthonormal to Vt\n",
    "#     Sigma: diagonal matrix where the chosen latent factors are in the diagonal line, ordered descendingly. \n",
    "#     Vt: Item matrix with values which represent the relation between the chosen latent factors, Items are the columns, matrix is orthonormal to U\n",
    "\n",
    "#     user_means (numpy.ndarray): Array containing mean ratings for each user.\n",
    "#     user_ids (numpy.ndarray): Array containing user IDs.\n",
    "#     num_recommendations (int): Number of recommendations to generate for each user.\n",
    "#     user_item_matrix (numpy.ndarray): Matrix representing user-item interactions, where rows correspond to users and columns correspond to items.\n",
    "\n",
    "#     Returns:\n",
    "#     all_recommendations (dict): Dictionary mapping user IDs to lists of top recommended item IDs.\n",
    "#     all_predicted_centered_ratings (numpy.ndarray): Array of predicted centered ratings for all users and items.\n",
    "#                                                     Predicted ratings are centered by adding the mean rating for each user.\n",
    "#                                                     Each row corresponds to a user, and each column corresponds to an item.\n",
    "#     \"\"\"\n",
    "#     all_recommendations = {}\n",
    "#     all_predicted_centered_ratings = np.zeros_like(user_item_matrix)  # Initialize array for predicted ratings\n",
    "\n",
    "#     user_id_to_index = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "\n",
    "#     for user_id in user_ids:\n",
    "#         user_index = user_id_to_index[user_id]\n",
    "#         # the matrix multiplication below gives me the new ratings based on the dot product of corresponding row(user_index and therefore userId) with the recomposed item matrix with less features. Essentially the ratings are computed again with less features.\n",
    "#         user_ratings = np.dot(U[user_index, :], np.dot(Sigma, Vt)) + user_means[user_index]\n",
    "#         # masks out items greater than 0, meaning an item a user already interacted with, the rest is filled with infinite values to show that a user did not interact with that item yet\n",
    "#         user_ratings[user_item_matrix[user_index, :] > 0] = -np.inf\n",
    "#         # sort values descendingly, to identify top rated item per user\n",
    "#         top_indices = np.argsort(user_ratings)[::-1]\n",
    "#         # store the top predicted ratings for the current user\n",
    "#         all_predicted_centered_ratings[user_index, :] = user_ratings\n",
    "#         # select the top 'num_recommendations' items as recommendations\n",
    "#         top_items = top_indices[:num_recommendations] + 1\n",
    "#         # store the top recommended items for the current user\n",
    "#         all_recommendations[user_id] = top_items\n",
    "\n",
    "#     return all_recommendations, all_predicted_centered_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old code for centering data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I will center the data in the function below, to make the matrix more robust to handle variations in user ratings\n",
    "# def center_data(user_item_matrix):\n",
    "#     \"\"\"\n",
    "#     Creates a centered matrix of the previously created user-item matrix\n",
    "\n",
    "#     Parameters:\n",
    "#     User-item matrix which is made a Numpy array with appended lists with ratings of each users of each item. Each position in each list corresponds to the same movieId. Datatype within the matrix is float64.\n",
    "\n",
    "#     Return:\n",
    "#     A centered user item matrix, where the row mean of each user is substracted from the initial ratings, to account for variations in ratings\n",
    "    \n",
    "#     \"\"\"\n",
    "#     user_means = np.mean(user_item_matrix, axis=1)\n",
    "#     centered_user_item_matrix = user_item_matrix - user_means[:, np.newaxis]\n",
    "#     return centered_user_item_matrix, user_means"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_ddb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
