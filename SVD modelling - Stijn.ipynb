{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # to make jupyter print all outputs, not just the last one\n",
    "from IPython.core.display import HTML # to pretty print pandas df and be able to copy them over (e.g. to ppt slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('cleaned/netflix_parquet')\n",
    "df2 = pd.read_parquet('cleaned/movielens_parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netflix data stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9998038 reviews in the NETFLIX dataframe.\n",
      "There are 447835 unique users who have reviewed a movie.\n",
      "There are 1962 movieIds in the NETFLIX dataset.\n",
      "A unique user places 22 reviews on average in the NETFLIX dataset.\n",
      "A movieId receives 5096 reviews on average in the NETFLIX dataset.\n"
     ]
    }
   ],
   "source": [
    "unique_users = set()  # Using a set to store unique user IDs\n",
    "\n",
    "# Iterate over each row\n",
    "for index, row in df.iterrows():\n",
    "    # Iterate over each dictionary in the 'review_data' column of the current row\n",
    "    for review_entry in row['review_data']:\n",
    "        user_id = review_entry.get('userId')  # Extracting 'userId' from the dictionary\n",
    "        if user_id:  # Check if 'userId' exists in the dictionary\n",
    "            unique_users.add(user_id)  # Add 'userId' to the set of unique user IDs\n",
    "\n",
    "amount_of_reviews = list()  # Using a set to store unique user IDs\n",
    "\n",
    "# Iterate over each row\n",
    "for index, row in df.iterrows():\n",
    "    # Iterate over each dictionary in the 'review_data' column of the current row\n",
    "    for review_entry in row['review_data']:\n",
    "        user_id = review_entry.get('userId')  # Extracting 'userId' from the dictionary\n",
    "        if user_id:  # Check if 'userId' exists in the dictionary\n",
    "            amount_of_reviews.append(user_id)  # Add 'userId' to the set of unique user IDs\n",
    "\n",
    "print(\"There are {} reviews in the NETFLIX dataframe.\".format(len(amount_of_reviews)))\n",
    "print(\"There are {} unique users who have reviewed a movie.\".format(len(unique_users)))\n",
    "print(\"There are {} movieIds in the NETFLIX dataset.\".format(len(df)))\n",
    "avg_reviews_per_uniqueUser = len(amount_of_reviews) / len(unique_users)\n",
    "print(\"A unique user places {} reviews on average in the NETFLIX dataset.\".format(round(avg_reviews_per_uniqueUser)))\n",
    "avg_reviews_per_movieId = len(amount_of_reviews) / len(df)\n",
    "print(\"A movieId receives {} reviews on average in the NETFLIX dataset.\".format(round(avg_reviews_per_movieId)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movielens data stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Iterate over each row\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df2\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Iterate over each dictionary in the 'review_data' column of the current row\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreview_entry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreview_entry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muserId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Extracting 'userId' from the dictionary\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Check if 'userId' exists in the dictionary\u001b[39;49;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "unique_users = set()  # Using a set to store unique user IDs\n",
    "\n",
    "# Iterate over each row\n",
    "for index, row in df2.iterrows():\n",
    "    # Iterate over each dictionary in the 'review_data' column of the current row\n",
    "    for review_entry in row['review_data']:\n",
    "        user_id = review_entry.get('userId')  # Extracting 'userId' from the dictionary\n",
    "        if user_id:  # Check if 'userId' exists in the dictionary\n",
    "            unique_users.add(user_id)  # Add 'userId' to the set of unique user IDs\n",
    "\n",
    "amount_of_reviews = list()  # Using a set to store unique user IDs\n",
    "\n",
    "# Iterate over each row\n",
    "for index, row in df2.iterrows():\n",
    "    # Iterate over each dictionary in the 'review_data' column of the current row\n",
    "    for review_entry in row['review_data']:\n",
    "        user_id = review_entry.get('userId')  # Extracting 'userId' from the dictionary\n",
    "        if user_id:  # Check if 'userId' exists in the dictionary\n",
    "            amount_of_reviews.append(user_id)  # Add 'userId' to the set of unique user IDs\n",
    "\n",
    "print(\"There are {} reviews in the MOVIELENS dataframe.\".format(len(amount_of_reviews)))\n",
    "print(\"There are {} unique users who have reviewed a movie.\".format(len(unique_users)))\n",
    "print(\"There are {} movieIds in the MOVIELENS dataset.\".format(len(df)))\n",
    "avg_reviews_per_uniqueUser = len(amount_of_reviews) / len(unique_users)\n",
    "print(\"A unique user places {} reviews on average in the MOVIELENS dataset.\".format(round(avg_reviews_per_uniqueUser)))\n",
    "avg_reviews_per_movieId = len(amount_of_reviews) / len(df)\n",
    "print(\"A movieId receives {} reviews on average in the MOVIELENS dataset.\".format(round(avg_reviews_per_movieId)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** the movielens can be considered as more sparse (meaning more null values) than Netflix as the amount of movieIds is much higher, but the avg. review per userId and per movieId is much lower. We will see in performance if this makes a difference.\n",
    "\n",
    "Define dataframe without date item in review_data dictionary to start with, later date features may be added for both Netflix and movielens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df = df[df.columns]\n",
    "netflix_df['review_data'] = netflix_df['review_data'].apply(lambda x: None if x is None else [{'userId': review['userId'], 'rating': review['rating']} for review in x if 'userId' in review and 'rating' in review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_df = df2[df2.columns]\n",
    "movielens_df['review_data'] = movielens_df['review_data'].apply(lambda x: None if x is None else [{'userId': review['userId'], 'rating': review['rating']} for review in x if 'userId' in review and 'rating' in review])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, I am going to make multiple random samples using random sample with replacement:\n",
    "\n",
    "*only samples from EDA which were able to catch differences during ANOVA are selected, and which showed similar distribution to the complete dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make NETFLIX samples using random sampling:\n",
    "sample_third = netflix_df.sample(frac=1/3, random_state=42)\n",
    "sample_quarter = netflix_df.sample(frac=1/4, random_state=42)\n",
    "sample_sixth = netflix_df.sample(frac=1/6, random_state=42)\n",
    "sample_tenth = netflix_df.sample(frac=1/10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make MOVIELENS samples using random sampling:\n",
    "sample_third_ml = movielens_df.sample(frac=1/3, random_state=42)\n",
    "sample_quarter_ml = movielens_df.sample(frac=1/4, random_state=42)\n",
    "sample_sixth_ml = movielens_df.sample(frac=1/6, random_state=42)\n",
    "sample_tenth_ml = movielens_df.sample(frac=1/10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering and pre processing:\n",
    "\n",
    "Year and title will be dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tenth = sample_tenth.drop(['year','title'],axis=1)\n",
    "sample_third = sample_third.drop(['year','title'],axis=1)\n",
    "sample_third_ml = sample_third_ml[['movieId','review_data']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's split our data into train, validation and test sets where we ensure that no training data flows into test and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and test sets, simultaneously ensuring no training data flows into validation or test data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the data to be split.\n",
    "    - train_ratio: float, ratio of the training set size to the total data size (default: 0.8).\n",
    "    - val_ratio: float, ratio of the validation set size to the total data size (default: 0.1).\n",
    "    - test_ratio: float, ratio of the test set size to the total data size (default: 0.1).\n",
    "\n",
    "    Returns:\n",
    "    - train_data: pandas DataFrame, training set.\n",
    "    - val_data: pandas DataFrame, validation set.\n",
    "    - test_data: pandas DataFrame, test set.\n",
    "    \"\"\"\n",
    "    # Shuffle the data\n",
    "    data_shuffled = data.sample(frac=1, random_state=42)\n",
    "\n",
    "    # Calculate the sizes of each set\n",
    "    num_samples = len(data_shuffled)\n",
    "    num_train = int(train_ratio * num_samples)\n",
    "    num_val = int(val_ratio * num_samples)\n",
    "    num_test = num_samples - num_train - num_val\n",
    "\n",
    "    # Split the data into train, validation, and test sets\n",
    "    train_data = data_shuffled[:num_train]\n",
    "    # Below is ensured the validation data and the test data starts after the indices which are already in the training data, ensuring that no training data will flow into validation of test data.\n",
    "    val_data = data_shuffled[num_train:num_train+num_val]\n",
    "    test_data = data_shuffled[num_train+num_val:]\n",
    "\n",
    "    # Reset index for each set\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    val_data.reset_index(drop=True, inplace=True)\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data accordingly and take two differenct sample sizes to see what effect it has on model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netflix dataset splitting\n",
    "train_data, val_data, test_data = train_val_test_split(sample_tenth)\n",
    "train_data2, val_data2, test_data2 = train_val_test_split(sample_third)\n",
    "# movielens dataset splitting\n",
    "train_data3, val_data3, test_data3 = train_val_test_split(sample_third_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, let's define some function to make our life easer for the compatibility of more datasets. We gather unique item and user ids, create user-item matrix which will be centered, followed by performing SVD en making recommendations using the dot product between the decomposed matrices resulting from SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_matrix(train_test_val_set):\n",
    "    \"\"\"\n",
    "    Creates a user-item matrix from the provided dataset containing review data.\n",
    "\n",
    "    Parameters:\n",
    "    train_test_val_set (DataFrame): DataFrame containing review data with columns 'review_data',\n",
    "                                    which is a list of dictionaries with keys 'userId', 'rating',\n",
    "                                    and 'movieId'.\n",
    "\n",
    "    Returns:\n",
    "    user_item_matrix (numpy.ndarray): Matrix representing users' ratings for items (movies), the matrix is an NumPy array which contains lists of user-item interactions, meaning a user and their corresponding ratings to the movieIds.    \n",
    "    \n",
    "    user_id_dict (dict): Dictionary mapping user IDs to unique indices in the user-item matrix.\n",
    "    \n",
    "    movie_id_dict (dict): Dictionary mapping movie IDs to unique indices in the user-item matrix.\n",
    "    \n",
    "    user_ids (numpy.ndarray): Array containing user IDs corresponding to each rating in the matrix.\n",
    "    \n",
    "    movie_ids (numpy.ndarray): Array containing movie IDs corresponding to each rating in the matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    review_data = train_test_val_set['review_data'].values\n",
    "    user_ids = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data])\n",
    "    ratings = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data])\n",
    "    movieIds = np.concatenate([[movieId] * len(row) for movieId, row in zip(train_test_val_set['movieId'], review_data)])\n",
    "\n",
    "    # create dictionaries to map user IDs and movie IDs to unique indices to map over\n",
    "    user_id_dict = {user_id: index for index, user_id in enumerate(np.unique(user_ids))}\n",
    "    movie_id_dict = {movie_id: index for index, movie_id in enumerate(np.unique(movieIds))}\n",
    "\n",
    "    # initialize an empty user-item matrix\n",
    "    user_count = len(user_id_dict)\n",
    "    movie_count = len(movie_id_dict)\n",
    "    user_item_matrix = np.zeros((user_count, movie_count))\n",
    "\n",
    "    # populate the user-item matrix with ratings from netflix dataset\n",
    "    for i, (user_id, movie_id, rating) in enumerate(zip(user_ids, movieIds, ratings)):\n",
    "        user_index = user_id_dict[user_id]\n",
    "        movie_index = movie_id_dict[movie_id]\n",
    "        user_item_matrix[user_index, movie_index] = rating\n",
    "\n",
    "    return user_item_matrix, user_id_dict, movie_id_dict, user_ids, movieIds\n",
    "\n",
    "# # I will center the data in the function below, to make the matrix more robust to handle variations in user ratings\n",
    "# def center_data(user_item_matrix):\n",
    "#     \"\"\"\n",
    "#     Creates a centered matrix of the previously created user-item matrix\n",
    "\n",
    "#     Parameters:\n",
    "#     User-item matrix which is made a Numpy array with appended lists with ratings of each users of each item. Each position in each list corresponds to the same movieId. Datatype within the matrix is float64.\n",
    "\n",
    "#     Return:\n",
    "#     A centered user item matrix, where the row mean of each user is substracted from the initial ratings, to account for variations in ratings\n",
    "    \n",
    "#     \"\"\"\n",
    "#     user_means = np.mean(user_item_matrix, axis=1)\n",
    "#     centered_user_item_matrix = user_item_matrix - user_means[:, np.newaxis]\n",
    "#     return centered_user_item_matrix, user_means\n",
    "\n",
    "def center_data(user_item_matrix):\n",
    "    \"\"\"\n",
    "    Creates a centered matrix of the previously created user-item matrix\n",
    "\n",
    "    Parameters:\n",
    "    User-item matrix which is made a Numpy array with appended lists with ratings of each users of each item. Each position in each list corresponds to the same movieId. Datatype within the matrix is float64. Each NaN value is converted to 0. In other words, for the time being the implicit feedback is converted to 0.\n",
    "\n",
    "    Return:\n",
    "    A centered user item matrix, where the row mean of each user is subtracted from the initial ratings, to account for variations in ratings\n",
    "    \n",
    "    \"\"\"\n",
    "    # Check for NaN values and replace them with 0\n",
    "    user_item_matrix[np.isnan(user_item_matrix)] = 0\n",
    "    \n",
    "    # Compute user means\n",
    "    user_means = np.mean(user_item_matrix, axis=1)\n",
    "    \n",
    "    # Center the data\n",
    "    centered_user_item_matrix = user_item_matrix - user_means[:, np.newaxis]\n",
    "    \n",
    "    return centered_user_item_matrix, user_means\n",
    "\n",
    "# I will decompose the user item matrix in this function using numpy\n",
    "def apply_svd(centered_user_item_matrix, num_latent_factors):\n",
    "    \"\"\"\n",
    "    Applies Singular Value Decomposition (SVD) to decompose the centered user-item matrix into three matrices:\n",
    "    U, Sigma, and Vt.\n",
    "\n",
    "    U: user matrix with values which represent the relation between the chosen latent factors, Users are the rows, matrix is orthonormal to Vt\n",
    "    Sigma: diagonal matrix where the chosen latent factors are in the diagonal line, ordered descendingly. \n",
    "    Vt: Item matrix with values which represent the relation between the chosen latent factors, Items are the columns, matrix is orthonormal to U\n",
    "\n",
    "    Parameters:\n",
    "    centered_user_item_matrix (numpy.ndarray): Centered user-item matrix to be decomposed.\n",
    "    num_latent_factors (int): Number of latent factors to retain in the decomposition.\n",
    "\n",
    "    Returns:\n",
    "    U (numpy.ndarray): Matrix representing the relationship between users and latent factors.\n",
    "    Sigma (numpy.ndarray): Diagonal matrix containing the singular values, representing the importance of each latent factor.\n",
    "    Vt (numpy.ndarray): Transpose of the matrix representing the relationship between items and latent factors.\n",
    "\n",
    "    \"\"\"\n",
    "    # U, sigma and Vt are created using the svd function from numpy\n",
    "    U, Sigma, Vt = np.linalg.svd(centered_user_item_matrix, full_matrices=False)\n",
    "    # set up sigma, which is the diagonal matrix from the decomposition\n",
    "    Sigma = np.diag(Sigma[:num_latent_factors])\n",
    "    # set up U and Vt which have to orthonormal to each other to ensure U represents each user and Vt represents each item, otherwise the total matrix would not add up.\n",
    "    U = U[:, :num_latent_factors]\n",
    "    Vt = Vt[:num_latent_factors, :]\n",
    "    return U, Sigma, Vt\n",
    "\n",
    "# I will compute recommendations by the dotproduct of the decomposed matrices from svd in this function\n",
    "def compute_recommendations_for_all_users(U, Sigma, Vt, user_means, user_ids, num_recommendations, user_item_matrix):\n",
    "    \"\"\"\n",
    "    Computes recommendations for all users based on the decomposed matrices from Singular Value Decomposition (SVD).\n",
    "\n",
    "    Parameters:\n",
    "    U: user matrix with values which represent the relation between the chosen latent factors, Users are the rows, matrix is orthonormal to Vt\n",
    "    Sigma: diagonal matrix where the chosen latent factors are in the diagonal line, ordered descendingly. \n",
    "    Vt: Item matrix with values which represent the relation between the chosen latent factors, Items are the columns, matrix is orthonormal to U\n",
    "\n",
    "    user_means (numpy.ndarray): Array containing mean ratings for each user.\n",
    "    user_ids (numpy.ndarray): Array containing user IDs.\n",
    "    num_recommendations (int): Number of recommendations to generate for each user.\n",
    "    user_item_matrix (numpy.ndarray): Matrix representing user-item interactions, where rows correspond to users and columns correspond to items.\n",
    "\n",
    "    Returns:\n",
    "    all_recommendations (dict): Dictionary mapping user IDs to lists of top recommended item IDs.\n",
    "    all_predicted_centered_ratings (numpy.ndarray): Array of predicted centered ratings for all users and items.\n",
    "                                                    Predicted ratings are centered by adding the mean rating for each user.\n",
    "                                                    Each row corresponds to a user, and each column corresponds to an item.\n",
    "    \"\"\"\n",
    "    all_recommendations = {}\n",
    "    all_predicted_centered_ratings = np.zeros_like(user_item_matrix)  # Initialize array for predicted ratings\n",
    "\n",
    "    user_id_to_index = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "\n",
    "    for user_id in user_ids:\n",
    "        user_index = user_id_to_index[user_id]\n",
    "        # the matrix multiplication below gives me the new ratings based on the dot product of corresponding row(user_index and therefore userId) with the recomposed item matrix with less features. Essentially the ratings are computed again with less features.\n",
    "        user_ratings = np.dot(U[user_index, :], np.dot(Sigma, Vt)) + user_means[user_index]\n",
    "        # masks out items greater than 0, meaning an item a user already interacted with, the rest is filled with infinite values to show that a user did not interact with that item yet\n",
    "        user_ratings[user_item_matrix[user_index, :] > 0] = -np.inf\n",
    "        # sort values descendingly, to identify top rated item per user\n",
    "        top_indices = np.argsort(user_ratings)[::-1]\n",
    "        # store the top predicted ratings for the current user\n",
    "        all_predicted_centered_ratings[user_index, :] = user_ratings\n",
    "        # select the top 'num_recommendations' items as recommendations\n",
    "        top_items = top_indices[:num_recommendations] + 1\n",
    "        # store the top recommended items for the current user\n",
    "        all_recommendations[user_id] = top_items\n",
    "\n",
    "    return all_recommendations, all_predicted_centered_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I can be sure user_ids across functions are the same, because:** in the function compute_recommendations_for_all_users, the user IDs are used to retrieve the corresponding user indices within the centered matrix. Here's how:\n",
    "\n",
    "User IDs are used to retrieve the corresponding user indices using the user_id_to_index dictionary.\n",
    "The predicted ratings for each user are computed based on their index within the centered matrix.\n",
    "After computation, the recommendations and predicted ratings are stored and returned in a manner that preserves the correspondence between user IDs and their respective predictions.\n",
    "\n",
    "**Therefore**, when accessing the predictions or recommendations for a specific user ID from the returned results, you can be confident that they correspond to the same user ID in the original centered matrix.\n",
    "\n",
    "***********************\n",
    "\n",
    "Before parameter tuning, I will run the recommender system for the train and validation set and record some baseline performance. Root Mean Squared Error (RMSE) will be used as performance metric. \n",
    "\n",
    "- Reason behind this is the corresponding original and predicted centered ratings from the train_data and val_data will be used for measuring performance. A form of squared mean error is appropriate for such cases. Recall and precision revolve around ratings which are relevant to the user or not, which is difficult and subjective to identify within this model. \n",
    "  \n",
    "- Furthermore, RMSE will is expressed in the same units as the input data, making it easy to interpret for a user but a stakeholder as well.\n",
    "- RMSE tend to highlight differences more on smaller sample sizes than MSE would do.\n",
    "\n",
    "# Netflix Prize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I make a baseline selection of latent factors\n",
    "num_latent_factors = 1\n",
    "\n",
    "# here I make a baseline selection of recommendations per user\n",
    "num_recommendations = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline train_data sample tenth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_train, user_id_dict_train, movie_id_dict_train, user_ids_train, movie_ids_train = create_user_item_matrix(train_data)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_train = list(set(user_ids_train))\n",
    "item_ids_train = list(set(movie_ids_train))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_train, user_means_train = center_data(user_item_matrix_train)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_TRAIN, Sigma_train, Vt_train = apply_svd(centered_user_item_matrix_train, num_latent_factors)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_train, all_predicted_centered_ratings_train = compute_recommendations_for_all_users(U_TRAIN, Sigma_train, Vt_train, user_means_train, user_ids_train, num_recommendations, user_item_matrix_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline val_data smaple tenth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_val, user_id_dict_val, movie_id_dict_val, user_ids_val, movie_ids_val = create_user_item_matrix(val_data)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_val = list(set(user_ids_val))\n",
    "item_ids_val = list(set(movie_ids_val))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_val, user_means_val = center_data(user_item_matrix_val)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_VAL, Sigma_val, Vt_val = apply_svd(centered_user_item_matrix_val, num_latent_factors)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_val, all_predicted_centered_ratings_val = compute_recommendations_for_all_users(U_VAL, Sigma_val, Vt_val, user_means_val, user_ids_val, num_recommendations, user_item_matrix_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline train_data sample third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_train2, user_id_dict_train2, movie_id_dict_train2, user_ids_train2, movie_ids_train2 = create_user_item_matrix(train_data2)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_train2 = list(set(user_ids_train2))\n",
    "item_ids_train2 = list(set(movie_ids_train2))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_train2, user_means_train2 = center_data(user_item_matrix_train2)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_TRAIN2, Sigma_train2, Vt_train2 = apply_svd(centered_user_item_matrix_train2, num_latent_factors)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_train2, all_predicted_centered_ratings_train2 = compute_recommendations_for_all_users(U_TRAIN2, Sigma_train2, Vt_train2, user_means_train2, user_ids_train2, num_recommendations, user_item_matrix_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline val_data sample third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_val2, user_id_dict_val2, movie_id_dict_val2, user_ids_val2, movie_ids_val2 = create_user_item_matrix(val_data2)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_val2 = list(set(user_ids_val2))\n",
    "item_ids_val2 = list(set(movie_ids_val2))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_val2, user_means_val2 = center_data(user_item_matrix_val2)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_val2, Sigma_val2, Vt_val2 = apply_svd(centered_user_item_matrix_val2, num_latent_factors)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_val2, all_predicted_centered_ratings_val2 = compute_recommendations_for_all_users(U_val2, Sigma_val2, Vt_val2, user_means_val2, user_ids_val2, num_recommendations, user_item_matrix_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_rmse(original_ratings, predicted_ratings):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean Square Error (RMSE) between the original ratings and the predicted ratings. MovieIds a user has not interacted with is turned into 0 for now.\n",
    "\n",
    "    Parameters:\n",
    "    original_ratings (numpy.ndarray): Array containing the original ratings.\n",
    "    predicted_ratings (numpy.ndarray): Array containing the predicted ratings.\n",
    "\n",
    "    Returns:\n",
    "    float: The RMSE value.\n",
    "    \n",
    "    \"\"\"\n",
    "    # handle implicit ratings with 0s for now\n",
    "    original_ratings = np.nan_to_num(original_ratings, nan=0, posinf=0, neginf=0)\n",
    "    predicted_ratings = np.nan_to_num(predicted_ratings, nan=0, posinf=0, neginf=0)\n",
    "\n",
    "    # make 1d arrays by flattening them to be able to make masks\n",
    "    original_ratings_flat = original_ratings.flatten()\n",
    "    predicted_ratings_flat = predicted_ratings.flatten()\n",
    "    \n",
    "    # remove entries with no original rating (unrated items)\n",
    "    mask = original_ratings_flat != 0\n",
    "    original_ratings_flat = original_ratings_flat[mask]\n",
    "    predicted_ratings_flat = predicted_ratings_flat[mask]\n",
    "    \n",
    "    # Compute the squared differences\n",
    "    squared_diff = np.square(original_ratings_flat - predicted_ratings_flat)\n",
    "    \n",
    "    # Compute the mean squared error\n",
    "    mse = np.mean(squared_diff)\n",
    "    \n",
    "    # Compute the square root of the mean squared error to get RMSE\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return mse,rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9086, 8)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(9086, 8)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(9086, 8)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item_matrix_train.shape\n",
    "centered_user_item_matrix_train.shape\n",
    "all_predicted_centered_ratings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SJYHa\\Desktop\\venv_ddb\\ddb_pymer4\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\SJYHa\\Desktop\\venv_ddb\\ddb_pymer4\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance on the FIRST training set\n",
    "train_mse,train_rmse = compute_mse_rmse(centered_user_item_matrix_train, all_predicted_centered_ratings_train)\n",
    "# Evaluate performance on the FIRST validation set\n",
    "val_mse,val_rmse = compute_mse_rmse(centered_user_item_matrix_val, all_predicted_centered_ratings_val)\n",
    "\n",
    "# Evaluate performance on the SECOND training set\n",
    "train_mse2,train_rmse2 = compute_mse_rmse(centered_user_item_matrix_train2, all_predicted_centered_ratings_train2)\n",
    "# Evaluate performance on the SECOND validation set\n",
    "val_mse2,val_rmse2 = compute_mse_rmse(centered_user_item_matrix_val2, all_predicted_centered_ratings_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIRST sample result (sample_tenth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training set: 1.3380707207291844\n",
      "RMSE on validation set: nan\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE on training set:\", train_rmse)\n",
    "print(\"RMSE on validation set:\", val_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline findings (sample_tenth):** MSE is lower on validation set than training set, indicating the model overfits to data it has already seen. Then again, it could also have something to do with the sample size. It choose the smaller one first. Having a smaller amount of reviews to rely on, could cause the model to perform worse on the validation data as the model has less reviews to work with.\n",
    "\n",
    "As in a SVD model the number of latent factors is the only tunable hyperparameter, I will perform hyperparameter tuning below on it below by looping throug different amounts of latent factors and computing their corresponding RMSE with relation to the orignal centered ratings.\n",
    "\n",
    "</BR>\n",
    "SECOND sample result (sample_third)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training set: 0.8220297034898152\n",
      "RMSE on validation set: 1.9386260127843542\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE on training set:\", train_rmse2)\n",
    "print(\"RMSE on validation set:\", val_rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline performance (increased sample size to a third of the dataset (sample_third)):** \n",
    "\n",
    "we can see two things:\n",
    "- the error stays more or less the same on the training and the validation sets \n",
    "- the difference between the error on the training and the validation set is substantially smaller\n",
    "\n",
    "**Conclusion:** the SVD recommender system is overfitting less to the training data when the sample size increases. This is a logic thing to happen, as the validation set becomes bigger, leading the RecSys to have more unseen reviews to work with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning (sample_tenth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Latent Factors: 1 | RMSE: 1.3380707207291844\n",
      "Num Latent Factors: 10 | RMSE: 1.1615958074236314\n",
      "Num Latent Factors: 25 | RMSE: 1.1615958074236314\n",
      "Num Latent Factors: 50 | RMSE: 1.1615958074236314\n",
      "Num Latent Factors: 250 | RMSE: 1.1615958074236314\n"
     ]
    }
   ],
   "source": [
    "latent_factors_range = [1, 10, 25, 50, 250]\n",
    "rmse_values = []\n",
    "\n",
    "for num_latent_factors in latent_factors_range:\n",
    "    # perform only SVD and generationg of recommendations again while doing the loop, as the matrix and centered matrix will not change depending on the amount of latent factors    \n",
    "    # apply Singular Value Decomposition (SVD)\n",
    "    U_TRAIN, Sigma_train, Vt_train = apply_svd(centered_user_item_matrix_train, num_latent_factors)\n",
    "    \n",
    "    # compute recommendations for all users\n",
    "    all_recommendations_train, all_predicted_centered_ratings_train = compute_recommendations_for_all_users(U_TRAIN, Sigma_train, Vt_train, user_means_train, user_ids_train, num_recommendations, user_item_matrix_train)\n",
    "    \n",
    "    # compute Mean Squared Error (MSE) or Root Mean Square Error (RMSE) with the function I have written for it\n",
    "    mse, rmse = compute_mse_rmse(centered_user_item_matrix_train, all_predicted_centered_ratings_train)\n",
    "    \n",
    "    # append the RMSE value to the list\n",
    "    rmse_values.append(rmse)\n",
    "\n",
    "# print the result descendingly\n",
    "for i, num_latent_factors in enumerate(latent_factors_range):\n",
    "    print(f\"Num Latent Factors: {num_latent_factors} | RMSE: {rmse_values[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning (sample_third)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Latent Factors: 1 | RMSE: 0.8220297034898152\n",
      "Num Latent Factors: 10 | RMSE: 0.7797149702343218\n",
      "Num Latent Factors: 25 | RMSE: 0.7686107981337318\n",
      "Num Latent Factors: 50 | RMSE: 0.7686107981337318\n",
      "Num Latent Factors: 250 | RMSE: 0.7686107981337318\n"
     ]
    }
   ],
   "source": [
    "latent_factors_range2 = [1, 10, 25, 50, 250]\n",
    "rmse_values2 = []\n",
    "\n",
    "for num_latent_factors in latent_factors_range2:\n",
    "    # perform only SVD and generationg of recommendations again while doing the loop, as the matrix and centered matrix will not change depending on the amount of latent factors    \n",
    "    # apply Singular Value Decomposition (SVD)\n",
    "    U_TRAIN2, Sigma_train2, Vt_train2 = apply_svd(centered_user_item_matrix_train2, num_latent_factors)\n",
    "    \n",
    "    # compute recommendations for all users\n",
    "    all_recommendations_train2, all_predicted_centered_ratings_train2 = compute_recommendations_for_all_users(U_TRAIN2, Sigma_train2, Vt_train2, user_means_train2, user_ids_train2, num_recommendations, user_item_matrix_train2)\n",
    "    \n",
    "    # compute Mean Squared Error (MSE) or Root Mean Square Error (RMSE) with the function I have written for it\n",
    "    mse, rmse = compute_mse_rmse(centered_user_item_matrix_train2, all_predicted_centered_ratings_train2)\n",
    "    \n",
    "    # append the RMSE value to the list\n",
    "    rmse_values2.append(rmse)\n",
    "\n",
    "# print the result descendingly\n",
    "for i, num_latent_factors in enumerate(latent_factors_range2):\n",
    "    print(f\"Num Latent Factors: {num_latent_factors} | RMSE: {rmse_values2[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best amount of latent factors is 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final predictions on test set:\n",
    "\n",
    "Increased sample size (sample_third) and the best amount of latent factors are used to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent_factors_test2 = 250\n",
    "\n",
    "user_item_matrix_test2, user_id_dict_test2, movie_id_dict_test2, user_ids_test2, movie_ids_test2 = create_user_item_matrix(test_data2)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_test2 = list(set(user_ids_test2))\n",
    "item_ids_test2 = list(set(movie_ids_test2))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_test2, user_means_test2 = center_data(user_item_matrix_test2)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_test2, Sigma_test2, Vt_test2 = apply_svd(centered_user_item_matrix_test2, num_latent_factors_test2)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_test2, all_predicted_centered_ratings_test2 = compute_recommendations_for_all_users(U_test2, Sigma_test2, Vt_test2, user_means_test2, user_ids_test2, num_recommendations, user_item_matrix_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test set: 1.5934194788493532\n"
     ]
    }
   ],
   "source": [
    "test_mse2,test_rmse2 = compute_mse_rmse(centered_user_item_matrix_test2, all_predicted_centered_ratings_test2)\n",
    "print(\"RMSE on test set:\", test_rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall conclusion:\n",
    "\n",
    "Hyperparametertuning and increasing the sample size did eventually not lead to a lower error on the test set. Maybe adding more features will help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movielens dataset:\n",
    "\n",
    "Fit the RecSys on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent_factors_train3 = 250\n",
    "\n",
    "user_item_matrix_train3, user_id_dict_train3, movie_id_dict_train3, user_ids_train3, movie_ids_train3 = create_user_item_matrix(train_data3)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_train3 = list(set(user_ids_train3))\n",
    "item_ids_train3 = list(set(movie_ids_train3))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_train3, user_means_train3 = center_data(user_item_matrix_train3)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_train3, Sigma_train3, Vt_train3 = apply_svd(centered_user_item_matrix_train3, num_latent_factors_train3)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_train3, all_predicted_centered_ratings_train3 = compute_recommendations_for_all_users(U_train3, Sigma_train3, Vt_train3, user_means_train3, user_ids_train3, num_recommendations, user_item_matrix_train3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the RecSys on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent_factors_val3 = 250\n",
    "\n",
    "user_item_matrix_val3, user_id_dict_val3, movie_id_dict_val3, user_ids_val3, movie_ids_val3 = create_user_item_matrix(val_data3)\n",
    "\n",
    "# get unique movieIds, use set to ensure unique values and put ids in a list\n",
    "user_ids_val3 = list(set(user_ids_val3))\n",
    "item_ids_val3 = list(set(movie_ids_val3))\n",
    "\n",
    "# unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "centered_user_item_matrix_val3, user_means_val3 = center_data(user_item_matrix_val3)\n",
    "\n",
    "# apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "U_val3, Sigma_val3, Vt_val3 = apply_svd(centered_user_item_matrix_val3, num_latent_factors_val3)\n",
    "\n",
    "# compute the recommendations\n",
    "all_recommendations_val3, all_predicted_centered_ratings_val3 = compute_recommendations_for_all_users(U_val3, Sigma_val3, Vt_val3, user_means_val3, user_ids_val3, num_recommendations, user_item_matrix_val3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See baseline performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training set: 0.7959795423781132\n",
      "RMSE on validation set: 1.566462504410829\n"
     ]
    }
   ],
   "source": [
    "train_mse3,train_rmse3 = compute_mse_rmse(centered_user_item_matrix_train3, all_predicted_centered_ratings_train3)\n",
    "print(\"RMSE on training set:\", train_rmse3)\n",
    "val_mse3,val_rmse3 = compute_mse_rmse(centered_user_item_matrix_val3, all_predicted_centered_ratings_val3)\n",
    "print(\"RMSE on validation set:\", val_rmse3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is performing slightly better, and the error is smaller as well. Adding a filter to the movieLens dataset helped to turn down the error.\n",
    "<p>\n",
    "</p>\n",
    "\n",
    "</br>Perform hyperparameter tuning on num_latent_factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Latent Factors: 1 | RMSE: 0.8419723262061493\n",
      "Num Latent Factors: 10 | RMSE: 0.8038566651519746\n",
      "Num Latent Factors: 25 | RMSE: 0.7959795423781132\n",
      "Num Latent Factors: 50 | RMSE: 0.7959795423781132\n",
      "Num Latent Factors: 250 | RMSE: 0.7959795423781132\n"
     ]
    }
   ],
   "source": [
    "latent_factors_range3 = [1, 10, 25, 50, 250]\n",
    "rmse_values3 = []\n",
    "\n",
    "for num_latent_factors in latent_factors_range3:\n",
    "    # perform only SVD and generationg of recommendations again while doing the loop, as the matrix and centered matrix will not change depending on the amount of latent factors    \n",
    "    # apply Singular Value Decomposition (SVD)\n",
    "    U_TRAIN3, Sigma_train3, Vt_train3 = apply_svd(centered_user_item_matrix_train3, num_latent_factors)\n",
    "    \n",
    "    # compute recommendations for all users\n",
    "    all_recommendations_train3, all_predicted_centered_ratings_train3 = compute_recommendations_for_all_users(U_TRAIN3, Sigma_train3, Vt_train3, user_means_train3, user_ids_train3, num_recommendations, user_item_matrix_train3)\n",
    "    \n",
    "    # compute Mean Squared Error (MSE) or Root Mean Square Error (RMSE) with the function I have written for it\n",
    "    mse, rmse = compute_mse_rmse(centered_user_item_matrix_train3, all_predicted_centered_ratings_train3)\n",
    "    \n",
    "    # append the RMSE value to the list\n",
    "    rmse_values3.append(rmse)\n",
    "\n",
    "# print the result descendingly\n",
    "for i, num_latent_factors in enumerate(latent_factors_range3):\n",
    "    print(f\"Num Latent Factors: {num_latent_factors} | RMSE: {rmse_values3[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter which of the iterated number of latent factors I put in, the RecSys model is not able to have a lower error term, meaning the model operates bad.\n",
    "\n",
    "### Overall conclusion:\n",
    "\n",
    "The current amount of features is not able to capture the complexity of the data, meaning it is not able to catch each preference of the user. Adding more features will maybe lead to a better result.\n",
    "\n",
    "\n",
    "**BY ADDING FILTER MATRIX IS LESS SPARSE, IMPLICIT FEEDBACK IS NOT TAKEN INTO ACCOUNT. BOTH ARE CASES WHICH COULD LEAD THE MODEL TO PERFORM BETTER. BUT THIS DOES NOT MEAN THE MODEL PERFORM WELL, AS IT ALSO NEEDS TO GENERATE RECOMMENDATIONS FOR NEW USERS (COLD START PROBLEM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include time feature in matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data2, val_data2, test_data2 = train_val_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract titles, user IDs, ratings, and dates\n",
    "# review_data2 = train_data2['review_data'].values\n",
    "# user_ids2 = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data2])\n",
    "# ratings2 = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data2])\n",
    "# dates = np.concatenate([np.array([entry['date'] for entry in row], dtype='datetime64') for row in review_data2])\n",
    "# movieIds2 = np.concatenate([[movieId] * len(row) for movieId, row in zip(train_data['movieId'], review_data2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movieIds2\n",
    "# user_ids2\n",
    "# ratings\n",
    "# dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define function to convert datetime64[D] to months to normalize the dates\n",
    "# def get_month(date):\n",
    "#     month = (date.astype('datetime64[M]').astype(int) % 12) + 1\n",
    "#     return month\n",
    "\n",
    "# # Convert datetime64[D] dates to months\n",
    "# months = np.array([get_month(date) for date in dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dictionaries to map user IDs and movie IDs to unique indices\n",
    "# user_id_dict2 = {user_id: index for index, user_id in enumerate(np.unique(user_ids2))}\n",
    "# movie_id_dict2 = {movie_id: index for index, movie_id in enumerate(np.unique(movieIds2))}\n",
    "\n",
    "# # Initialize the user-item-time matrix\n",
    "# user_count2 = len(user_id_dict2)\n",
    "# movie_count2 = len(movie_id_dict2)\n",
    "# matrix_3d = np.zeros((user_count2, movie_count2, 2))\n",
    "\n",
    "# # Populate the matrix with ratings and normalized timestamps\n",
    "# for user_id, movie_id, rating, month in zip(user_ids2, movieIds2, ratings2, months):\n",
    "#     user_index = user_id_dict2[user_id]\n",
    "#     movie_index = movie_id_dict2[movie_id]\n",
    "#     matrix_3d[user_index, movie_index] = [rating, month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_values = np.unique(matrix_3d)\n",
    "# print(\"Unique values in the user-item-time matrix:\", unique_values)\n",
    "# matrix_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set is used because it does not allow for duplicates\n",
    "# user_ids2 = set()\n",
    "\n",
    "# # iterate over each row\n",
    "# for index, row in train_data2.iterrows():\n",
    "#     # iterate over each dictionary in the 'review_data' column of the current row\n",
    "#     for review_dict in row['review_data']:\n",
    "#         user_id = review_dict.get('userId')  # Extract userId from the dictionary\n",
    "#         if user_id:  # Check if userId exists\n",
    "#             user_ids2.add(int(user_id))  # Add user ID to the set\n",
    "\n",
    "# user_ids2 = list(user_ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # put movieids in set so duplicates are not allowed here either\n",
    "# item_ids2 = list(set(train_data2['movieId'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def center_data_3d(matrix_3d):\n",
    "#     # Calculate mean along the second axis (movies axis)\n",
    "#     user_means = np.mean(matrix_3d, axis=(1, 2), keepdims=True)\n",
    "#     # Subtract the mean from the original matrix\n",
    "#     centered_user_item_matrix_3d = matrix_3d - user_means\n",
    "#     return centered_user_item_matrix_3d, user_means\n",
    "\n",
    "# def apply_svd_3d(centered_user_item_matrix_3d, num_latent_factors):\n",
    "#     # Reshape the matrix to be 2D for SVD\n",
    "#     reshaped_matrix = centered_user_item_matrix_3d.reshape(centered_user_item_matrix_3d.shape[0], -1)\n",
    "#     # Perform SVD\n",
    "#     U, Sigma, Vt = np.linalg.svd(reshaped_matrix, full_matrices=False)\n",
    "#     # Keep only the specified number of latent factors\n",
    "#     U = U[:, :num_latent_factors]\n",
    "#     Sigma = np.diag(Sigma[:num_latent_factors])\n",
    "#     Vt = Vt[:num_latent_factors, :]\n",
    "#     return U, Sigma, Vt\n",
    "\n",
    "# def compute_recommendations_for_all_users_3d(U, Sigma, Vt, user_means, user_ids2, num_recommendations):\n",
    "#     all_recommendations = {}\n",
    "#     user_id_to_index = {user_id: idx for idx, user_id in enumerate(user_ids2)}\n",
    "#     for user_id in user_ids2:\n",
    "#         user_index = user_id_to_index[user_id]\n",
    "#         # Perform dot product for each user\n",
    "#         user_ratings = np.dot(U[user_index], np.dot(Sigma, Vt)) + user_means[user_index]\n",
    "#         # Exclude items already interacted with\n",
    "#         user_ratings[user_item_matrix[user_index] > 0] = -np.inf\n",
    "#         # Get indices of top recommendations\n",
    "#         top_indices = np.argsort(user_ratings)[::-1][:num_recommendations]\n",
    "#         top_items = top_indices + 1\n",
    "#         all_recommendations[user_id] = top_items\n",
    "#     return all_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select the Number of Latent Factors\n",
    "# num_latent_factors = 4 \n",
    "\n",
    "# # unpack the tuple returned by center_data function to get an updates user item matrix which is more robust to variations in rating\n",
    "# centered_user_item_matrix_3d, user_means = center_data_3d(matrix_3d)\n",
    "\n",
    "# # apply SVD using the centered matrix to reduce memory usage and to decompose the matrix to be able to make recommendations using the dot product method\n",
    "# U, Sigma, Vt = apply_svd_3d(centered_user_item_matrix_3d, num_latent_factors)\n",
    "# U \n",
    "# Sigma\n",
    "# Vt\n",
    "\n",
    "# # define number of recommendations per user\n",
    "# num_recommendations = 4\n",
    "\n",
    "# # compute the recommendations\n",
    "# all_recommendations2 = compute_recommendations_for_all_users_3d(U, Sigma, Vt, user_means, user_ids, num_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_recommendations2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redundant but maybe useful for troubleshooting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract unique user IDs from the dataset\n",
    "# dataset_user_ids = set()\n",
    "# for review_list in train_data['review_data']:\n",
    "#     for review_dict in review_list:\n",
    "#         user_id = review_dict.get('userId')\n",
    "#         if user_id:\n",
    "#             dataset_user_ids.add(user_id)\n",
    "\n",
    "# # Check if all user IDs in the matrix are also in the dataset, and vice versa\n",
    "# user_ids_in_dataset_not_in_matrix = dataset_user_ids - set(user_ids)\n",
    "# user_ids_in_matrix_not_in_dataset = set(user_ids) - dataset_user_ids\n",
    "# len(user_ids_in_dataset_not_in_matrix)\n",
    "# len(user_ids_in_matrix_not_in_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User item matrix with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract review dates, user IDs, and ratings using NumPy\n",
    "# review_data = train_data['review_data'].values\n",
    "# user_ids = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data])\n",
    "# ratings = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data])\n",
    "\n",
    "# # Extract movie titles\n",
    "# movieIds = np.concatenate([[movieId] * len(row) for movieId, row in zip(train_data['movieId'], review_data)])\n",
    "\n",
    "# # Create a DataFrame with review dates, user IDs, ratings, and movie titles\n",
    "# review_df = pd.DataFrame({'userId': user_ids, 'rating': ratings, 'movieId': movieIds})\n",
    "\n",
    "# # Pivot review_df to get user-item matrix with reviews as values\n",
    "# user_item_matrix_df = review_df.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "# # Fill NaN values with 0\n",
    "# user_item_matrix_df = user_item_matrix_df.fillna(0)\n",
    "\n",
    "# # Convert DataFrame to NumPy array\n",
    "# user_item_matrix = user_item_matrix_df.to_numpy()\n",
    "\n",
    "# user_item_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract titles from dataframe, user IDs, and ratings from dictionary using NumPy\n",
    "# review_data = train_data['review_data'].values\n",
    "# user_ids = np.concatenate([np.array([entry['userId'] for entry in row]) for row in review_data])\n",
    "# ratings = np.concatenate([np.array([entry['rating'] for entry in row]) for row in review_data])\n",
    "# movieIds = np.concatenate([[movieId] * len(row) for movieId, row in zip(train_data['movieId'], review_data)])\n",
    "\n",
    "# # create dictionaries to map user IDs and movie IDs to unique indices to map over\n",
    "# user_id_dict = {user_id: index for index, user_id in enumerate(np.unique(user_ids))}\n",
    "# movie_id_dict = {movie_id: index for index, movie_id in enumerate(np.unique(movieIds))}\n",
    "\n",
    "# # initialize an empty user-item matrix\n",
    "# user_count = len(user_id_dict)\n",
    "# movie_count = len(movie_id_dict)\n",
    "# user_item_matrix = np.zeros((user_count, movie_count))\n",
    "\n",
    "# # populate the user-item matrix with ratings from netflix dataset\n",
    "# for i, (user_id, movie_id, rating) in enumerate(zip(user_ids, movieIds, ratings)):\n",
    "#     user_index = user_id_dict[user_id]\n",
    "#     movie_index = movie_id_dict[movie_id]\n",
    "#     user_item_matrix[user_index, movie_index] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set is used because it does not allow for duplicates\n",
    "# user_ids = set()\n",
    "\n",
    "# # iterate over each row\n",
    "# for index, row in train_data.iterrows():\n",
    "#     # iterate over each dictionary in the 'review_data' column of the current row\n",
    "#     for review_dict in row['review_data']:\n",
    "#         user_id = review_dict.get('userId')  # Extract userId from the dictionary\n",
    "#         if user_id:  # Check if userId exists\n",
    "#             user_ids.add(int(user_id))  # Add user ID to the set\n",
    "\n",
    "# user_ids = list(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # put movieids in set so duplicates are not allowed here either\n",
    "# item_ids = list(set(train_data['movieId'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function which computes recommendations and returns recommendations only, not the predicted ratings for every item after the svd matrix dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will compute recommendations for each user_id in the training/test/validation data by performing the dot product between the previous reviews in the matrix by the reconstruction of the user item matrix with less features\n",
    "# def compute_recommendations_for_all_users(U, Sigma, Vt, user_means, user_ids, num_recommendations):\n",
    "#     all_recommendations = {}\n",
    "#     user_id_to_index = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "#     for user_id in user_ids:\n",
    "#         user_index = user_id_to_index[user_id]\n",
    "#         # matrix multriplication between Sigma and Vt, to reconstruct an item matrix with less features, followed by the dot product of U and the reconstruction of item matrix. It essentially calculates the predicted ratings for each item for the given user based on their latent given ratings. user_means[user_index] rules out the items the user already interacted with.\n",
    "#         user_ratings = np.dot(U[user_index, :], np.dot(Sigma, Vt)) + user_means[user_index]\n",
    "#         user_ratings[user_item_matrix[user_index, :] > 0] = -np.inf\n",
    "#         top_indices = np.argsort(user_ratings)[::-1][:num_recommendations]\n",
    "#         top_items = top_indices + 1\n",
    "#         all_recommendations[user_id] = top_items\n",
    "#     return all_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old functions to extract unique user and item ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_unique_user_ids(train_test_val_set):\n",
    "#     user_ids = set()\n",
    "#     # iterate over each row\n",
    "#     for index, row in train_test_val_set.iterrows():\n",
    "#         # iterate over each dictionary in the 'review_data' column of the current row\n",
    "#         for review_dict in row['review_data']:\n",
    "#             user_id = review_dict.get('userId')  # Extract userId from the dictionary\n",
    "#             if user_id:  # Check if userId exists\n",
    "#                 user_ids.add(int(user_id))  # Add user ID to the set\n",
    "#     return list(user_ids)\n",
    "\n",
    "# def extract_unique_movie_ids(train_test_val_set):\n",
    "#     movie_ids = set(train_test_val_set['movieId'].unique())\n",
    "#     return list(movie_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For looking at arrays if they are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_recommendations_train\n",
    "# all_recommendations_val\n",
    "# centered_user_item_matrix_train # variable for centered user item matrix train\n",
    "# all_predicted_centered_ratings_train # variable for predicted centered user item matrix train\n",
    "# centered_user_item_matrix_val # variable for centered user item matrix val\n",
    "# all_predicted_centered_ratings_val # variable for predicted centered user item matrix val\n",
    "# print('All arrays are in the same format, meaning they are appropriately prepped for model evaluation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_ddb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
